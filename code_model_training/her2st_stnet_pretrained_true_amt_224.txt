nohup: ignoring input
2025-11-14 16:48:59,736 - INFO - Logging setup complete.
2025-11-14 16:48:59,736 - INFO - Experiment information saved to the path: /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/experiment_info.json
2025-11-14 16:48:59,736 - INFO - Total samples: 36
2025-11-14 16:48:59,736 - INFO -   - A1.h5 ↔ A1.h5ad
2025-11-14 16:48:59,736 - INFO -   - A2.h5 ↔ A2.h5ad
2025-11-14 16:48:59,736 - INFO -   - A3.h5 ↔ A3.h5ad
2025-11-14 16:48:59,736 - INFO -   - A4.h5 ↔ A4.h5ad
2025-11-14 16:48:59,736 - INFO -   - A5.h5 ↔ A5.h5ad
2025-11-14 16:48:59,736 - INFO -   - A6.h5 ↔ A6.h5ad
2025-11-14 16:48:59,736 - INFO -   - B1.h5 ↔ B1.h5ad
2025-11-14 16:48:59,736 - INFO -   - B2.h5 ↔ B2.h5ad
2025-11-14 16:48:59,736 - INFO -   - B3.h5 ↔ B3.h5ad
2025-11-14 16:48:59,736 - INFO -   - B4.h5 ↔ B4.h5ad
2025-11-14 16:48:59,736 - INFO -   - B5.h5 ↔ B5.h5ad
2025-11-14 16:48:59,736 - INFO -   - B6.h5 ↔ B6.h5ad
2025-11-14 16:48:59,736 - INFO -   - C1.h5 ↔ C1.h5ad
2025-11-14 16:48:59,736 - INFO -   - C2.h5 ↔ C2.h5ad
2025-11-14 16:48:59,736 - INFO -   - C3.h5 ↔ C3.h5ad
2025-11-14 16:48:59,736 - INFO -   - C4.h5 ↔ C4.h5ad
2025-11-14 16:48:59,736 - INFO -   - C5.h5 ↔ C5.h5ad
2025-11-14 16:48:59,736 - INFO -   - C6.h5 ↔ C6.h5ad
2025-11-14 16:48:59,736 - INFO -   - D1.h5 ↔ D1.h5ad
2025-11-14 16:48:59,736 - INFO -   - D2.h5 ↔ D2.h5ad
2025-11-14 16:48:59,736 - INFO -   - D3.h5 ↔ D3.h5ad
2025-11-14 16:48:59,736 - INFO -   - D4.h5 ↔ D4.h5ad
2025-11-14 16:48:59,736 - INFO -   - D5.h5 ↔ D5.h5ad
2025-11-14 16:48:59,736 - INFO -   - D6.h5 ↔ D6.h5ad
2025-11-14 16:48:59,736 - INFO -   - E1.h5 ↔ E1.h5ad
2025-11-14 16:48:59,736 - INFO -   - E2.h5 ↔ E2.h5ad
2025-11-14 16:48:59,736 - INFO -   - E3.h5 ↔ E3.h5ad
2025-11-14 16:48:59,736 - INFO -   - F1.h5 ↔ F1.h5ad
2025-11-14 16:48:59,736 - INFO -   - F2.h5 ↔ F2.h5ad
2025-11-14 16:48:59,736 - INFO -   - F3.h5 ↔ F3.h5ad
2025-11-14 16:48:59,737 - INFO -   - G1.h5 ↔ G1.h5ad
2025-11-14 16:48:59,737 - INFO -   - G2.h5 ↔ G2.h5ad
2025-11-14 16:48:59,737 - INFO -   - G3.h5 ↔ G3.h5ad
2025-11-14 16:48:59,737 - INFO -   - H1.h5 ↔ H1.h5ad
2025-11-14 16:48:59,737 - INFO -   - H2.h5 ↔ H2.h5ad
2025-11-14 16:48:59,737 - INFO -   - H3.h5 ↔ H3.h5ad
2025-11-14 16:48:59,737 - INFO -  Using gene list from: ['HPS6', 'TNC', 'NR1H2', 'NUP93', 'HNRNPUL2', 'MARS', 'SUGP1', 'DEAF1', 'WDR13', 'FLNA', 'ELN', 'MX2', 'COQ4', 'LARP7', 'TXNDC17', 'MIIP', 'AQP1', 'POSTN', 'PSCA', 'FAM117A', 'AKT1S1', 'DVL1', 'SPIN1', 'LUC7L', 'TTC31', 'CIR1', 'GAR1', 'RALGAPA2', 'TIMP1', 'GNAI2', 'WDR73', 'UXS1', 'CNN2', 'C1QB', 'CCDC106', 'ARHGAP10', 'ATL2', 'UBL7', 'NDUFA12', 'PTRF', 'PPP1R1A', 'KAT6B', 'CORO1A', 'EZH2', 'CSF1', 'CD79A', 'SDHB', 'GATC', 'TMEM160', 'UXT', 'ZNF787', 'ATF6B', 'SNRPD3', 'ZNF217', 'SLC35A2', 'MMP9', 'ECHDC2', 'PEX11G', 'SF3B4', 'SYDE1', 'IGKC', 'RASGRP2', 'RND3', 'PNISR', 'HSPB1', 'UBR2', 'TESK1', 'STAMBP', 'DPYSL3', 'CPSF1', 'ZMYND19', 'COMP', 'TBCA', 'PPHLN1', 'MYL6B', 'AHCY', 'TRAP1', 'R3HDM2', 'TMEM123', 'TOM1L2', 'VPS13D', 'CD3D', 'MID1IP1', 'DPP7', 'LTBP3', 'GANAB', 'HLA-DOA', 'FAM193B', 'BRPF1', 'ITGB6', 'GBP5', 'CADM4', 'ARID1B', 'CEP250', 'PTRH1', 'GPSM1', 'DLG1', 'MLLT4', 'APBB2', 'C19orf24', 'CCS', 'DHX16', 'CRACR2B', 'CRABP2', 'MEN1', 'ARHGEF40', 'NDUFV3', 'HMGB2', 'ZFPL1', 'POLR2E', 'DAPK3', 'FAM213B', 'RNF135', 'FDPS', 'GPS1', 'DDAH1', 'CSF1R', 'CHCHD6', 'ARID3A', 'RABGAP1L', 'PHF11', 'IDH3G', 'SLC43A1', 'PER1', 'CORO7', 'DNAJC1', 'KMT2B', 'PSMD4', 'ADCY3', 'CTU2', 'PODXL', 'HLA-DMA', 'ATP6V0D1', 'BGN', 'GNAS', 'STRN3', 'PAQR4', 'MPND', 'METTL12', 'SLC9A3R2', 'CLDN3', 'TBXAS1', 'EFHD2', 'APOBEC3G', 'LRWD1', 'HLA-DPA1', 'ZNF592', 'ZG16B', 'NIT2', 'INPP4B', 'C4orf48', 'CPVL', 'TCEA2', 'CCDC167', 'MUCL1', 'MGMT', 'AKAP9', 'TXN2', 'YIPF4', 'RASSF3', 'HSPA4', 'SGTA', 'MLST8', 'MFAP4', 'FAIM2', 'TM7SF2', 'EXD3', 'SLFN13', 'ARID5B', 'EPHB3', 'CCL2', 'FAM118A', 'SMG9', 'MS4A6A', 'PIGQ', 'KLF9', 'ITGA5', 'C1S', 'NANS', 'AGPAT2', 'MST1R', 'ARF5', 'S100A14', 'SEMA3B', 'LMF1', 'ZFP36', 'CHFR', 'CFDP1', 'DDB2', 'AVEN', 'TDRD3', 'RAB11FIP1', 'SLC22A18', 'SRSF5', 'DNAJC7', 'NTMT1', 'SLC4A5', 'HLA-B', 'EXOSC2', 'MVP', 'NUP214', 'VIM', 'PTGDS', 'COL6A2', 'CPNE1', 'PDCD5', 'IGLC7', 'EXOSC10', 'TSR2', 'ATG12', 'ALG3', 'HYI', 'CCNE2', 'JAK1', 'ZMYND8', 'UBAP2L', 'C5orf38', 'JCHAIN', 'FNBP4', 'TRIM22', 'DPP9', 'NDUFA1', 'MORC2', 'MIF', 'IL10RA', 'ATP5O', 'RASA2', 'SCD', 'KCTD10', 'NDUFS1', 'UCK1', 'FUOM', 'CD52', 'MED25', 'RSL1D1', 'TMEM259', 'NEK7', 'FAP', 'PPM1G', 'SPSB1', 'KRT17', 'S100A9', 'BABAM1', 'ZNF771', 'RNASET2', 'IGHM', 'STAT5A', 'COX5A', 'KPNB1', 'MKNK1', 'C19orf52', 'ADI1', 'AGO2', 'RGS1', 'IFI27L1', 'MAPKAPK5', 'LYL1', 'MUS81', 'CAMTA2', 'GBP1', 'ASL', 'TIGD5', 'TMEM101', 'HOOK3', 'KMT2E', 'EIF3B', 'ELOF1', 'MZT2A', 'GPR108', 'KIAA1211L', 'MCTS1', 'TYROBP', 'C19orf60', 'ACER3', 'RAB3GAP1', 'C3', 'PLVAP', 'SURF1', 'LIMCH1', 'P4HA2', 'WDR34', 'PRPSAP1', 'NXN', 'FKBP10', 'PTOV1', 'MED13L', 'RBM42', 'BOK', 'C12orf45', 'SPAG9', 'BCAM', 'FKBP8', 'SNAPC3', 'ZNF791', 'AP5B1', 'GPRC5A', 'TAX1BP1', 'MICAL1', 'ZNF337', 'PTPRC', 'RALBP1', 'ZMAT5', 'GSTM3', 'PHF21A', 'CD14', 'PDGFRB', 'PRPF6', 'SRGN', 'MRPL2', 'CES2', 'FSTL3', 'IGHA1', 'SCAF1', 'TRIM47', 'MRPL51', 'XPO6', 'SOS2', 'POLR2J', 'MAP1S', 'PNMT', 'SP6', 'SIGMAR1', 'DNAJB2', 'SASH1', 'EVA1B', 'CDK5RAP1', 'IKZF3', 'BIRC2', 'MGP', 'IFI44L', 'SNTA1', 'PWP1', 'DCAF6', 'EIF1AD', 'SAT1', 'BCL2L12', 'ASNS', 'RMND1', 'SRSF1', 'ABCA7', 'CTDSPL', 'KIF13B', 'NMRAL1', 'POR', 'MRPL54', 'MAP3K14', 'GPX3', 'ARHGEF10L', 'TSPAN33', 'PCIF1', 'SNX8', 'ZFAND3', 'COPS7A', 'HES1', 'PDGFRA', 'ETFB', 'PSTPIP1', 'MYO1G', 'CCT4', 'ABCA3', 'CDH11', 'COL4A1', 'RBL1', 'MIB2', 'PODNL1', 'PLK1', 'FASN', 'AP5Z1', 'RHOB', 'SCX', 'LAMTOR2', 'PPP1R37', 'BAIAP3', 'ABHD14A', 'AEBP1', 'MAGOHB', 'XIAP', 'SLC2A1', 'TEX10', 'MAPK9', 'ISG15', 'TPM2', 'AAMDC', 'KIAA1715', 'PKNOX1', 'HLA-DPB1', 'PYURF', 'TARBP1', 'A2M', 'SOX4', 'CCDC80', 'SMYD2', 'PUS3', 'CORO1B', 'LST1', 'SEMA4B', 'ENPP1', 'IGFBP2', 'PIM2', 'SSSCA1', 'PLSCR1', 'GMFG', 'GEMIN4', 'BSG', 'EBAG9', 'VWF', 'FGFR1', 'ANKRD28', 'RECQL5', 'CGGBP1', 'ARHGEF3', 'TMEM86A', 'EXOSC1', 'ZSWIM8', 'HDAC6', 'ANKLE2', 'TRAPPC4', 'ARNT', 'PILRA', 'KIAA1217', 'HDAC5', 'TFF3', 'MFSD7', 'FAM173A', 'TNNT1', 'DOCK6', 'ANGPTL4', 'THOC6', 'ARFGEF2', 'COMMD1', 'SCNN1D', 'CLDN4', 'ERI3', 'KRT86', 'LAMB2', 'FUS', 'METTL16', 'PIH1D1', 'HCFC1R1', 'KCNAB2', 'PTPRJ', 'LARP1B', 'PAF1', 'MYH10', 'ABCF3', 'GLTSCR2', 'REEP6', 'IGHG3', 'C10orf54', 'CX3CL1', 'IDUA', 'PLK2', 'SAMM50', 'NDUFB9', 'STK17A', 'CTSK', 'TANC2', 'MAP3K6', 'ERAL1', 'MPST', 'NAA10', 'RAMP2', 'PTP4A3', 'LINGO1', 'EMC3', 'PLAC9', 'EXOC5', 'CRELD2', 'PMP22', 'PLTP', 'ZNF524', 'NCKAP1L', 'APOC1', 'PPP1R1B', 'PLIN2', 'ZNF362', 'THOC5', 'GLG1', 'AKR1A1', 'PPIB', 'BRK1', 'VCAM1', 'AGAP2', 'SEC23A', 'SLC9A8', 'PRKAB1', 'IGHG4', 'HCST', 'PEX14', 'SYK', 'TRIM25', 'PWWP2B', 'FSTL1', 'RIOK1', 'FZD2', 'FN1', 'GSK3A', 'BRF1', 'VPS45', 'FAM102A', 'TRAF5', 'JAG2', 'PCCB', 'TTI1', 'KRT10', 'ZNF91', 'TMX1', 'IGLC2', 'ARID4A', 'ASCL2', 'RSRP1', 'GBP3', 'UBE2W', 'FARP1', 'ATAD1', 'DDT', 'ATG16L2', 'RAB3A', 'LINC00176', 'XAB2', 'PLXND1', 'ARHGAP5', 'GRK3', 'CCL19', 'MYO9B', 'BOLA3', 'NSMCE1', 'UQCRC1', 'VARS', 'CCDC57', 'TEX2', 'CCL21', 'TXNRD2', 'TESMIN', 'TMEM14B', 'CD320', 'RAP1B', 'ABHD4', 'PXDN', 'CHST11', 'OSBPL8', 'PAFAH1B3', 'DPH3', 'FAM45A', 'SLPI', 'EIF3G', 'SOD3', 'ASNA1', 'FBF1', 'SFSWAP', 'GBP4', 'HELZ2', 'STT3A', 'ITGB4', 'ASMTL', 'NDUFB2', 'GYPC', 'PLXNA1', 'PLLP', 'TJP3', 'APBA2', 'COL3A1', 'NRP1', 'CXCL10', 'ERMP1', 'AGR2', 'FAAH', 'CCM2', 'INPP5J', 'FAH', 'THBS1', 'NFIC', 'PLP2', 'GLCCI1', 'ZNF644', 'HSPB11', 'SPARC', 'NEBL', 'ATP5H', 'KIAA0895L', 'TMBIM6', 'SNF8', 'NDUFB3', 'DDIT3', 'KATNA1', 'BAD', 'WDR74', 'PROSER1', 'KIAA1468', 'FBLN2', 'INPP5D', 'ECM1', 'CYP27A1', 'VEGFB', 'C1R', 'KRAS', 'FCER1G', 'MUC1', 'MTMR12', 'PNPLA6', 'CUX1', 'FOXP4', 'ZDHHC12', 'C10orf10', 'ZNF33B', 'IFI27', 'TNFRSF21', 'CBLC', 'ARHGAP4', 'WIPF1', 'RAB3IL1', 'C6orf48', 'ASH2L', 'MRPL21', 'KDM4B', 'TCF25', 'C6orf136', 'COL8A2', 'RARS2', 'RNF170', 'WDR46', 'ILKAP', 'TBC1D10A', 'BRF2', 'BST2', 'BRD9', 'OASL', 'CHD1', 'GAS6', 'OPTN', 'DMAP1', 'CORO2A', 'DPM2', 'C2CD2L', 'UBA6', 'TBC1D23', 'LUC7L3', 'JAK3', 'CALML5', 'LRRC8A', 'ATP6AP1', 'PHF13', 'LCP1', 'MAVS', 'MFSD3', 'TRIM14', 'ALDH16A1', 'INTS8', 'LUM', 'ZNF83', 'ARF6', 'PARN', 'ISLR', 'GBP2', 'WDFY3', 'ADAM15', 'MAP4K2', 'MRPL23', 'C2', 'CD4', 'KRT8', 'CUTC', 'HAGHL', 'ACAA1', 'ZFR', 'LANCL1', 'SLC8B1', 'TRAC', 'ANO10', 'SIGLEC1', 'OGFRL1', 'BIN3', 'NDUFC1', 'RRAS', 'GATA3', 'CCNB1IP1', 'IER5L', 'DERL3', 'PNP', 'TACC3', 'CERCAM', 'NINJ1', 'C1orf216', 'TCEA3', 'TMEM184A', 'BICDL1', 'DCXR', 'NELFE', 'FARSB', 'PPIH', 'CERS5', 'GMPPA', 'SH3BP5L', 'ZNF668', 'FADS2', 'NR4A1', 'COL14A1', 'PRKCSH', 'HLA-DRA', 'PLEKHO2', 'NCKAP5L', 'LIMK1', 'ITSN1', 'ARID5A', 'STMN1', 'FAM3A', 'TNFRSF4', 'GALK1', 'MYL9', 'SVIL', 'CLU', 'LCK', 'SMCHD1', 'QSOX1', 'STK4', 'IGLC3', 'JMJD6', 'RUVBL1', 'CFAP36', 'EFEMP2', 'FAM160B1', 'SLC16A2', 'SNX33', 'MAGEF1', 'DENND1A', 'PITX1', 'UFD1L', 'NFATC4', 'ORC2', 'CROCC', 'AES', 'HLA-DMB', 'ANKRD12', 'UBA52', 'SEC14L2', 'PLPPR2', 'CD74', 'CTIF', 'AURKAIP1', 'LGALS1', 'MTG2', 'TAX1BP3', 'SLC25A24', 'DESI1', 'RECQL4', 'CCDC88B', 'MYL12B', 'SRRT', 'INAFM1', 'LTB4R', 'CARS2', 'DDRGK1', 'CHD6', 'HSD17B1', 'YTHDC1', 'C15orf61', 'CAPG', 'PHKB', 'HLA-DRB1', 'BTG1', 'LMNA', 'SRPX2', 'MRPS22', 'CXXC5', 'PGLS', 'LTF', 'ZNRF1', 'IGFBP4', 'CLASRP', 'ARHGAP45', 'MICALL2', 'MAGI1', 'SRR', 'CD68', 'GNL2', 'TP53I3', 'TGFBR2', 'UBXN1', 'EIF3F', 'SLC2A8', 'LLGL1', 'SLC16A3', 'VAV1', 'NDRG1', 'TNFRSF14']
2025-11-14 16:48:59,737 - INFO - 
===== Starting Fold 1/5 =====
2025-11-14 16:48:59,737 - INFO - Fold 1: Train=28, Val=8
+++++++++++++++++++++++++++++++ /home/puneet/maninder/code_model_training/gene_outputs/her2.npy
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A1.h5ad
 Loaded images: (346, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (346, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A2.h5ad
 Loaded images: (325, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (325, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A3.h5ad
 Loaded images: (359, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (359, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A4.h5ad
 Loaded images: (343, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (343, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A5.h5ad
 Loaded images: (332, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (332, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A6.h5ad
 Loaded images: (360, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (360, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B1.h5ad
 Loaded images: (295, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (295, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B2.h5ad
 Loaded images: (270, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (270, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B3.h5ad
 Loaded images: (298, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (298, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B4.h5ad
 Loaded images: (283, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (283, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B5.h5ad
 Loaded images: (289, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (289, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B6.h5ad
 Loaded images: (277, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (277, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C3.h5ad
 Loaded images: (180, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (180, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C4.h5ad
 Loaded images: (184, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (184, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C6.h5ad
 Loaded images: (178, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (178, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D1.h5ad
 Loaded images: (306, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (306, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D2.h5ad
 Loaded images: (303, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (303, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D3.h5ad
 Loaded images: (301, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (301, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D5.h5ad
 Loaded images: (306, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (306, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D6.h5ad
 Loaded images: (315, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (315, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E1.h5ad
 Loaded images: (587, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (587, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E2.h5ad
2025-11-14 16:49:16,630 - INFO - Fold 1: Train=10786, Val=2834
2025-11-14 16:49:16,630 - INFO - train_datasets length:  10786
2025-11-14 16:49:16,631 - INFO - Number of train batches: 85
2025-11-14 16:49:16,631 - INFO - Initializing model...
2025-11-14 16:49:16,719 - INFO - Model
STNet(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (denseblock1): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition1): _Transition(
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock2): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition2): _Transition(
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock3): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer17): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer18): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer19): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer20): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer21): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer22): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer23): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer24): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition3): _Transition(
      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock4): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=1024, out_features=785, bias=True)
)
2025-11-14 16:49:16,722 - INFO - Using device: cuda
2025-11-14 16:49:17,767 - INFO - Fold 1 Train Epoch 1/200, Batch 0, Loss: 15.1587, Pearson: -0.0061, Spearman: -0.0061
2025-11-14 16:49:23,462 - INFO - Fold 1 Train Epoch 1/200, Batch 10, Loss: 15.6625, Pearson: 0.1260, Spearman: 0.0729
 Loaded images: (572, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (572, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F1.h5ad
 Loaded images: (691, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (691, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F2.h5ad
 Loaded images: (695, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (695, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F3.h5ad
 Loaded images: (712, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (712, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G3.h5ad
 Loaded images: (463, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (463, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H1.h5ad
 Loaded images: (613, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (613, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H2.h5ad
 Loaded images: (603, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (603, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C1.h5ad
 Loaded images: (176, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (176, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C2.h5ad
 Loaded images: (187, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (187, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C5.h5ad
 Loaded images: (181, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (181, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D4.h5ad
 Loaded images: (302, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (302, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E3.h5ad
 Loaded images: (570, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (570, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G1.h5ad
 Loaded images: (441, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (441, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G2.h5ad
 Loaded images: (467, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (467, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H3.h5ad
 Loaded images: (510, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (510, 785)
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 1 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        6.4964933 0.        7.8816557 7.5940995 0.
 0.        6.4964933 8.574614 ]
Sample y_pred values (first sample, first 10 genes):
[0.         0.4786843  0.         0.         0.43229592 0.
 0.         0.         0.         0.08593689]
y_true  -> mean=1.9737, std=3.4397, min=0.0000, max=12.6515
y_pred  -> mean=0.1693, std=0.2472, min=0.0000, max=2.2169
Batch 0 Pearson correlation: -0.0061
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.0082
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.0256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.0392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.0437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.0515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.0782
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.0988
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.1119
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.1006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.1260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.1322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.1613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.1598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.1663
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 16:49:29,138 - INFO - Fold 1 Train Epoch 1/200, Batch 20, Loss: 14.8419, Pearson: 0.1986, Spearman: 0.1400
2025-11-14 16:49:34,814 - INFO - Fold 1 Train Epoch 1/200, Batch 30, Loss: 13.7170, Pearson: 0.2751, Spearman: 0.2224
2025-11-14 16:49:40,486 - INFO - Fold 1 Train Epoch 1/200, Batch 40, Loss: 11.5105, Pearson: 0.3438, Spearman: 0.3171
2025-11-14 16:49:46,199 - INFO - Fold 1 Train Epoch 1/200, Batch 50, Loss: 10.9808, Pearson: 0.4008, Spearman: 0.3925
2025-11-14 16:49:51,932 - INFO - Fold 1 Train Epoch 1/200, Batch 60, Loss: 10.6520, Pearson: 0.4302, Spearman: 0.4434
2025-11-14 16:49:57,686 - INFO - Fold 1 Train Epoch 1/200, Batch 70, Loss: 9.5742, Pearson: 0.4758, Spearman: 0.4660
Batch 15 Pearson correlation: 0.1789
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.1827
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.1948
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.1976
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.2119
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.1986
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.2168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.2377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.2397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.2331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.2566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.2565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.2661
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.2868
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.2836
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.2751
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.3155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.2863
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.2920
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.2980
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.2955
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.3186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.3427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.3199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.3281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.3438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.3534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.3679
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.3744
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.3797
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.3898
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.3733
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.4106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.4145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.4151
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.4008
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.4223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.4381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.4287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.4321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.4251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.4210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.4280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.4282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.4386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.4302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.4396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.4326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.4695
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.4374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.4545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.4700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.4630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.4641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.4646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.4758
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.4759
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.4670
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.4539
2025-11-14 16:50:03,498 - INFO - Fold 1 Train Epoch 1/200, Batch 80, Loss: 9.4569, Pearson: 0.5003, Spearman: 0.4863
2025-11-14 16:50:06,911 - INFO - Fold 1 Train Epoch 1/200, Train Loss: 12.1296, Pearson Mean: 0.3263, Spearman Mean: 0.2960
2025-11-14 16:50:06,911 - INFO - Training Metrics: {'pearson_mean_genewise': 0.17, 'spearman_mean_genewise': 0.1569, 'l1_error_mean': 2.2979, 'l2_errors_mean': 12.1565, 'r2_scores_mean': -0.1899, 'pearson_std': 0.069, 'l2_error_q1': 5.961, 'l2_error_q2': 9.0948, 'l2_error_q3': 15.6002, 'r2_score_q1': -0.128, 'r2_score_q2': -0.0229, 'r2_score_q3': 0.0074, 'mape_mean': 80.6146, 'mape_std': 6.9527, 'rmse_mean': 3.301, 'rmse_std': 1.1225}
2025-11-14 16:50:07,134 - INFO - Fold 1 Val Epoch 1/200, Batch 0, Loss: 10.6116, Pearson: 0.4758, Spearman: 0.4638
2025-11-14 16:50:08,349 - INFO - Fold 1 Val Epoch 1/200, Batch 10, Loss: 8.9260, Pearson: 0.5445, Spearman: 0.5562
2025-11-14 16:50:09,561 - INFO - Fold 1 Val Epoch 1/200, Batch 20, Loss: 8.2299, Pearson: 0.1088, Spearman: 0.1290
2025-11-14 16:50:11,914 - INFO - Fold 1 Val Epoch 1/200, Val Loss: 9.8975, Pearson Mean: 0.3798, Spearman Mean: 0.3971
2025-11-14 16:50:11,914 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2487, 'spearman_mean_genewise': 0.2246, 'l1_error_mean': 2.0087, 'l2_errors_mean': 9.9357, 'r2_scores_mean': -0.2603, 'pearson_std': 0.0966, 'l2_error_q1': 4.8127, 'l2_error_q2': 7.6315, 'l2_error_q3': 12.4296, 'r2_score_q1': -0.0247, 'r2_score_q2': 0.0221, 'r2_score_q3': 0.0553, 'mape_mean': 76.6667, 'mape_std': 11.2698, 'rmse_mean': 2.9764, 'rmse_std': 1.0377}
2025-11-14 16:50:11,914 - INFO - Learning rate for epoch 1: 0.0001
2025-11-14 16:50:11,957 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_1/best_model.pth
2025-11-14 16:50:12,643 - INFO - Fold 1 Train Epoch 2/200, Batch 0, Loss: 8.9838, Pearson: 0.5161, Spearman: 0.4993
2025-11-14 16:50:18,503 - INFO - Fold 1 Train Epoch 2/200, Batch 10, Loss: 9.1342, Pearson: 0.5105, Spearman: 0.4943
2025-11-14 16:50:24,367 - INFO - Fold 1 Train Epoch 2/200, Batch 20, Loss: 8.9634, Pearson: 0.5141, Spearman: 0.5043
2025-11-14 16:50:30,244 - INFO - Fold 1 Train Epoch 2/200, Batch 30, Loss: 8.6553, Pearson: 0.5343, Spearman: 0.5027
2025-11-14 16:50:36,148 - INFO - Fold 1 Train Epoch 2/200, Batch 40, Loss: 8.4724, Pearson: 0.5355, Spearman: 0.5097
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.4892
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.4892
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.4910
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.4938
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.4771
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5003
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.4842
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.4982
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.4957
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.5077
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 12.156536102294922
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 2 =====================
Sample y_true values (first sample, first 10 genes):
[0.       6.473629 0.       6.473629 6.473629 8.418216 0.       6.473629
 6.473629 9.180238]
Sample y_pred values (first sample, first 10 genes):
[0.9730654 2.2771575 2.2074687 1.0127454 4.966297  3.7899847 0.6933931
 2.8086224 4.294231  6.606703 ]
y_true  -> mean=2.0654, std=3.4749, min=0.0000, max=12.8087
y_pred  -> mean=1.7917, std=1.5692, min=0.0000, max=8.3090
Batch 0 Pearson correlation: 0.5161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5173
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5101
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5156
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5266
2025-11-14 16:50:42,091 - INFO - Fold 1 Train Epoch 2/200, Batch 50, Loss: 9.2257, Pearson: 0.5482, Spearman: 0.5300
2025-11-14 16:50:47,978 - INFO - Fold 1 Train Epoch 2/200, Batch 60, Loss: 8.8476, Pearson: 0.5432, Spearman: 0.5156
2025-11-14 16:50:53,900 - INFO - Fold 1 Train Epoch 2/200, Batch 70, Loss: 8.9155, Pearson: 0.5451, Spearman: 0.5256
2025-11-14 16:50:59,842 - INFO - Fold 1 Train Epoch 2/200, Batch 80, Loss: 8.6170, Pearson: 0.5513, Spearman: 0.5314
2025-11-14 16:51:03,112 - INFO - Fold 1 Train Epoch 2/200, Train Loss: 8.9226, Pearson Mean: 0.5311, Spearman Mean: 0.5127
2025-11-14 16:51:03,112 - INFO - Training Metrics: {'pearson_mean_genewise': 0.2943, 'spearman_mean_genewise': 0.275, 'l1_error_mean': 2.2012, 'l2_errors_mean': 8.9241, 'r2_scores_mean': 0.0652, 'pearson_std': 0.0964, 'l2_error_q1': 5.5223, 'l2_error_q2': 8.1049, 'l2_error_q3': 11.9973, 'r2_score_q1': 0.0403, 'r2_score_q2': 0.0685, 'r2_score_q3': 0.1115, 'mape_mean': 67.2631, 'mape_std': 15.9, 'rmse_mean': 2.9145, 'rmse_std': 0.6554}
2025-11-14 16:51:03,361 - INFO - Fold 1 Val Epoch 2/200, Batch 0, Loss: 9.4767, Pearson: 0.5105, Spearman: 0.4958
2025-11-14 16:51:04,587 - INFO - Fold 1 Val Epoch 2/200, Batch 10, Loss: 7.9045, Pearson: 0.5836, Spearman: 0.5919
2025-11-14 16:51:05,810 - INFO - Fold 1 Val Epoch 2/200, Batch 20, Loss: 6.9348, Pearson: 0.3142, Spearman: 0.3651
2025-11-14 16:51:08,176 - INFO - Fold 1 Val Epoch 2/200, Val Loss: 8.5482, Pearson Mean: 0.4704, Spearman Mean: 0.4843
2025-11-14 16:51:08,176 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2698, 'spearman_mean_genewise': 0.2432, 'l1_error_mean': 2.0389, 'l2_errors_mean': 8.5649, 'r2_scores_mean': -0.0476, 'pearson_std': 0.0968, 'l2_error_q1': 4.7064, 'l2_error_q2': 7.2997, 'l2_error_q3': 11.6959, 'r2_score_q1': 0.0125, 'r2_score_q2': 0.052, 'r2_score_q3': 0.095, 'mape_mean': 67.3286, 'mape_std': 16.0803, 'rmse_mean': 2.8209, 'rmse_std': 0.7793}
2025-11-14 16:51:08,176 - INFO - Learning rate for epoch 2: 0.0001
2025-11-14 16:51:08,236 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_1/best_model.pth
2025-11-14 16:51:08,951 - INFO - Fold 1 Train Epoch 3/200, Batch 0, Loss: 8.4204, Pearson: 0.5509, Spearman: 0.5240
2025-11-14 16:51:14,924 - INFO - Fold 1 Train Epoch 3/200, Batch 10, Loss: 8.7654, Pearson: 0.5631, Spearman: 0.5354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5441
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.5328
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.924115180969238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 3 =====================
Sample y_true values (first sample, first 10 genes):
[0.        6.8811674 6.189047  7.28629   7.573801  6.189047  6.189047
 6.189047  6.8811674 9.182828 ]
Sample y_pred values (first sample, first 10 genes):
[ 1.3661726  3.9979646  3.2786365  2.1041903  6.741805   4.9576707
  1.2018757  3.4459457  4.7423315 10.39558  ]
y_true  -> mean=2.0594, std=3.4742, min=0.0000, max=12.9888
y_pred  -> mean=1.9810, std=2.0031, min=0.0000, max=12.7258
Batch 0 Pearson correlation: 0.5509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5617
2025-11-14 16:51:20,832 - INFO - Fold 1 Train Epoch 3/200, Batch 20, Loss: 9.1173, Pearson: 0.5358, Spearman: 0.5334
2025-11-14 16:51:26,760 - INFO - Fold 1 Train Epoch 3/200, Batch 30, Loss: 8.6332, Pearson: 0.5591, Spearman: 0.5380
2025-11-14 16:51:32,749 - INFO - Fold 1 Train Epoch 3/200, Batch 40, Loss: 8.4904, Pearson: 0.5604, Spearman: 0.5325
2025-11-14 16:51:38,762 - INFO - Fold 1 Train Epoch 3/200, Batch 50, Loss: 8.2964, Pearson: 0.5615, Spearman: 0.5359
2025-11-14 16:51:44,763 - INFO - Fold 1 Train Epoch 3/200, Batch 60, Loss: 8.6037, Pearson: 0.5780, Spearman: 0.5527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5707
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5694
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5699
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5742
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5780
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5720
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5763
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5712
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5740
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5751
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5789
2025-11-14 16:51:50,771 - INFO - Fold 1 Train Epoch 3/200, Batch 70, Loss: 8.3615, Pearson: 0.5683, Spearman: 0.5386
2025-11-14 16:51:56,847 - INFO - Fold 1 Train Epoch 3/200, Batch 80, Loss: 8.4832, Pearson: 0.5647, Spearman: 0.5457
2025-11-14 16:52:00,043 - INFO - Fold 1 Train Epoch 3/200, Train Loss: 8.5205, Pearson Mean: 0.5594, Spearman Mean: 0.5364
2025-11-14 16:52:00,043 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3223, 'spearman_mean_genewise': 0.2981, 'l1_error_mean': 2.1372, 'l2_errors_mean': 8.5177, 'r2_scores_mean': 0.0993, 'pearson_std': 0.1018, 'l2_error_q1': 5.4351, 'l2_error_q2': 7.9439, 'l2_error_q3': 11.4634, 'r2_score_q1': 0.0554, 'r2_score_q2': 0.0873, 'r2_score_q3': 0.1356, 'mape_mean': 65.7056, 'mape_std': 16.7956, 'rmse_mean': 2.8574, 'rmse_std': 0.5942}
2025-11-14 16:52:00,294 - INFO - Fold 1 Val Epoch 3/200, Batch 0, Loss: 9.2004, Pearson: 0.5293, Spearman: 0.5055
2025-11-14 16:52:01,609 - INFO - Fold 1 Val Epoch 3/200, Batch 10, Loss: 7.7971, Pearson: 0.5982, Spearman: 0.6054
2025-11-14 16:52:02,918 - INFO - Fold 1 Val Epoch 3/200, Batch 20, Loss: 7.2778, Pearson: 0.2761, Spearman: 0.3291
2025-11-14 16:52:05,256 - INFO - Fold 1 Val Epoch 3/200, Val Loss: 8.0943, Pearson Mean: 0.5072, Spearman Mean: 0.5120
2025-11-14 16:52:05,256 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2945, 'spearman_mean_genewise': 0.2636, 'l1_error_mean': 2.0159, 'l2_errors_mean': 8.109, 'r2_scores_mean': 0.016, 'pearson_std': 0.1015, 'l2_error_q1': 4.632, 'l2_error_q2': 7.2148, 'l2_error_q3': 11.1919, 'r2_score_q1': 0.0327, 'r2_score_q2': 0.0732, 'r2_score_q3': 0.1207, 'mape_mean': 66.7165, 'mape_std': 16.4448, 'rmse_mean': 2.7579, 'rmse_std': 0.7093}
2025-11-14 16:52:05,256 - INFO - Learning rate for epoch 3: 0.0001
2025-11-14 16:52:05,316 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_1/best_model.pth
2025-11-14 16:52:06,043 - INFO - Fold 1 Train Epoch 4/200, Batch 0, Loss: 8.1731, Pearson: 0.5611, Spearman: 0.5401
2025-11-14 16:52:11,987 - INFO - Fold 1 Train Epoch 4/200, Batch 10, Loss: 8.3886, Pearson: 0.5774, Spearman: 0.5464
2025-11-14 16:52:17,879 - INFO - Fold 1 Train Epoch 4/200, Batch 20, Loss: 8.6234, Pearson: 0.5730, Spearman: 0.5513
2025-11-14 16:52:23,833 - INFO - Fold 1 Train Epoch 4/200, Batch 30, Loss: 8.3116, Pearson: 0.5839, Spearman: 0.5477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5683
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5690
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5705
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5745
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5805
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5600
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.5545
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.517745018005371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 4 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        7.780447  0.        0.
 0.        7.0877175 9.032911 ]
Sample y_pred values (first sample, first 10 genes):
[0.40692073 2.8645425  2.5742586  0.92496586 5.689881   3.4408529
 0.5667668  2.1393495  3.5157785  8.676329  ]
y_true  -> mean=1.9864, std=3.4535, min=0.0000, max=12.7169
y_pred  -> mean=2.0278, std=1.9264, min=0.0000, max=12.6635
Batch 0 Pearson correlation: 0.5611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5718
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5718
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5792
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5726
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5688
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5793
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5782
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5774
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5663
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5732
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5837
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5684
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5717
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5869
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5791
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5766
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5730
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5710
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5777
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5728
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5849
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5832
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5825
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5839
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5745
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5802
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5828
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5740
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5723
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5740
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5713
2025-11-14 16:52:29,737 - INFO - Fold 1 Train Epoch 4/200, Batch 40, Loss: 8.4890, Pearson: 0.5738, Spearman: 0.5553
2025-11-14 16:52:35,759 - INFO - Fold 1 Train Epoch 4/200, Batch 50, Loss: 8.3518, Pearson: 0.5795, Spearman: 0.5520
2025-11-14 16:52:41,806 - INFO - Fold 1 Train Epoch 4/200, Batch 60, Loss: 8.5263, Pearson: 0.5780, Spearman: 0.5594
2025-11-14 16:52:47,827 - INFO - Fold 1 Train Epoch 4/200, Batch 70, Loss: 8.0357, Pearson: 0.5927, Spearman: 0.5535
2025-11-14 16:52:53,725 - INFO - Fold 1 Train Epoch 4/200, Batch 80, Loss: 8.1628, Pearson: 0.5806, Spearman: 0.5537
2025-11-14 16:52:57,047 - INFO - Fold 1 Train Epoch 4/200, Train Loss: 8.2487, Pearson Mean: 0.5773, Spearman Mean: 0.5492
2025-11-14 16:52:57,047 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3448, 'spearman_mean_genewise': 0.3169, 'l1_error_mean': 2.0957, 'l2_errors_mean': 8.2475, 'r2_scores_mean': 0.1225, 'pearson_std': 0.1056, 'l2_error_q1': 5.3821, 'l2_error_q2': 7.816, 'l2_error_q3': 11.0424, 'r2_score_q1': 0.0698, 'r2_score_q2': 0.1055, 'r2_score_q3': 0.1577, 'mape_mean': 63.8012, 'mape_std': 17.4784, 'rmse_mean': 2.8161, 'rmse_std': 0.563}
2025-11-14 16:52:57,297 - INFO - Fold 1 Val Epoch 4/200, Batch 0, Loss: 9.0695, Pearson: 0.5402, Spearman: 0.5170
2025-11-14 16:52:58,524 - INFO - Fold 1 Val Epoch 4/200, Batch 10, Loss: 7.5612, Pearson: 0.5995, Spearman: 0.6074
2025-11-14 16:52:59,747 - INFO - Fold 1 Val Epoch 4/200, Batch 20, Loss: 6.8198, Pearson: 0.3476, Spearman: 0.3916
2025-11-14 16:53:02,059 - INFO - Fold 1 Val Epoch 4/200, Val Loss: 7.9653, Pearson Mean: 0.5203, Spearman Mean: 0.5233
2025-11-14 16:53:02,060 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2976, 'spearman_mean_genewise': 0.2669, 'l1_error_mean': 1.9948, 'l2_errors_mean': 7.9738, 'r2_scores_mean': 0.0427, 'pearson_std': 0.1005, 'l2_error_q1': 4.623, 'l2_error_q2': 7.224, 'l2_error_q3': 11.2113, 'r2_score_q1': 0.0378, 'r2_score_q2': 0.076, 'r2_score_q3': 0.1217, 'mape_mean': 66.5181, 'mape_std': 17.2578, 'rmse_mean': 2.7393, 'rmse_std': 0.6855}
2025-11-14 16:53:02,060 - INFO - Learning rate for epoch 4: 0.0001
2025-11-14 16:53:02,121 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_1/best_model.pth
2025-11-14 16:53:02,826 - INFO - Fold 1 Train Epoch 5/200, Batch 0, Loss: 8.1821, Pearson: 0.5871, Spearman: 0.5527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5788
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5738
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5822
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5917
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5831
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5785
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5846
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5796
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5821
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5902
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5735
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5795
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5826
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5863
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5926
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5864
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5824
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5708
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5951
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5774
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5736
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5780
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5718
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5808
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5913
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5889
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5729
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5817
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5756
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5724
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5854
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5927
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5738
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5796
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5819
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5787
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5818
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5810
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5844
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5776
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5806
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5744
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5809
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5594
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.5817
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.247525215148926
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 5 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.894305 0.       0.       0.       0.       0.
 0.       0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.52130556 0.36949816 1.530436   0.9430074  2.5523925  1.2025836
 0.53435403 1.6336676  2.6626546  6.4492745 ]
y_true  -> mean=2.1680, std=3.5331, min=0.0000, max=12.5075
y_pred  -> mean=2.1338, std=2.0730, min=0.0000, max=13.7512
Batch 0 Pearson correlation: 0.5871
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5794
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5946
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5773
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5842
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5758
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5897
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5774
2025-11-14 16:53:08,789 - INFO - Fold 1 Train Epoch 5/200, Batch 10, Loss: 8.0536, Pearson: 0.5742, Spearman: 0.5483
2025-11-14 16:53:14,799 - INFO - Fold 1 Train Epoch 5/200, Batch 20, Loss: 8.0551, Pearson: 0.5896, Spearman: 0.5551
2025-11-14 16:53:20,810 - INFO - Fold 1 Train Epoch 5/200, Batch 30, Loss: 8.0927, Pearson: 0.5877, Spearman: 0.5594
2025-11-14 16:53:26,784 - INFO - Fold 1 Train Epoch 5/200, Batch 40, Loss: 8.0563, Pearson: 0.5714, Spearman: 0.5446
2025-11-14 16:53:32,758 - INFO - Fold 1 Train Epoch 5/200, Batch 50, Loss: 8.1637, Pearson: 0.5867, Spearman: 0.5561
2025-11-14 16:53:38,742 - INFO - Fold 1 Train Epoch 5/200, Batch 60, Loss: 8.1422, Pearson: 0.5930, Spearman: 0.5560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5970
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5834
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5742
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5830
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5781
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5895
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5896
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5957
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5795
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5872
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5901
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5867
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5896
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5733
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5912
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5903
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5798
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5896
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5894
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5746
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5680
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5879
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5877
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5751
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5933
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5825
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5950
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5834
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5908
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5975
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5943
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6014
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5897
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5896
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5901
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6020
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5803
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5910
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5965
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5938
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5867
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5972
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5964
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5926
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5926
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5984
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5931
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5991
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5885
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5915
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5930
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5939
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5975
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5845
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5828
2025-11-14 16:53:44,710 - INFO - Fold 1 Train Epoch 5/200, Batch 70, Loss: 8.0392, Pearson: 0.6025, Spearman: 0.5659
2025-11-14 16:53:50,681 - INFO - Fold 1 Train Epoch 5/200, Batch 80, Loss: 7.8767, Pearson: 0.5937, Spearman: 0.5594
2025-11-14 16:53:53,962 - INFO - Fold 1 Train Epoch 5/200, Train Loss: 8.0654, Pearson Mean: 0.5899, Spearman Mean: 0.5580
2025-11-14 16:53:53,962 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3626, 'spearman_mean_genewise': 0.3307, 'l1_error_mean': 2.0605, 'l2_errors_mean': 8.0643, 'r2_scores_mean': 0.1385, 'pearson_std': 0.1078, 'l2_error_q1': 5.3335, 'l2_error_q2': 7.6971, 'l2_error_q3': 10.7226, 'r2_score_q1': 0.0792, 'r2_score_q2': 0.1184, 'r2_score_q3': 0.1732, 'mape_mean': 62.9588, 'mape_std': 17.6048, 'rmse_mean': 2.7871, 'rmse_std': 0.5446}
2025-11-14 16:53:54,212 - INFO - Fold 1 Val Epoch 5/200, Batch 0, Loss: 8.9927, Pearson: 0.5425, Spearman: 0.5222
2025-11-14 16:53:55,447 - INFO - Fold 1 Val Epoch 5/200, Batch 10, Loss: 7.5074, Pearson: 0.6083, Spearman: 0.6165
2025-11-14 16:53:56,686 - INFO - Fold 1 Val Epoch 5/200, Batch 20, Loss: 7.3011, Pearson: 0.2877, Spearman: 0.3405
2025-11-14 16:53:59,009 - INFO - Fold 1 Val Epoch 5/200, Val Loss: 8.1278, Pearson Mean: 0.5050, Spearman Mean: 0.5160
2025-11-14 16:53:59,009 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2921, 'spearman_mean_genewise': 0.2643, 'l1_error_mean': 1.9957, 'l2_errors_mean': 8.1536, 'r2_scores_mean': 0.0153, 'pearson_std': 0.0986, 'l2_error_q1': 4.6518, 'l2_error_q2': 7.2321, 'l2_error_q3': 11.2817, 'r2_score_q1': 0.0295, 'r2_score_q2': 0.0662, 'r2_score_q3': 0.1112, 'mape_mean': 65.0381, 'mape_std': 17.0605, 'rmse_mean': 2.765, 'rmse_std': 0.713}
2025-11-14 16:53:59,009 - INFO - Learning rate for epoch 5: 0.0001
2025-11-14 16:53:59,009 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 16:53:59,722 - INFO - Fold 1 Train Epoch 6/200, Batch 0, Loss: 8.0232, Pearson: 0.5971, Spearman: 0.5607
2025-11-14 16:54:05,782 - INFO - Fold 1 Train Epoch 6/200, Batch 10, Loss: 8.0976, Pearson: 0.5964, Spearman: 0.5610
2025-11-14 16:54:11,794 - INFO - Fold 1 Train Epoch 6/200, Batch 20, Loss: 8.1352, Pearson: 0.6024, Spearman: 0.5628
2025-11-14 16:54:17,819 - INFO - Fold 1 Train Epoch 6/200, Batch 30, Loss: 7.8610, Pearson: 0.6006, Spearman: 0.5650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5883
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5855
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5916
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5973
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6025
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5987
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6020
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5981
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5686
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5945
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6038
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5976
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5958
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6021
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5937
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5996
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6045
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6033
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.5852
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.064294815063477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 6 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.4258657 0.        0.        0.        0.
 0.        8.524081  8.524081 ]
Sample y_pred values (first sample, first 10 genes):
[0.5795295  0.8088014  1.5207785  1.3902788  2.9994442  1.9174936
 0.74322945 2.3216085  3.1990333  6.8761034 ]
y_true  -> mean=2.1584, std=3.5208, min=0.0000, max=12.6548
y_pred  -> mean=1.9829, std=1.9775, min=0.0000, max=14.2218
Batch 0 Pearson correlation: 0.5971
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6001
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5937
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5930
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6005
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5877
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5957
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5871
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5872
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5964
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5919
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5955
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6068
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5923
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5911
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6062
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6022
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6062
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5906
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5888
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5993
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5847
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6012
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5926
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5930
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5988
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5924
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6018
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6085
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6029
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6065
2025-11-14 16:54:23,847 - INFO - Fold 1 Train Epoch 6/200, Batch 40, Loss: 7.8432, Pearson: 0.5974, Spearman: 0.5624
2025-11-14 16:54:29,854 - INFO - Fold 1 Train Epoch 6/200, Batch 50, Loss: 8.1339, Pearson: 0.5911, Spearman: 0.5576
2025-11-14 16:54:35,914 - INFO - Fold 1 Train Epoch 6/200, Batch 60, Loss: 7.8878, Pearson: 0.5891, Spearman: 0.5621
2025-11-14 16:54:41,913 - INFO - Fold 1 Train Epoch 6/200, Batch 70, Loss: 7.9636, Pearson: 0.5972, Spearman: 0.5617
2025-11-14 16:54:47,925 - INFO - Fold 1 Train Epoch 6/200, Batch 80, Loss: 7.9694, Pearson: 0.5947, Spearman: 0.5650
2025-11-14 16:54:51,286 - INFO - Fold 1 Train Epoch 6/200, Train Loss: 7.9394, Pearson Mean: 0.5981, Spearman Mean: 0.5636
2025-11-14 16:54:51,286 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3754, 'spearman_mean_genewise': 0.3415, 'l1_error_mean': 2.0254, 'l2_errors_mean': 7.9372, 'r2_scores_mean': 0.1497, 'pearson_std': 0.1095, 'l2_error_q1': 5.3021, 'l2_error_q2': 7.6208, 'l2_error_q3': 10.456, 'r2_score_q1': 0.0854, 'r2_score_q2': 0.1275, 'r2_score_q3': 0.1849, 'mape_mean': 61.9539, 'mape_std': 17.9085, 'rmse_mean': 2.7664, 'rmse_std': 0.5333}
2025-11-14 16:54:51,536 - INFO - Fold 1 Val Epoch 6/200, Batch 0, Loss: 8.8550, Pearson: 0.5535, Spearman: 0.5326
2025-11-14 16:54:52,773 - INFO - Fold 1 Val Epoch 6/200, Batch 10, Loss: 7.5036, Pearson: 0.6078, Spearman: 0.6160
2025-11-14 16:54:54,012 - INFO - Fold 1 Val Epoch 6/200, Batch 20, Loss: 6.9472, Pearson: 0.3573, Spearman: 0.4001
2025-11-14 16:54:56,356 - INFO - Fold 1 Val Epoch 6/200, Val Loss: 7.8008, Pearson Mean: 0.5371, Spearman Mean: 0.5400
2025-11-14 16:54:56,357 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3056, 'spearman_mean_genewise': 0.2781, 'l1_error_mean': 2.0181, 'l2_errors_mean': 7.8261, 'r2_scores_mean': 0.0648, 'pearson_std': 0.1007, 'l2_error_q1': 4.6258, 'l2_error_q2': 7.1788, 'l2_error_q3': 10.8804, 'r2_score_q1': 0.0381, 'r2_score_q2': 0.0758, 'r2_score_q3': 0.1235, 'mape_mean': 63.0016, 'mape_std': 17.7472, 'rmse_mean': 2.7181, 'rmse_std': 0.6617}
2025-11-14 16:54:56,357 - INFO - Learning rate for epoch 6: 0.0001
2025-11-14 16:54:56,418 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_1/best_model.pth
2025-11-14 16:54:57,176 - INFO - Fold 1 Train Epoch 7/200, Batch 0, Loss: 7.9499, Pearson: 0.6100, Spearman: 0.5708
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5950
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5917
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5986
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5891
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5999
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5974
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5991
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5971
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5999
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6029
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5909
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6013
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6022
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5958
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5911
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5935
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5827
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5978
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5972
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6048
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5841
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6005
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6043
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5891
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6003
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6044
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5909
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5943
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5969
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6032
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5910
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5972
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6094
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5982
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5958
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5949
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5983
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5913
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5961
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5947
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6058
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5970
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6074
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.937178134918213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 7 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.5152698 0.        7.5152698 8.901155  0.
 0.        0.        9.124271 ]
Sample y_pred values (first sample, first 10 genes):
[0.6973759  0.02351312 1.4595785  1.3156745  2.3344927  1.5391414
 0.6499368  2.6698117  3.0471349  6.3479404 ]
y_true  -> mean=2.2432, std=3.5551, min=0.0000, max=12.5441
y_pred  -> mean=2.1272, std=2.1478, min=0.0000, max=15.9237
Batch 0 Pearson correlation: 0.6100
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5908
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6087
2025-11-14 16:55:03,183 - INFO - Fold 1 Train Epoch 7/200, Batch 10, Loss: 7.8280, Pearson: 0.5964, Spearman: 0.5582
2025-11-14 16:55:09,124 - INFO - Fold 1 Train Epoch 7/200, Batch 20, Loss: 8.1112, Pearson: 0.6098, Spearman: 0.5753
2025-11-14 16:55:15,139 - INFO - Fold 1 Train Epoch 7/200, Batch 30, Loss: 7.9473, Pearson: 0.6152, Spearman: 0.5772
2025-11-14 16:55:21,124 - INFO - Fold 1 Train Epoch 7/200, Batch 40, Loss: 7.9889, Pearson: 0.5888, Spearman: 0.5551
2025-11-14 16:55:27,109 - INFO - Fold 1 Train Epoch 7/200, Batch 50, Loss: 7.6744, Pearson: 0.6135, Spearman: 0.5797
2025-11-14 16:55:33,097 - INFO - Fold 1 Train Epoch 7/200, Batch 60, Loss: 7.7364, Pearson: 0.6110, Spearman: 0.5780
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6061
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6117
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6130
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5970
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5964
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6040
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6070
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5987
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6062
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6015
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6013
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5914
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6070
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6152
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5977
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6090
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5971
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5888
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6059
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6137
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6090
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5915
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6032
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6100
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6070
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5958
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6074
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6010
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5890
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6060
2025-11-14 16:55:39,092 - INFO - Fold 1 Train Epoch 7/200, Batch 70, Loss: 7.7626, Pearson: 0.6254, Spearman: 0.5830
2025-11-14 16:55:45,140 - INFO - Fold 1 Train Epoch 7/200, Batch 80, Loss: 8.0355, Pearson: 0.5921, Spearman: 0.5634
2025-11-14 16:55:48,500 - INFO - Fold 1 Train Epoch 7/200, Train Loss: 7.8231, Pearson Mean: 0.6051, Spearman Mean: 0.5697
2025-11-14 16:55:48,500 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3853, 'spearman_mean_genewise': 0.3493, 'l1_error_mean': 2.0136, 'l2_errors_mean': 7.8232, 'r2_scores_mean': 0.1592, 'pearson_std': 0.1123, 'l2_error_q1': 5.2688, 'l2_error_q2': 7.4674, 'l2_error_q3': 10.2709, 'r2_score_q1': 0.0917, 'r2_score_q2': 0.137, 'r2_score_q3': 0.1959, 'mape_mean': 61.2558, 'mape_std': 17.9927, 'rmse_mean': 2.7478, 'rmse_std': 0.5223}
2025-11-14 16:55:48,754 - INFO - Fold 1 Val Epoch 7/200, Batch 0, Loss: 8.8904, Pearson: 0.5523, Spearman: 0.5357
2025-11-14 16:55:49,994 - INFO - Fold 1 Val Epoch 7/200, Batch 10, Loss: 7.5197, Pearson: 0.6037, Spearman: 0.6153
2025-11-14 16:55:51,234 - INFO - Fold 1 Val Epoch 7/200, Batch 20, Loss: 7.0153, Pearson: 0.3593, Spearman: 0.4025
2025-11-14 16:55:53,558 - INFO - Fold 1 Val Epoch 7/200, Val Loss: 7.9091, Pearson Mean: 0.5311, Spearman Mean: 0.5381
2025-11-14 16:55:53,559 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.294, 'spearman_mean_genewise': 0.2732, 'l1_error_mean': 2.02, 'l2_errors_mean': 7.9337, 'r2_scores_mean': 0.0541, 'pearson_std': 0.1009, 'l2_error_q1': 4.6145, 'l2_error_q2': 7.233, 'l2_error_q3': 11.0776, 'r2_score_q1': 0.0335, 'r2_score_q2': 0.069, 'r2_score_q3': 0.1155, 'mape_mean': 63.5358, 'mape_std': 18.3454, 'rmse_mean': 2.7341, 'rmse_std': 0.6773}
2025-11-14 16:55:53,559 - INFO - Learning rate for epoch 7: 0.0001
2025-11-14 16:55:53,559 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 16:55:54,275 - INFO - Fold 1 Train Epoch 8/200, Batch 0, Loss: 7.7028, Pearson: 0.6015, Spearman: 0.5705
2025-11-14 16:56:00,261 - INFO - Fold 1 Train Epoch 8/200, Batch 10, Loss: 7.9045, Pearson: 0.6164, Spearman: 0.5743
2025-11-14 16:56:06,240 - INFO - Fold 1 Train Epoch 8/200, Batch 20, Loss: 7.8196, Pearson: 0.6046, Spearman: 0.5754
2025-11-14 16:56:12,206 - INFO - Fold 1 Train Epoch 8/200, Batch 30, Loss: 7.7686, Pearson: 0.6093, Spearman: 0.5718
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6063
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6080
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6020
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6040
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6031
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6051
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5956
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6064
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5960
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5921
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5989
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6029
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6068
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.5575
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.823193550109863
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 8 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       9.811267]
Sample y_pred values (first sample, first 10 genes):
[0.23455673 1.6662797  1.9584799  0.23270428 3.485831   1.7674575
 0.27298912 1.0451225  2.472478   7.8722086 ]
y_true  -> mean=2.0120, std=3.4653, min=0.0000, max=12.6515
y_pred  -> mean=2.1826, std=2.1853, min=0.0000, max=14.7657
Batch 0 Pearson correlation: 0.6015
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5966
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5910
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6019
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6005
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5926
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6068
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6069
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6054
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6051
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6023
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6005
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5988
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6046
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6089
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6069
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6005
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6065
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6093
2025-11-14 16:56:18,149 - INFO - Fold 1 Train Epoch 8/200, Batch 40, Loss: 7.8236, Pearson: 0.6041, Spearman: 0.5705
2025-11-14 16:56:24,123 - INFO - Fold 1 Train Epoch 8/200, Batch 50, Loss: 7.8493, Pearson: 0.6234, Spearman: 0.5816
2025-11-14 16:56:30,091 - INFO - Fold 1 Train Epoch 8/200, Batch 60, Loss: 7.7149, Pearson: 0.6099, Spearman: 0.5733
2025-11-14 16:56:36,106 - INFO - Fold 1 Train Epoch 8/200, Batch 70, Loss: 7.4666, Pearson: 0.6125, Spearman: 0.5678
2025-11-14 16:56:42,172 - INFO - Fold 1 Train Epoch 8/200, Batch 80, Loss: 7.8867, Pearson: 0.5980, Spearman: 0.5623
2025-11-14 16:56:45,530 - INFO - Fold 1 Train Epoch 8/200, Train Loss: 7.7899, Pearson Mean: 0.6075, Spearman Mean: 0.5723
2025-11-14 16:56:45,530 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3881, 'spearman_mean_genewise': 0.3526, 'l1_error_mean': 1.9958, 'l2_errors_mean': 7.793, 'r2_scores_mean': 0.1618, 'pearson_std': 0.1131, 'l2_error_q1': 5.2598, 'l2_error_q2': 7.4505, 'l2_error_q3': 10.2094, 'r2_score_q1': 0.0937, 'r2_score_q2': 0.1378, 'r2_score_q3': 0.1972, 'mape_mean': 61.3331, 'mape_std': 18.182, 'rmse_mean': 2.7427, 'rmse_std': 0.52}
2025-11-14 16:56:45,781 - INFO - Fold 1 Val Epoch 8/200, Batch 0, Loss: 8.7839, Pearson: 0.5579, Spearman: 0.5400
2025-11-14 16:56:47,014 - INFO - Fold 1 Val Epoch 8/200, Batch 10, Loss: 7.6957, Pearson: 0.6062, Spearman: 0.6174
2025-11-14 16:56:48,251 - INFO - Fold 1 Val Epoch 8/200, Batch 20, Loss: 6.2609, Pearson: 0.4292, Spearman: 0.4585
2025-11-14 16:56:50,592 - INFO - Fold 1 Val Epoch 8/200, Val Loss: 7.6100, Pearson Mean: 0.5525, Spearman Mean: 0.5545
2025-11-14 16:56:50,592 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3192, 'spearman_mean_genewise': 0.2866, 'l1_error_mean': 1.9116, 'l2_errors_mean': 7.6381, 'r2_scores_mean': 0.0891, 'pearson_std': 0.1042, 'l2_error_q1': 4.5801, 'l2_error_q2': 7.0144, 'l2_error_q3': 10.5895, 'r2_score_q1': 0.0475, 'r2_score_q2': 0.0879, 'r2_score_q3': 0.1357, 'mape_mean': 65.4108, 'mape_std': 17.28, 'rmse_mean': 2.6884, 'rmse_std': 0.6409}
2025-11-14 16:56:50,592 - INFO - Learning rate for epoch 8: 0.0001
2025-11-14 16:56:50,645 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_1/best_model.pth
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6032
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6070
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5994
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6027
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6078
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5974
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6041
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6059
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6118
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6090
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6020
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6041
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5931
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6054
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6148
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6009
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6119
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6099
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5980
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6121
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6085
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6000
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6095
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6101
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5951
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5950
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6018
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5980
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6073
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6128
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6080
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.792964458465576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 9 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 7.9437475 9.042124  7.9437475]
Sample y_pred values (first sample, first 10 genes):
[0.69022393 0.         0.96726525 1.6355217  1.7213454  1.330593
 0.63728714 2.3834572  2.534241   5.002452  ]
2025-11-14 16:56:51,353 - INFO - Fold 1 Train Epoch 9/200, Batch 0, Loss: 7.8282, Pearson: 0.6191, Spearman: 0.5899
2025-11-14 16:56:57,310 - INFO - Fold 1 Train Epoch 9/200, Batch 10, Loss: 7.6034, Pearson: 0.6140, Spearman: 0.5835
2025-11-14 16:57:03,301 - INFO - Fold 1 Train Epoch 9/200, Batch 20, Loss: 7.6812, Pearson: 0.6114, Spearman: 0.5760
2025-11-14 16:57:09,334 - INFO - Fold 1 Train Epoch 9/200, Batch 30, Loss: 7.5470, Pearson: 0.6144, Spearman: 0.5793
2025-11-14 16:57:15,341 - INFO - Fold 1 Train Epoch 9/200, Batch 40, Loss: 7.6532, Pearson: 0.6116, Spearman: 0.5787
2025-11-14 16:57:21,318 - INFO - Fold 1 Train Epoch 9/200, Batch 50, Loss: 7.6866, Pearson: 0.6158, Spearman: 0.5866
y_true  -> mean=2.2643, std=3.5578, min=0.0000, max=12.5033
y_pred  -> mean=2.1176, std=2.1981, min=0.0000, max=15.6363
Batch 0 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6002
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6084
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6054
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6121
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6165
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5977
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6021
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6063
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6091
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6099
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6086
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6089
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6052
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6111
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6283
2025-11-14 16:57:27,340 - INFO - Fold 1 Train Epoch 9/200, Batch 60, Loss: 7.7376, Pearson: 0.6096, Spearman: 0.5764
2025-11-14 16:57:33,396 - INFO - Fold 1 Train Epoch 9/200, Batch 70, Loss: 7.7386, Pearson: 0.6217, Spearman: 0.5806
2025-11-14 16:57:39,516 - INFO - Fold 1 Train Epoch 9/200, Batch 80, Loss: 7.9920, Pearson: 0.6065, Spearman: 0.5811
2025-11-14 16:57:42,899 - INFO - Fold 1 Train Epoch 9/200, Train Loss: 7.6760, Pearson Mean: 0.6150, Spearman Mean: 0.5782
2025-11-14 16:57:42,899 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3992, 'spearman_mean_genewise': 0.3619, 'l1_error_mean': 1.9774, 'l2_errors_mean': 7.6782, 'r2_scores_mean': 0.1716, 'pearson_std': 0.1154, 'l2_error_q1': 5.2234, 'l2_error_q2': 7.2863, 'l2_error_q3': 9.9475, 'r2_score_q1': 0.0995, 'r2_score_q2': 0.1459, 'r2_score_q3': 0.2116, 'mape_mean': 60.611, 'mape_std': 18.2338, 'rmse_mean': 2.7236, 'rmse_std': 0.5103}
2025-11-14 16:57:43,165 - INFO - Fold 1 Val Epoch 9/200, Batch 0, Loss: 8.7001, Pearson: 0.5680, Spearman: 0.5520
2025-11-14 16:57:44,398 - INFO - Fold 1 Val Epoch 9/200, Batch 10, Loss: 7.5396, Pearson: 0.6094, Spearman: 0.6194
2025-11-14 16:57:45,633 - INFO - Fold 1 Val Epoch 9/200, Batch 20, Loss: 5.8157, Pearson: 0.5084, Spearman: 0.5180
2025-11-14 16:57:47,994 - INFO - Fold 1 Val Epoch 9/200, Val Loss: 7.5266, Pearson Mean: 0.5634, Spearman Mean: 0.5635
2025-11-14 16:57:47,994 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3202, 'spearman_mean_genewise': 0.2892, 'l1_error_mean': 1.9488, 'l2_errors_mean': 7.5468, 'r2_scores_mean': 0.1028, 'pearson_std': 0.1075, 'l2_error_q1': 4.5263, 'l2_error_q2': 7.0016, 'l2_error_q3': 10.4558, 'r2_score_q1': 0.051, 'r2_score_q2': 0.0903, 'r2_score_q3': 0.1398, 'mape_mean': 63.1527, 'mape_std': 18.6944, 'rmse_mean': 2.673, 'rmse_std': 0.6337}
2025-11-14 16:57:47,994 - INFO - Learning rate for epoch 9: 0.0001
2025-11-14 16:57:48,055 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_1/best_model.pth
2025-11-14 16:57:48,786 - INFO - Fold 1 Train Epoch 10/200, Batch 0, Loss: 7.6569, Pearson: 0.6144, Spearman: 0.5790
2025-11-14 16:57:54,835 - INFO - Fold 1 Train Epoch 10/200, Batch 10, Loss: 7.5988, Pearson: 0.6131, Spearman: 0.5813
2025-11-14 16:58:00,929 - INFO - Fold 1 Train Epoch 10/200, Batch 20, Loss: 7.6698, Pearson: 0.6287, Spearman: 0.5842
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6044
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6065
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6204
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6349
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.678173065185547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 10 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       7.419181 0.       0.       0.       0.
 8.517393 7.419181]
Sample y_pred values (first sample, first 10 genes):
[0.         0.2622203  0.8862626  0.38869485 2.8984585  0.9494461
 0.3179255  0.78007776 3.2966628  7.2473793 ]
y_true  -> mean=2.0982, std=3.5068, min=0.0000, max=13.8155
y_pred  -> mean=2.0853, std=2.1317, min=0.0000, max=15.5021
Batch 0 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6073
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6156
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6095
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6051
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5950
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6074
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6011
2025-11-14 16:58:06,986 - INFO - Fold 1 Train Epoch 10/200, Batch 30, Loss: 7.6691, Pearson: 0.6281, Spearman: 0.5837
2025-11-14 16:58:13,082 - INFO - Fold 1 Train Epoch 10/200, Batch 40, Loss: 7.8438, Pearson: 0.6083, Spearman: 0.5674
2025-11-14 16:58:19,106 - INFO - Fold 1 Train Epoch 10/200, Batch 50, Loss: 7.8412, Pearson: 0.6190, Spearman: 0.5846
2025-11-14 16:58:25,144 - INFO - Fold 1 Train Epoch 10/200, Batch 60, Loss: 7.7614, Pearson: 0.6132, Spearman: 0.5810
2025-11-14 16:58:31,141 - INFO - Fold 1 Train Epoch 10/200, Batch 70, Loss: 7.4560, Pearson: 0.6254, Spearman: 0.5899
2025-11-14 16:58:37,134 - INFO - Fold 1 Train Epoch 10/200, Batch 80, Loss: 7.7166, Pearson: 0.6220, Spearman: 0.5897
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6054
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6028
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6040
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6119
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6082
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6070
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6080
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6074
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6118
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6070
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6103
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6060
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.5928
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
2025-11-14 16:58:40,465 - INFO - Fold 1 Train Epoch 10/200, Train Loss: 7.6675, Pearson Mean: 0.6157, Spearman Mean: 0.5802
2025-11-14 16:58:40,466 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4007, 'spearman_mean_genewise': 0.363, 'l1_error_mean': 1.9721, 'l2_errors_mean': 7.6618, 'r2_scores_mean': 0.173, 'pearson_std': 0.1159, 'l2_error_q1': 5.1932, 'l2_error_q2': 7.2787, 'l2_error_q3': 9.945, 'r2_score_q1': 0.0988, 'r2_score_q2': 0.148, 'r2_score_q3': 0.2108, 'mape_mean': 60.4682, 'mape_std': 18.264, 'rmse_mean': 2.7206, 'rmse_std': 0.5102}
2025-11-14 16:58:40,721 - INFO - Fold 1 Val Epoch 10/200, Batch 0, Loss: 8.9293, Pearson: 0.5517, Spearman: 0.5340
2025-11-14 16:58:41,955 - INFO - Fold 1 Val Epoch 10/200, Batch 10, Loss: 7.4562, Pearson: 0.6053, Spearman: 0.6147
2025-11-14 16:58:43,190 - INFO - Fold 1 Val Epoch 10/200, Batch 20, Loss: 5.6400, Pearson: 0.5139, Spearman: 0.5205
2025-11-14 16:58:45,954 - INFO - Fold 1 Val Epoch 10/200, Val Loss: 7.5951, Pearson Mean: 0.5590, Spearman Mean: 0.5598
2025-11-14 16:58:45,954 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.309, 'spearman_mean_genewise': 0.2795, 'l1_error_mean': 1.9606, 'l2_errors_mean': 7.6087, 'r2_scores_mean': 0.0953, 'pearson_std': 0.1034, 'l2_error_q1': 4.6016, 'l2_error_q2': 7.0523, 'l2_error_q3': 10.5708, 'r2_score_q1': 0.0475, 'r2_score_q2': 0.0857, 'r2_score_q3': 0.1335, 'mape_mean': 65.3914, 'mape_std': 19.0586, 'rmse_mean': 2.6843, 'rmse_std': 0.6349}
2025-11-14 16:58:45,954 - INFO - Learning rate for epoch 10: 0.0001
2025-11-14 16:58:45,954 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 16:58:46,690 - INFO - Fold 1 Train Epoch 11/200, Batch 0, Loss: 7.6550, Pearson: 0.6103, Spearman: 0.5806
2025-11-14 16:58:52,795 - INFO - Fold 1 Train Epoch 11/200, Batch 10, Loss: 7.5325, Pearson: 0.6259, Spearman: 0.5827
2025-11-14 16:58:59,057 - INFO - Fold 1 Train Epoch 11/200, Batch 20, Loss: 7.7679, Pearson: 0.6061, Spearman: 0.5778
2025-11-14 16:59:05,379 - INFO - Fold 1 Train Epoch 11/200, Batch 30, Loss: 7.6536, Pearson: 0.6216, Spearman: 0.5843
2025-11-14 16:59:11,650 - INFO - Fold 1 Train Epoch 11/200, Batch 40, Loss: 7.7309, Pearson: 0.6220, Spearman: 0.5884
2025-11-14 16:59:21,890 - INFO - Fold 1 Train Epoch 11/200, Batch 50, Loss: 7.5373, Pearson: 0.6194, Spearman: 0.5797
========================= 7.661755561828613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 11 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        7.6014023
 0.        7.6014023 8.2943   ]
Sample y_pred values (first sample, first 10 genes):
[0.49192262 0.         0.9203011  1.3240108  2.3366957  1.3250493
 0.47332507 2.2666345  2.545826   5.9889736 ]
y_true  -> mean=2.0654, std=3.4922, min=0.0000, max=12.5162
y_pred  -> mean=2.0961, std=2.1637, min=0.0000, max=14.7729
Batch 0 Pearson correlation: 0.6103
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6051
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6014
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6079
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6148
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6022
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6061
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6070
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6059
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6074
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6112
2025-11-14 16:59:31,975 - INFO - Fold 1 Train Epoch 11/200, Batch 60, Loss: 7.5635, Pearson: 0.6198, Spearman: 0.5834
2025-11-14 16:59:42,133 - INFO - Fold 1 Train Epoch 11/200, Batch 70, Loss: 7.6209, Pearson: 0.6246, Spearman: 0.5926
2025-11-14 16:59:52,276 - INFO - Fold 1 Train Epoch 11/200, Batch 80, Loss: 7.5916, Pearson: 0.6225, Spearman: 0.5820
2025-11-14 16:59:57,042 - INFO - Fold 1 Train Epoch 11/200, Train Loss: 7.6390, Pearson Mean: 0.6171, Spearman Mean: 0.5817
2025-11-14 16:59:57,042 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4023, 'spearman_mean_genewise': 0.3651, 'l1_error_mean': 1.9689, 'l2_errors_mean': 7.643, 'r2_scores_mean': 0.1747, 'pearson_std': 0.1165, 'l2_error_q1': 5.2034, 'l2_error_q2': 7.282, 'l2_error_q3': 9.874, 'r2_score_q1': 0.0996, 'r2_score_q2': 0.147, 'r2_score_q3': 0.2135, 'mape_mean': 60.2427, 'mape_std': 18.3971, 'rmse_mean': 2.7173, 'rmse_std': 0.5094}
2025-11-14 16:59:57,341 - INFO - Fold 1 Val Epoch 11/200, Batch 0, Loss: 8.7261, Pearson: 0.5642, Spearman: 0.5484
2025-11-14 16:59:59,213 - INFO - Fold 1 Val Epoch 11/200, Batch 10, Loss: 7.3989, Pearson: 0.6115, Spearman: 0.6199
2025-11-14 17:00:00,983 - INFO - Fold 1 Val Epoch 11/200, Batch 20, Loss: 5.8896, Pearson: 0.4791, Spearman: 0.4966
2025-11-14 17:00:03,520 - INFO - Fold 1 Val Epoch 11/200, Val Loss: 7.5037, Pearson Mean: 0.5627, Spearman Mean: 0.5624
2025-11-14 17:00:03,520 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3257, 'spearman_mean_genewise': 0.2943, 'l1_error_mean': 1.9328, 'l2_errors_mean': 7.5164, 'r2_scores_mean': 0.1042, 'pearson_std': 0.1071, 'l2_error_q1': 4.5266, 'l2_error_q2': 6.9531, 'l2_error_q3': 10.4024, 'r2_score_q1': 0.0546, 'r2_score_q2': 0.0942, 'r2_score_q3': 0.1459, 'mape_mean': 63.3309, 'mape_std': 18.9875, 'rmse_mean': 2.6686, 'rmse_std': 0.6285}
2025-11-14 17:00:03,520 - INFO - Learning rate for epoch 11: 0.0001
2025-11-14 17:00:03,581 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_1/best_model.pth
2025-11-14 17:00:04,612 - INFO - Fold 1 Train Epoch 12/200, Batch 0, Loss: 7.5842, Pearson: 0.6154, Spearman: 0.5771
2025-11-14 17:00:14,777 - INFO - Fold 1 Train Epoch 12/200, Batch 10, Loss: 7.3976, Pearson: 0.6328, Spearman: 0.5952
2025-11-14 17:00:24,921 - INFO - Fold 1 Train Epoch 12/200, Batch 20, Loss: 7.2597, Pearson: 0.6259, Spearman: 0.5808
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6131
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6318
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.642978668212891
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 12 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        7.2824097 7.975213  0.
 0.        7.2824097 8.891297 ]
Sample y_pred values (first sample, first 10 genes):
[0.95349467 0.83147913 2.2894735  2.1514854  3.6966853  2.9088848
 0.8498236  3.4610684  3.6682594  7.3085885 ]
y_true  -> mean=2.0620, std=3.4921, min=0.0000, max=12.5443
y_pred  -> mean=2.0996, std=2.2260, min=0.0000, max=14.4856
Batch 0 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6111
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6202
2025-11-14 17:00:33,828 - INFO - Fold 1 Train Epoch 12/200, Batch 30, Loss: 7.6646, Pearson: 0.6139, Spearman: 0.5760
2025-11-14 17:00:41,197 - INFO - Fold 1 Train Epoch 12/200, Batch 40, Loss: 7.5800, Pearson: 0.6147, Spearman: 0.5790
2025-11-14 17:00:51,395 - INFO - Fold 1 Train Epoch 12/200, Batch 50, Loss: 7.3326, Pearson: 0.6234, Spearman: 0.5853
2025-11-14 17:01:01,586 - INFO - Fold 1 Train Epoch 12/200, Batch 60, Loss: 7.8326, Pearson: 0.6167, Spearman: 0.5840
2025-11-14 17:01:11,733 - INFO - Fold 1 Train Epoch 12/200, Batch 70, Loss: 7.4622, Pearson: 0.6174, Spearman: 0.5830
2025-11-14 17:01:21,898 - INFO - Fold 1 Train Epoch 12/200, Batch 80, Loss: 7.5647, Pearson: 0.6141, Spearman: 0.5849
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6117
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6060
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6069
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6141
2025-11-14 17:01:26,749 - INFO - Fold 1 Train Epoch 12/200, Train Loss: 7.5943, Pearson Mean: 0.6201, Spearman Mean: 0.5844
2025-11-14 17:01:26,750 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4078, 'spearman_mean_genewise': 0.3696, 'l1_error_mean': 1.9549, 'l2_errors_mean': 7.5926, 'r2_scores_mean': 0.1793, 'pearson_std': 0.117, 'l2_error_q1': 5.187, 'l2_error_q2': 7.2333, 'l2_error_q3': 9.7794, 'r2_score_q1': 0.1042, 'r2_score_q2': 0.1508, 'r2_score_q3': 0.2216, 'mape_mean': 60.0581, 'mape_std': 18.3739, 'rmse_mean': 2.7087, 'rmse_std': 0.5057}
2025-11-14 17:01:27,025 - INFO - Fold 1 Val Epoch 12/200, Batch 0, Loss: 8.7774, Pearson: 0.5623, Spearman: 0.5518
2025-11-14 17:01:28,823 - INFO - Fold 1 Val Epoch 12/200, Batch 10, Loss: 7.3623, Pearson: 0.6148, Spearman: 0.6214
2025-11-14 17:01:30,566 - INFO - Fold 1 Val Epoch 12/200, Batch 20, Loss: 5.5869, Pearson: 0.5180, Spearman: 0.5249
2025-11-14 17:01:33,092 - INFO - Fold 1 Val Epoch 12/200, Val Loss: 7.4360, Pearson Mean: 0.5691, Spearman Mean: 0.5683
2025-11-14 17:01:33,092 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3289, 'spearman_mean_genewise': 0.2962, 'l1_error_mean': 1.9217, 'l2_errors_mean': 7.4555, 'r2_scores_mean': 0.1117, 'pearson_std': 0.109, 'l2_error_q1': 4.4973, 'l2_error_q2': 6.8557, 'l2_error_q3': 10.3199, 'r2_score_q1': 0.0555, 'r2_score_q2': 0.0971, 'r2_score_q3': 0.1463, 'mape_mean': 63.5578, 'mape_std': 18.8294, 'rmse_mean': 2.658, 'rmse_std': 0.625}
2025-11-14 17:01:33,093 - INFO - Learning rate for epoch 12: 0.0001
2025-11-14 17:01:33,154 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_1/best_model.pth
2025-11-14 17:01:34,083 - INFO - Fold 1 Train Epoch 13/200, Batch 0, Loss: 7.2611, Pearson: 0.6225, Spearman: 0.5871
2025-11-14 17:01:41,506 - INFO - Fold 1 Train Epoch 13/200, Batch 10, Loss: 7.6143, Pearson: 0.6173, Spearman: 0.5799
2025-11-14 17:01:47,440 - INFO - Fold 1 Train Epoch 13/200, Batch 20, Loss: 7.6847, Pearson: 0.6337, Spearman: 0.5957
2025-11-14 17:01:53,404 - INFO - Fold 1 Train Epoch 13/200, Batch 30, Loss: 7.6915, Pearson: 0.6196, Spearman: 0.5822
2025-11-14 17:01:59,387 - INFO - Fold 1 Train Epoch 13/200, Batch 40, Loss: 7.5247, Pearson: 0.6239, Spearman: 0.5842
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6194
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6003
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.592601776123047
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 13 =====================
Sample y_true values (first sample, first 10 genes):
[0.        6.7476616 0.        0.        0.        0.        0.
 0.        0.        8.692565 ]
Sample y_pred values (first sample, first 10 genes):
[0.         0.07582861 0.8599911  0.29038066 2.9489467  0.944181
 0.4558343  0.9259947  3.8758504  7.7435093 ]
y_true  -> mean=1.9680, std=3.4389, min=0.0000, max=12.5505
y_pred  -> mean=2.0930, std=2.1785, min=0.0000, max=13.7906
Batch 0 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6173
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6113
2025-11-14 17:02:05,384 - INFO - Fold 1 Train Epoch 13/200, Batch 50, Loss: 7.5843, Pearson: 0.6305, Spearman: 0.5894
2025-11-14 17:02:11,389 - INFO - Fold 1 Train Epoch 13/200, Batch 60, Loss: 7.6426, Pearson: 0.6205, Spearman: 0.5873
2025-11-14 17:02:17,377 - INFO - Fold 1 Train Epoch 13/200, Batch 70, Loss: 7.4846, Pearson: 0.6303, Spearman: 0.5914
2025-11-14 17:02:23,358 - INFO - Fold 1 Train Epoch 13/200, Batch 80, Loss: 7.4771, Pearson: 0.6277, Spearman: 0.5917
2025-11-14 17:02:26,576 - INFO - Fold 1 Train Epoch 13/200, Train Loss: 7.5627, Pearson Mean: 0.6221, Spearman Mean: 0.5864
2025-11-14 17:02:26,576 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4113, 'spearman_mean_genewise': 0.3726, 'l1_error_mean': 1.949, 'l2_errors_mean': 7.5595, 'r2_scores_mean': 0.1824, 'pearson_std': 0.1174, 'l2_error_q1': 5.1489, 'l2_error_q2': 7.2095, 'l2_error_q3': 9.7274, 'r2_score_q1': 0.1061, 'r2_score_q2': 0.1544, 'r2_score_q3': 0.2238, 'mape_mean': 59.7936, 'mape_std': 18.4496, 'rmse_mean': 2.7027, 'rmse_std': 0.5046}
2025-11-14 17:02:26,832 - INFO - Fold 1 Val Epoch 13/200, Batch 0, Loss: 8.7151, Pearson: 0.5666, Spearman: 0.5532
2025-11-14 17:02:28,064 - INFO - Fold 1 Val Epoch 13/200, Batch 10, Loss: 7.3927, Pearson: 0.6137, Spearman: 0.6227
2025-11-14 17:02:29,298 - INFO - Fold 1 Val Epoch 13/200, Batch 20, Loss: 5.6507, Pearson: 0.5249, Spearman: 0.5333
2025-11-14 17:02:31,657 - INFO - Fold 1 Val Epoch 13/200, Val Loss: 7.4577, Pearson Mean: 0.5697, Spearman Mean: 0.5694
2025-11-14 17:02:31,657 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3271, 'spearman_mean_genewise': 0.2958, 'l1_error_mean': 1.9338, 'l2_errors_mean': 7.4785, 'r2_scores_mean': 0.1093, 'pearson_std': 0.1088, 'l2_error_q1': 4.5269, 'l2_error_q2': 6.9303, 'l2_error_q3': 10.3732, 'r2_score_q1': 0.0554, 'r2_score_q2': 0.0942, 'r2_score_q3': 0.1452, 'mape_mean': 62.8206, 'mape_std': 19.1642, 'rmse_mean': 2.6619, 'rmse_std': 0.6265}
2025-11-14 17:02:31,657 - INFO - Learning rate for epoch 13: 0.0001
2025-11-14 17:02:31,657 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 17:02:32,367 - INFO - Fold 1 Train Epoch 14/200, Batch 0, Loss: 7.7020, Pearson: 0.6181, Spearman: 0.5857
2025-11-14 17:02:38,379 - INFO - Fold 1 Train Epoch 14/200, Batch 10, Loss: 7.2814, Pearson: 0.6366, Spearman: 0.5897
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6239
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6019
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.559505462646484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 14 =====================
Sample y_true values (first sample, first 10 genes):
[5.8288956 6.925545  5.8288956 0.        6.520571  7.6182013 0.
 5.8288956 5.8288956 9.610207 ]
Sample y_pred values (first sample, first 10 genes):
[2.2731223 5.9372926 5.195223  3.7348933 7.2822886 6.2798533 1.918454
 5.2969446 5.3337502 9.878112 ]
y_true  -> mean=2.1615, std=3.5291, min=0.0000, max=12.5172
y_pred  -> mean=2.0916, std=2.1757, min=0.0000, max=14.0888
Batch 0 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6157
2025-11-14 17:02:44,448 - INFO - Fold 1 Train Epoch 14/200, Batch 20, Loss: 7.3559, Pearson: 0.6187, Spearman: 0.5871
2025-11-14 17:02:50,480 - INFO - Fold 1 Train Epoch 14/200, Batch 30, Loss: 7.5444, Pearson: 0.6006, Spearman: 0.5766
2025-11-14 17:02:56,557 - INFO - Fold 1 Train Epoch 14/200, Batch 40, Loss: 7.5863, Pearson: 0.6242, Spearman: 0.5925
2025-11-14 17:03:02,633 - INFO - Fold 1 Train Epoch 14/200, Batch 50, Loss: 7.4358, Pearson: 0.6322, Spearman: 0.5947
2025-11-14 17:03:08,686 - INFO - Fold 1 Train Epoch 14/200, Batch 60, Loss: 7.5348, Pearson: 0.6223, Spearman: 0.5867
2025-11-14 17:03:14,657 - INFO - Fold 1 Train Epoch 14/200, Batch 70, Loss: 7.4946, Pearson: 0.6241, Spearman: 0.5873
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6090
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6231
2025-11-14 17:03:20,700 - INFO - Fold 1 Train Epoch 14/200, Batch 80, Loss: 7.6186, Pearson: 0.6242, Spearman: 0.5870
2025-11-14 17:03:24,074 - INFO - Fold 1 Train Epoch 14/200, Train Loss: 7.5289, Pearson Mean: 0.6246, Spearman Mean: 0.5885
2025-11-14 17:03:24,074 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4152, 'spearman_mean_genewise': 0.376, 'l1_error_mean': 1.9378, 'l2_errors_mean': 7.5268, 'r2_scores_mean': 0.1855, 'pearson_std': 0.1172, 'l2_error_q1': 5.139, 'l2_error_q2': 7.1612, 'l2_error_q3': 9.6636, 'r2_score_q1': 0.1092, 'r2_score_q2': 0.1587, 'r2_score_q3': 0.2285, 'mape_mean': 59.7443, 'mape_std': 18.3936, 'rmse_mean': 2.6971, 'rmse_std': 0.5025}
2025-11-14 17:03:24,328 - INFO - Fold 1 Val Epoch 14/200, Batch 0, Loss: 8.6364, Pearson: 0.5714, Spearman: 0.5551
2025-11-14 17:03:25,584 - INFO - Fold 1 Val Epoch 14/200, Batch 10, Loss: 7.5025, Pearson: 0.6103, Spearman: 0.6202
2025-11-14 17:03:26,817 - INFO - Fold 1 Val Epoch 14/200, Batch 20, Loss: 5.5053, Pearson: 0.5295, Spearman: 0.5361
2025-11-14 17:03:29,161 - INFO - Fold 1 Val Epoch 14/200, Val Loss: 7.4214, Pearson Mean: 0.5711, Spearman Mean: 0.5706
2025-11-14 17:03:29,162 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3316, 'spearman_mean_genewise': 0.3005, 'l1_error_mean': 1.8949, 'l2_errors_mean': 7.4399, 'r2_scores_mean': 0.1138, 'pearson_std': 0.1102, 'l2_error_q1': 4.5052, 'l2_error_q2': 6.863, 'l2_error_q3': 10.2898, 'r2_score_q1': 0.056, 'r2_score_q2': 0.0985, 'r2_score_q3': 0.1497, 'mape_mean': 63.6248, 'mape_std': 18.6077, 'rmse_mean': 2.6551, 'rmse_std': 0.6246}
2025-11-14 17:03:29,162 - INFO - Learning rate for epoch 14: 0.0001
2025-11-14 17:03:29,222 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_1/best_model.pth
2025-11-14 17:03:29,938 - INFO - Fold 1 Train Epoch 15/200, Batch 0, Loss: 7.4580, Pearson: 0.6283, Spearman: 0.5986
2025-11-14 17:03:35,974 - INFO - Fold 1 Train Epoch 15/200, Batch 10, Loss: 7.4432, Pearson: 0.6345, Spearman: 0.5891
2025-11-14 17:03:42,005 - INFO - Fold 1 Train Epoch 15/200, Batch 20, Loss: 7.8298, Pearson: 0.6278, Spearman: 0.5908
2025-11-14 17:03:47,989 - INFO - Fold 1 Train Epoch 15/200, Batch 30, Loss: 7.3551, Pearson: 0.6236, Spearman: 0.5881
2025-11-14 17:03:54,003 - INFO - Fold 1 Train Epoch 15/200, Batch 40, Loss: 7.5797, Pearson: 0.6313, Spearman: 0.5979
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6183
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6335
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.52677583694458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 15 =====================
Sample y_true values (first sample, first 10 genes):
[0.      0.      0.      0.      0.      0.      0.      0.      0.
 8.84353]
Sample y_pred values (first sample, first 10 genes):
[0.24569891 0.9581008  1.8698691  0.49055493 3.6123822  1.9157972
 0.         1.2360293  2.5246978  8.004131  ]
y_true  -> mean=2.1441, std=3.5104, min=0.0000, max=12.6166
y_pred  -> mean=2.1421, std=2.2105, min=0.0000, max=13.6189
Batch 0 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6097
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6202
2025-11-14 17:03:59,971 - INFO - Fold 1 Train Epoch 15/200, Batch 50, Loss: 7.4856, Pearson: 0.6408, Spearman: 0.6018
2025-11-14 17:04:05,980 - INFO - Fold 1 Train Epoch 15/200, Batch 60, Loss: 7.7545, Pearson: 0.6202, Spearman: 0.5883
2025-11-14 17:04:12,158 - INFO - Fold 1 Train Epoch 15/200, Batch 70, Loss: 7.6376, Pearson: 0.6260, Spearman: 0.5905
2025-11-14 17:04:18,293 - INFO - Fold 1 Train Epoch 15/200, Batch 80, Loss: 7.3252, Pearson: 0.6140, Spearman: 0.5830
2025-11-14 17:04:21,746 - INFO - Fold 1 Train Epoch 15/200, Train Loss: 7.4945, Pearson Mean: 0.6266, Spearman Mean: 0.5908
2025-11-14 17:04:21,746 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4184, 'spearman_mean_genewise': 0.379, 'l1_error_mean': 1.936, 'l2_errors_mean': 7.4943, 'r2_scores_mean': 0.1884, 'pearson_std': 0.1175, 'l2_error_q1': 5.1302, 'l2_error_q2': 7.1532, 'l2_error_q3': 9.6057, 'r2_score_q1': 0.1104, 'r2_score_q2': 0.1605, 'r2_score_q3': 0.2333, 'mape_mean': 59.2757, 'mape_std': 18.4264, 'rmse_mean': 2.6916, 'rmse_std': 0.4998}
2025-11-14 17:04:22,004 - INFO - Fold 1 Val Epoch 15/200, Batch 0, Loss: 8.7750, Pearson: 0.5613, Spearman: 0.5447
2025-11-14 17:04:23,228 - INFO - Fold 1 Val Epoch 15/200, Batch 10, Loss: 7.4336, Pearson: 0.6135, Spearman: 0.6213
2025-11-14 17:04:24,446 - INFO - Fold 1 Val Epoch 15/200, Batch 20, Loss: 5.5719, Pearson: 0.5180, Spearman: 0.5282
2025-11-14 17:04:26,946 - INFO - Fold 1 Val Epoch 15/200, Val Loss: 7.5313, Pearson Mean: 0.5619, Spearman Mean: 0.5629
2025-11-14 17:04:26,946 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3214, 'spearman_mean_genewise': 0.2924, 'l1_error_mean': 1.9146, 'l2_errors_mean': 7.5569, 'r2_scores_mean': 0.0977, 'pearson_std': 0.1081, 'l2_error_q1': 4.5341, 'l2_error_q2': 6.9783, 'l2_error_q3': 10.4978, 'r2_score_q1': 0.049, 'r2_score_q2': 0.0909, 'r2_score_q3': 0.1416, 'mape_mean': 63.8836, 'mape_std': 18.845, 'rmse_mean': 2.6758, 'rmse_std': 0.6302}
2025-11-14 17:04:26,946 - INFO - Learning rate for epoch 15: 0.0001
2025-11-14 17:04:26,946 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 17:04:27,741 - INFO - Fold 1 Train Epoch 16/200, Batch 0, Loss: 7.4829, Pearson: 0.6281, Spearman: 0.5915
2025-11-14 17:04:37,913 - INFO - Fold 1 Train Epoch 16/200, Batch 10, Loss: 7.8718, Pearson: 0.6305, Spearman: 0.5943
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6237
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6245
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.494334697723389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 16 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        8.5582075 0.        8.5582075 0.        0.
 0.        0.        9.251258 ]
Sample y_pred values (first sample, first 10 genes):
[0.17590627 0.         1.0754774  0.         2.3429193  0.2715172
 0.         0.51978123 1.5814735  6.374716  ]
y_true  -> mean=2.1313, std=3.5131, min=0.0000, max=12.6561
y_pred  -> mean=2.0336, std=2.1747, min=0.0000, max=13.3081
Batch 0 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6253
2025-11-14 17:04:48,136 - INFO - Fold 1 Train Epoch 16/200, Batch 20, Loss: 7.3922, Pearson: 0.6326, Spearman: 0.5895
2025-11-14 17:04:58,194 - INFO - Fold 1 Train Epoch 16/200, Batch 30, Loss: 7.7060, Pearson: 0.6286, Spearman: 0.6013
2025-11-14 17:05:08,330 - INFO - Fold 1 Train Epoch 16/200, Batch 40, Loss: 7.2802, Pearson: 0.6284, Spearman: 0.5862
2025-11-14 17:05:18,459 - INFO - Fold 1 Train Epoch 16/200, Batch 50, Loss: 7.4714, Pearson: 0.6154, Spearman: 0.5800
2025-11-14 17:05:28,615 - INFO - Fold 1 Train Epoch 16/200, Batch 60, Loss: 7.2057, Pearson: 0.6230, Spearman: 0.5863
2025-11-14 17:05:38,740 - INFO - Fold 1 Train Epoch 16/200, Batch 70, Loss: 7.6124, Pearson: 0.6183, Spearman: 0.5890
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6248
2025-11-14 17:05:48,896 - INFO - Fold 1 Train Epoch 16/200, Batch 80, Loss: 7.2147, Pearson: 0.6351, Spearman: 0.5938
2025-11-14 17:05:53,764 - INFO - Fold 1 Train Epoch 16/200, Train Loss: 7.4743, Pearson Mean: 0.6281, Spearman Mean: 0.5922
2025-11-14 17:05:53,764 - INFO - Training Metrics: {'pearson_mean_genewise': 0.421, 'spearman_mean_genewise': 0.3812, 'l1_error_mean': 1.9263, 'l2_errors_mean': 7.4735, 'r2_scores_mean': 0.1905, 'pearson_std': 0.1172, 'l2_error_q1': 5.1046, 'l2_error_q2': 7.1317, 'l2_error_q3': 9.5984, 'r2_score_q1': 0.1135, 'r2_score_q2': 0.1633, 'r2_score_q3': 0.2343, 'mape_mean': 59.3646, 'mape_std': 18.4069, 'rmse_mean': 2.6878, 'rmse_std': 0.4991}
2025-11-14 17:05:54,090 - INFO - Fold 1 Val Epoch 16/200, Batch 0, Loss: 8.7637, Pearson: 0.5630, Spearman: 0.5458
2025-11-14 17:05:56,038 - INFO - Fold 1 Val Epoch 16/200, Batch 10, Loss: 7.3629, Pearson: 0.6151, Spearman: 0.6230
2025-11-14 17:05:58,091 - INFO - Fold 1 Val Epoch 16/200, Batch 20, Loss: 5.5235, Pearson: 0.5243, Spearman: 0.5341
2025-11-14 17:06:00,531 - INFO - Fold 1 Val Epoch 16/200, Val Loss: 7.4412, Pearson Mean: 0.5686, Spearman Mean: 0.5683
2025-11-14 17:06:00,531 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3306, 'spearman_mean_genewise': 0.2995, 'l1_error_mean': 1.9006, 'l2_errors_mean': 7.4614, 'r2_scores_mean': 0.1085, 'pearson_std': 0.1094, 'l2_error_q1': 4.5037, 'l2_error_q2': 6.9006, 'l2_error_q3': 10.2809, 'r2_score_q1': 0.0547, 'r2_score_q2': 0.0974, 'r2_score_q3': 0.1492, 'mape_mean': 63.2784, 'mape_std': 18.6447, 'rmse_mean': 2.6599, 'rmse_std': 0.6215}
2025-11-14 17:06:00,531 - INFO - Learning rate for epoch 16: 0.0001
2025-11-14 17:06:00,531 - INFO - No improvement in spearman genewise. Patience: 2/30
2025-11-14 17:06:01,556 - INFO - Fold 1 Train Epoch 17/200, Batch 0, Loss: 7.3898, Pearson: 0.6263, Spearman: 0.5993
2025-11-14 17:06:11,582 - INFO - Fold 1 Train Epoch 17/200, Batch 10, Loss: 7.4904, Pearson: 0.6375, Spearman: 0.6020
2025-11-14 17:06:21,732 - INFO - Fold 1 Train Epoch 17/200, Batch 20, Loss: 7.4656, Pearson: 0.6333, Spearman: 0.5984
2025-11-14 17:06:31,886 - INFO - Fold 1 Train Epoch 17/200, Batch 30, Loss: 7.1839, Pearson: 0.6334, Spearman: 0.5935
2025-11-14 17:06:42,048 - INFO - Fold 1 Train Epoch 17/200, Batch 40, Loss: 7.4286, Pearson: 0.6435, Spearman: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6305
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6396
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.473541259765625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 17 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       9.915609]
Sample y_pred values (first sample, first 10 genes):
[0.3507751  0.80684704 1.7889841  0.14364043 3.7043562  1.4206413
 0.26869217 1.7786915  2.4962173  6.939702  ]
y_true  -> mean=2.0471, std=3.4866, min=0.0000, max=12.4976
y_pred  -> mean=2.0838, std=2.1973, min=0.0000, max=12.9794
Batch 0 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6268
2025-11-14 17:06:52,231 - INFO - Fold 1 Train Epoch 17/200, Batch 50, Loss: 7.3320, Pearson: 0.6340, Spearman: 0.5977
2025-11-14 17:07:02,404 - INFO - Fold 1 Train Epoch 17/200, Batch 60, Loss: 7.3583, Pearson: 0.6338, Spearman: 0.5987
2025-11-14 17:07:12,576 - INFO - Fold 1 Train Epoch 17/200, Batch 70, Loss: 7.2933, Pearson: 0.6353, Spearman: 0.6014
2025-11-14 17:07:22,737 - INFO - Fold 1 Train Epoch 17/200, Batch 80, Loss: 7.5892, Pearson: 0.6384, Spearman: 0.6014
2025-11-14 17:07:27,618 - INFO - Fold 1 Train Epoch 17/200, Train Loss: 7.4210, Pearson Mean: 0.6313, Spearman Mean: 0.5955
2025-11-14 17:07:27,618 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4269, 'spearman_mean_genewise': 0.3862, 'l1_error_mean': 1.9173, 'l2_errors_mean': 7.4165, 'r2_scores_mean': 0.1957, 'pearson_std': 0.1177, 'l2_error_q1': 5.0558, 'l2_error_q2': 7.0992, 'l2_error_q3': 9.5058, 'r2_score_q1': 0.1171, 'r2_score_q2': 0.1676, 'r2_score_q3': 0.2406, 'mape_mean': 59.0381, 'mape_std': 18.4222, 'rmse_mean': 2.6779, 'rmse_std': 0.4955}
2025-11-14 17:07:27,958 - INFO - Fold 1 Val Epoch 17/200, Batch 0, Loss: 8.7031, Pearson: 0.5665, Spearman: 0.5512
2025-11-14 17:07:29,838 - INFO - Fold 1 Val Epoch 17/200, Batch 10, Loss: 7.4062, Pearson: 0.6160, Spearman: 0.6231
2025-11-14 17:07:31,960 - INFO - Fold 1 Val Epoch 17/200, Batch 20, Loss: 5.7505, Pearson: 0.4953, Spearman: 0.5108
2025-11-14 17:07:34,470 - INFO - Fold 1 Val Epoch 17/200, Val Loss: 7.6130, Pearson Mean: 0.5544, Spearman Mean: 0.5569
2025-11-14 17:07:34,471 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3243, 'spearman_mean_genewise': 0.2953, 'l1_error_mean': 1.8709, 'l2_errors_mean': 7.6313, 'r2_scores_mean': 0.0827, 'pearson_std': 0.1094, 'l2_error_q1': 4.5482, 'l2_error_q2': 7.0142, 'l2_error_q3': 10.5717, 'r2_score_q1': 0.0475, 'r2_score_q2': 0.0877, 'r2_score_q3': 0.1407, 'mape_mean': 64.2112, 'mape_std': 18.3609, 'rmse_mean': 2.6871, 'rmse_std': 0.6411}
2025-11-14 17:07:34,471 - INFO - Learning rate for epoch 17: 0.0001
2025-11-14 17:07:34,471 - INFO - No improvement in spearman genewise. Patience: 3/30
2025-11-14 17:07:35,334 - INFO - Fold 1 Train Epoch 18/200, Batch 0, Loss: 7.3063, Pearson: 0.6338, Spearman: 0.5935
2025-11-14 17:07:45,540 - INFO - Fold 1 Train Epoch 18/200, Batch 10, Loss: 7.4021, Pearson: 0.6190, Spearman: 0.5902
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6090
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6307
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.5988
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.41651725769043
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 18 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       8.306369 0.       0.       0.
 0.       8.306369]
Sample y_pred values (first sample, first 10 genes):
[0.43997806 0.8660015  2.939781   0.29150975 2.1130292  2.2073536
 0.02190602 0.31207457 2.4372134  5.762317  ]
y_true  -> mean=2.0680, std=3.4939, min=0.0000, max=12.5808
y_pred  -> mean=2.0881, std=2.2614, min=0.0000, max=13.7400
Batch 0 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6190
2025-11-14 17:07:55,702 - INFO - Fold 1 Train Epoch 18/200, Batch 20, Loss: 7.7043, Pearson: 0.6126, Spearman: 0.5838
2025-11-14 17:08:05,874 - INFO - Fold 1 Train Epoch 18/200, Batch 30, Loss: 7.5344, Pearson: 0.6192, Spearman: 0.5885
2025-11-14 17:08:16,076 - INFO - Fold 1 Train Epoch 18/200, Batch 40, Loss: 7.5417, Pearson: 0.6233, Spearman: 0.5921
2025-11-14 17:08:26,246 - INFO - Fold 1 Train Epoch 18/200, Batch 50, Loss: 7.7171, Pearson: 0.6204, Spearman: 0.5913
2025-11-14 17:08:36,451 - INFO - Fold 1 Train Epoch 18/200, Batch 60, Loss: 7.3219, Pearson: 0.6293, Spearman: 0.5948
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6107
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6117
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6204
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6317
2025-11-14 17:08:46,634 - INFO - Fold 1 Train Epoch 18/200, Batch 70, Loss: 7.2407, Pearson: 0.6311, Spearman: 0.5994
2025-11-14 17:08:56,788 - INFO - Fold 1 Train Epoch 18/200, Batch 80, Loss: 7.6030, Pearson: 0.6390, Spearman: 0.5983
2025-11-14 17:09:01,696 - INFO - Fold 1 Train Epoch 18/200, Train Loss: 7.5105, Pearson Mean: 0.6258, Spearman Mean: 0.5916
2025-11-14 17:09:01,696 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4166, 'spearman_mean_genewise': 0.3775, 'l1_error_mean': 1.932, 'l2_errors_mean': 7.5102, 'r2_scores_mean': 0.1869, 'pearson_std': 0.118, 'l2_error_q1': 5.1018, 'l2_error_q2': 7.1658, 'l2_error_q3': 9.6583, 'r2_score_q1': 0.1106, 'r2_score_q2': 0.159, 'r2_score_q3': 0.23, 'mape_mean': 59.814, 'mape_std': 18.3056, 'rmse_mean': 2.694, 'rmse_std': 0.5026}
2025-11-14 17:09:02,060 - INFO - Fold 1 Val Epoch 18/200, Batch 0, Loss: 8.6746, Pearson: 0.5681, Spearman: 0.5553
2025-11-14 17:09:04,046 - INFO - Fold 1 Val Epoch 18/200, Batch 10, Loss: 7.5294, Pearson: 0.6073, Spearman: 0.6185
2025-11-14 17:09:05,970 - INFO - Fold 1 Val Epoch 18/200, Batch 20, Loss: 5.4796, Pearson: 0.5330, Spearman: 0.5378
2025-11-14 17:09:08,464 - INFO - Fold 1 Val Epoch 18/200, Val Loss: 7.4457, Pearson Mean: 0.5696, Spearman Mean: 0.5695
2025-11-14 17:09:08,465 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3259, 'spearman_mean_genewise': 0.2965, 'l1_error_mean': 1.9113, 'l2_errors_mean': 7.4702, 'r2_scores_mean': 0.1096, 'pearson_std': 0.1115, 'l2_error_q1': 4.5253, 'l2_error_q2': 6.9171, 'l2_error_q3': 10.2903, 'r2_score_q1': 0.052, 'r2_score_q2': 0.0941, 'r2_score_q3': 0.1437, 'mape_mean': 63.3851, 'mape_std': 18.9543, 'rmse_mean': 2.6609, 'rmse_std': 0.6242}
2025-11-14 17:09:08,465 - INFO - Learning rate for epoch 18: 0.0001
2025-11-14 17:09:08,465 - INFO - No improvement in spearman genewise. Patience: 4/30
2025-11-14 17:09:09,284 - INFO - Fold 1 Train Epoch 19/200, Batch 0, Loss: 7.4098, Pearson: 0.6259, Spearman: 0.5953
2025-11-14 17:09:19,487 - INFO - Fold 1 Train Epoch 19/200, Batch 10, Loss: 7.6998, Pearson: 0.6362, Spearman: 0.6005
2025-11-14 17:09:29,633 - INFO - Fold 1 Train Epoch 19/200, Batch 20, Loss: 7.2752, Pearson: 0.6385, Spearman: 0.6030
2025-11-14 17:09:39,809 - INFO - Fold 1 Train Epoch 19/200, Batch 30, Loss: 7.2984, Pearson: 0.6221, Spearman: 0.5864
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6279
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6426
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.510180950164795
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 19 =====================
Sample y_true values (first sample, first 10 genes):
[7.0652637 0.        7.0652637 0.        7.0652637 0.        0.
 7.0652637 0.        9.549387 ]
Sample y_pred values (first sample, first 10 genes):
[0.43496877 1.6209326  2.0591788  0.825706   4.2594957  2.364747
 0.6055952  2.0286741  3.767426   8.374651  ]
y_true  -> mean=2.0663, std=3.4895, min=0.0000, max=12.7196
y_pred  -> mean=2.1151, std=2.2194, min=0.0000, max=13.3176
Batch 0 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6374
2025-11-14 17:09:49,988 - INFO - Fold 1 Train Epoch 19/200, Batch 40, Loss: 7.3032, Pearson: 0.6318, Spearman: 0.6020
2025-11-14 17:10:00,168 - INFO - Fold 1 Train Epoch 19/200, Batch 50, Loss: 7.1730, Pearson: 0.6286, Spearman: 0.5936
2025-11-14 17:10:10,329 - INFO - Fold 1 Train Epoch 19/200, Batch 60, Loss: 7.6100, Pearson: 0.6336, Spearman: 0.6030
2025-11-14 17:10:20,507 - INFO - Fold 1 Train Epoch 19/200, Batch 70, Loss: 7.4558, Pearson: 0.6412, Spearman: 0.6008
2025-11-14 17:10:30,715 - INFO - Fold 1 Train Epoch 19/200, Batch 80, Loss: 7.3334, Pearson: 0.6433, Spearman: 0.6040
2025-11-14 17:10:35,620 - INFO - Fold 1 Train Epoch 19/200, Train Loss: 7.4041, Pearson Mean: 0.6326, Spearman Mean: 0.5971
2025-11-14 17:10:35,620 - INFO - Training Metrics: {'pearson_mean_genewise': 0.428, 'spearman_mean_genewise': 0.3871, 'l1_error_mean': 1.9188, 'l2_errors_mean': 7.4004, 'r2_scores_mean': 0.197, 'pearson_std': 0.1185, 'l2_error_q1': 5.054, 'l2_error_q2': 7.0988, 'l2_error_q3': 9.4772, 'r2_score_q1': 0.1187, 'r2_score_q2': 0.1679, 'r2_score_q3': 0.2454, 'mape_mean': 58.8907, 'mape_std': 18.4932, 'rmse_mean': 2.6748, 'rmse_std': 0.4956}
2025-11-14 17:10:35,987 - INFO - Fold 1 Val Epoch 19/200, Batch 0, Loss: 8.7734, Pearson: 0.5606, Spearman: 0.5458
2025-11-14 17:10:38,017 - INFO - Fold 1 Val Epoch 19/200, Batch 10, Loss: 7.4456, Pearson: 0.6137, Spearman: 0.6217
2025-11-14 17:10:40,071 - INFO - Fold 1 Val Epoch 19/200, Batch 20, Loss: 5.4971, Pearson: 0.5316, Spearman: 0.5383
2025-11-14 17:10:42,570 - INFO - Fold 1 Val Epoch 19/200, Val Loss: 7.4447, Pearson Mean: 0.5704, Spearman Mean: 0.5694
2025-11-14 17:10:42,571 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3261, 'spearman_mean_genewise': 0.2956, 'l1_error_mean': 1.8907, 'l2_errors_mean': 7.4667, 'r2_scores_mean': 0.11, 'pearson_std': 0.1105, 'l2_error_q1': 4.52, 'l2_error_q2': 6.9482, 'l2_error_q3': 10.2854, 'r2_score_q1': 0.0529, 'r2_score_q2': 0.0956, 'r2_score_q3': 0.1453, 'mape_mean': 64.7128, 'mape_std': 19.0624, 'rmse_mean': 2.6606, 'rmse_std': 0.6227}
2025-11-14 17:10:42,571 - INFO - Learning rate for epoch 19: 0.0001
2025-11-14 17:10:42,571 - INFO - No improvement in spearman genewise. Patience: 5/30
2025-11-14 17:10:43,427 - INFO - Fold 1 Train Epoch 20/200, Batch 0, Loss: 7.0975, Pearson: 0.6461, Spearman: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6277
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6309
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.400386333465576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 20 =====================
Sample y_true values (first sample, first 10 genes):
[0.        6.9077563 0.        6.9077563 7.6004033 7.6004033 0.
 0.        0.        9.104092 ]
Sample y_pred values (first sample, first 10 genes):
[1.5721853 3.9177046 3.6852539 2.772544  5.373365  4.375942  0.7247211
 3.6786747 3.0311246 8.008072 ]
y_true  -> mean=2.1008, std=3.4900, min=0.0000, max=12.7169
y_pred  -> mean=2.1260, std=2.2905, min=0.0000, max=14.1716
Batch 0 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6484
2025-11-14 17:10:53,385 - INFO - Fold 1 Train Epoch 20/200, Batch 10, Loss: 7.7276, Pearson: 0.6217, Spearman: 0.5927
2025-11-14 17:11:03,548 - INFO - Fold 1 Train Epoch 20/200, Batch 20, Loss: 7.2556, Pearson: 0.6425, Spearman: 0.5979
2025-11-14 17:11:13,711 - INFO - Fold 1 Train Epoch 20/200, Batch 30, Loss: 7.4417, Pearson: 0.6384, Spearman: 0.6087
2025-11-14 17:11:23,884 - INFO - Fold 1 Train Epoch 20/200, Batch 40, Loss: 7.3269, Pearson: 0.6390, Spearman: 0.6059
2025-11-14 17:11:34,049 - INFO - Fold 1 Train Epoch 20/200, Batch 50, Loss: 7.4136, Pearson: 0.6397, Spearman: 0.6040
2025-11-14 17:11:44,221 - INFO - Fold 1 Train Epoch 20/200, Batch 60, Loss: 7.4612, Pearson: 0.6334, Spearman: 0.6008
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6349
2025-11-14 17:11:54,413 - INFO - Fold 1 Train Epoch 20/200, Batch 70, Loss: 7.0711, Pearson: 0.6507, Spearman: 0.6068
2025-11-14 17:12:04,589 - INFO - Fold 1 Train Epoch 20/200, Batch 80, Loss: 7.2600, Pearson: 0.6367, Spearman: 0.6038
2025-11-14 17:12:09,500 - INFO - Fold 1 Train Epoch 20/200, Train Loss: 7.3356, Pearson Mean: 0.6367, Spearman Mean: 0.6006
2025-11-14 17:12:09,500 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4352, 'spearman_mean_genewise': 0.3929, 'l1_error_mean': 1.9101, 'l2_errors_mean': 7.3355, 'r2_scores_mean': 0.2032, 'pearson_std': 0.1183, 'l2_error_q1': 5.01, 'l2_error_q2': 7.0436, 'l2_error_q3': 9.359, 'r2_score_q1': 0.1225, 'r2_score_q2': 0.1739, 'r2_score_q3': 0.2513, 'mape_mean': 58.5545, 'mape_std': 18.4993, 'rmse_mean': 2.6634, 'rmse_std': 0.4918}
2025-11-14 17:12:09,803 - INFO - Fold 1 Val Epoch 20/200, Batch 0, Loss: 8.7555, Pearson: 0.5632, Spearman: 0.5495
2025-11-14 17:12:11,455 - INFO - Fold 1 Val Epoch 20/200, Batch 10, Loss: 7.4507, Pearson: 0.6123, Spearman: 0.6208
2025-11-14 17:12:13,519 - INFO - Fold 1 Val Epoch 20/200, Batch 20, Loss: 5.5231, Pearson: 0.5288, Spearman: 0.5353
2025-11-14 17:12:15,932 - INFO - Fold 1 Val Epoch 20/200, Val Loss: 7.4517, Pearson Mean: 0.5699, Spearman Mean: 0.5686
2025-11-14 17:12:15,932 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3244, 'spearman_mean_genewise': 0.2935, 'l1_error_mean': 1.902, 'l2_errors_mean': 7.4756, 'r2_scores_mean': 0.1095, 'pearson_std': 0.1115, 'l2_error_q1': 4.5302, 'l2_error_q2': 6.9022, 'l2_error_q3': 10.3253, 'r2_score_q1': 0.0528, 'r2_score_q2': 0.0948, 'r2_score_q3': 0.1441, 'mape_mean': 64.6862, 'mape_std': 19.505, 'rmse_mean': 2.6618, 'rmse_std': 0.625}
2025-11-14 17:12:15,933 - INFO - Learning rate for epoch 20: 1e-05
2025-11-14 17:12:15,933 - INFO - No improvement in spearman genewise. Patience: 6/30
2025-11-14 17:12:16,859 - INFO - Fold 1 Train Epoch 21/200, Batch 0, Loss: 7.1958, Pearson: 0.6413, Spearman: 0.6032
2025-11-14 17:12:26,869 - INFO - Fold 1 Train Epoch 21/200, Batch 10, Loss: 7.3090, Pearson: 0.6412, Spearman: 0.6093
2025-11-14 17:12:37,027 - INFO - Fold 1 Train Epoch 21/200, Batch 20, Loss: 7.3818, Pearson: 0.6381, Spearman: 0.6034
2025-11-14 17:12:47,203 - INFO - Fold 1 Train Epoch 21/200, Batch 30, Loss: 7.1556, Pearson: 0.6419, Spearman: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6391
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6448
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.335537433624268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 21 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 8.728076 9.421143]
Sample y_pred values (first sample, first 10 genes):
[0.21877448 0.         0.82414305 0.         1.419093   0.5775989
 0.         0.47973436 1.035725   5.918717  ]
y_true  -> mean=2.0954, std=3.4958, min=0.0000, max=12.7741
y_pred  -> mean=2.1101, std=2.2655, min=0.0000, max=14.4131
Batch 0 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6458
2025-11-14 17:12:57,376 - INFO - Fold 1 Train Epoch 21/200, Batch 40, Loss: 7.3885, Pearson: 0.6303, Spearman: 0.5978
2025-11-14 17:13:07,581 - INFO - Fold 1 Train Epoch 21/200, Batch 50, Loss: 7.3570, Pearson: 0.6384, Spearman: 0.6049
2025-11-14 17:13:17,779 - INFO - Fold 1 Train Epoch 21/200, Batch 60, Loss: 7.4007, Pearson: 0.6286, Spearman: 0.5960
2025-11-14 17:13:27,957 - INFO - Fold 1 Train Epoch 21/200, Batch 70, Loss: 7.2012, Pearson: 0.6498, Spearman: 0.6111
2025-11-14 17:13:38,160 - INFO - Fold 1 Train Epoch 21/200, Batch 80, Loss: 7.4579, Pearson: 0.6336, Spearman: 0.6065
2025-11-14 17:13:43,072 - INFO - Fold 1 Train Epoch 21/200, Train Loss: 7.2830, Pearson Mean: 0.6403, Spearman Mean: 0.6044
2025-11-14 17:13:43,072 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4423, 'spearman_mean_genewise': 0.399, 'l1_error_mean': 1.8962, 'l2_errors_mean': 7.2771, 'r2_scores_mean': 0.209, 'pearson_std': 0.1173, 'l2_error_q1': 4.9553, 'l2_error_q2': 6.9931, 'l2_error_q3': 9.2859, 'r2_score_q1': 0.1284, 'r2_score_q2': 0.1797, 'r2_score_q3': 0.2591, 'mape_mean': 58.3991, 'mape_std': 18.4745, 'rmse_mean': 2.6529, 'rmse_std': 0.4892}
2025-11-14 17:13:43,406 - INFO - Fold 1 Val Epoch 21/200, Batch 0, Loss: 8.7184, Pearson: 0.5647, Spearman: 0.5504
2025-11-14 17:13:45,413 - INFO - Fold 1 Val Epoch 21/200, Batch 10, Loss: 7.4213, Pearson: 0.6132, Spearman: 0.6218
2025-11-14 17:13:47,448 - INFO - Fold 1 Val Epoch 21/200, Batch 20, Loss: 5.5212, Pearson: 0.5287, Spearman: 0.5351
2025-11-14 17:13:49,898 - INFO - Fold 1 Val Epoch 21/200, Val Loss: 7.4193, Pearson Mean: 0.5715, Spearman Mean: 0.5700
2025-11-14 17:13:49,899 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3281, 'spearman_mean_genewise': 0.2972, 'l1_error_mean': 1.9095, 'l2_errors_mean': 7.4413, 'r2_scores_mean': 0.1129, 'pearson_std': 0.1116, 'l2_error_q1': 4.5261, 'l2_error_q2': 6.8663, 'l2_error_q3': 10.2372, 'r2_score_q1': 0.0553, 'r2_score_q2': 0.0977, 'r2_score_q3': 0.1477, 'mape_mean': 63.8662, 'mape_std': 19.1779, 'rmse_mean': 2.656, 'rmse_std': 0.6219}
2025-11-14 17:13:49,899 - INFO - Learning rate for epoch 21: 1e-05
2025-11-14 17:13:49,899 - INFO - No improvement in spearman genewise. Patience: 7/30
2025-11-14 17:13:50,872 - INFO - Fold 1 Train Epoch 22/200, Batch 0, Loss: 7.2232, Pearson: 0.6462, Spearman: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6351
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6308
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.277073383331299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 22 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.1113   6.418968 7.804039 7.516493 6.418968 0.
 0.       9.784689]
Sample y_pred values (first sample, first 10 genes):
[1.4276764 4.8227797 3.810844  2.6392536 6.484194  5.085005  1.1356308
 4.190777  4.7073035 9.9992   ]
y_true  -> mean=2.1559, std=3.5211, min=0.0000, max=12.6507
y_pred  -> mean=2.1124, std=2.2477, min=0.0000, max=13.6909
Batch 0 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6323
2025-11-14 17:14:00,911 - INFO - Fold 1 Train Epoch 22/200, Batch 10, Loss: 7.0605, Pearson: 0.6322, Spearman: 0.5983
2025-11-14 17:14:11,070 - INFO - Fold 1 Train Epoch 22/200, Batch 20, Loss: 7.3662, Pearson: 0.6373, Spearman: 0.6070
2025-11-14 17:14:21,237 - INFO - Fold 1 Train Epoch 22/200, Batch 30, Loss: 7.2410, Pearson: 0.6397, Spearman: 0.6004
2025-11-14 17:14:31,409 - INFO - Fold 1 Train Epoch 22/200, Batch 40, Loss: 7.0086, Pearson: 0.6441, Spearman: 0.6061
2025-11-14 17:14:41,580 - INFO - Fold 1 Train Epoch 22/200, Batch 50, Loss: 7.3309, Pearson: 0.6486, Spearman: 0.6081
2025-11-14 17:14:51,763 - INFO - Fold 1 Train Epoch 22/200, Batch 60, Loss: 7.2924, Pearson: 0.6431, Spearman: 0.6046
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6431
2025-11-14 17:15:01,952 - INFO - Fold 1 Train Epoch 22/200, Batch 70, Loss: 7.2462, Pearson: 0.6392, Spearman: 0.5989
2025-11-14 17:15:12,142 - INFO - Fold 1 Train Epoch 22/200, Batch 80, Loss: 7.0556, Pearson: 0.6555, Spearman: 0.6123
2025-11-14 17:15:17,007 - INFO - Fold 1 Train Epoch 22/200, Train Loss: 7.2681, Pearson Mean: 0.6408, Spearman Mean: 0.6047
2025-11-14 17:15:17,007 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4434, 'spearman_mean_genewise': 0.3999, 'l1_error_mean': 1.8974, 'l2_errors_mean': 7.2681, 'r2_scores_mean': 0.21, 'pearson_std': 0.117, 'l2_error_q1': 4.9589, 'l2_error_q2': 6.9861, 'l2_error_q3': 9.2709, 'r2_score_q1': 0.1296, 'r2_score_q2': 0.1816, 'r2_score_q3': 0.26, 'mape_mean': 58.375, 'mape_std': 18.4609, 'rmse_mean': 2.6513, 'rmse_std': 0.4888}
2025-11-14 17:15:17,354 - INFO - Fold 1 Val Epoch 22/200, Batch 0, Loss: 8.7632, Pearson: 0.5620, Spearman: 0.5473
2025-11-14 17:15:19,349 - INFO - Fold 1 Val Epoch 22/200, Batch 10, Loss: 7.4160, Pearson: 0.6115, Spearman: 0.6204
2025-11-14 17:15:21,283 - INFO - Fold 1 Val Epoch 22/200, Batch 20, Loss: 5.5379, Pearson: 0.5275, Spearman: 0.5344
2025-11-14 17:15:23,728 - INFO - Fold 1 Val Epoch 22/200, Val Loss: 7.4505, Pearson Mean: 0.5690, Spearman Mean: 0.5676
2025-11-14 17:15:23,729 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3262, 'spearman_mean_genewise': 0.2954, 'l1_error_mean': 1.9358, 'l2_errors_mean': 7.4741, 'r2_scores_mean': 0.1097, 'pearson_std': 0.1105, 'l2_error_q1': 4.531, 'l2_error_q2': 6.9413, 'l2_error_q3': 10.3294, 'r2_score_q1': 0.054, 'r2_score_q2': 0.0951, 'r2_score_q3': 0.145, 'mape_mean': 63.2699, 'mape_std': 19.1634, 'rmse_mean': 2.6616, 'rmse_std': 0.6245}
2025-11-14 17:15:23,729 - INFO - Learning rate for epoch 22: 1e-05
2025-11-14 17:15:23,729 - INFO - No improvement in spearman genewise. Patience: 8/30
2025-11-14 17:15:24,624 - INFO - Fold 1 Train Epoch 23/200, Batch 0, Loss: 7.2336, Pearson: 0.6513, Spearman: 0.6138
2025-11-14 17:15:34,603 - INFO - Fold 1 Train Epoch 23/200, Batch 10, Loss: 7.3140, Pearson: 0.6477, Spearman: 0.6097
2025-11-14 17:15:44,785 - INFO - Fold 1 Train Epoch 23/200, Batch 20, Loss: 7.2860, Pearson: 0.6499, Spearman: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6507
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6147
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.268148899078369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 23 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        6.7691426 0.        0.        7.8669887
 0.        8.154575  9.070693 ]
Sample y_pred values (first sample, first 10 genes):
[0.         1.0132595  1.1598666  0.9151375  3.9233458  2.2422743
 0.8957526  0.56216025 4.0744286  7.9366574 ]
y_true  -> mean=2.2371, std=3.5389, min=0.0000, max=12.4808
y_pred  -> mean=2.1101, std=2.2294, min=0.0000, max=13.1544
Batch 0 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6460
2025-11-14 17:15:54,967 - INFO - Fold 1 Train Epoch 23/200, Batch 30, Loss: 7.2363, Pearson: 0.6448, Spearman: 0.6070
2025-11-14 17:16:05,175 - INFO - Fold 1 Train Epoch 23/200, Batch 40, Loss: 7.3967, Pearson: 0.6418, Spearman: 0.6107
2025-11-14 17:16:15,330 - INFO - Fold 1 Train Epoch 23/200, Batch 50, Loss: 7.0193, Pearson: 0.6488, Spearman: 0.6008
2025-11-14 17:16:25,511 - INFO - Fold 1 Train Epoch 23/200, Batch 60, Loss: 7.3263, Pearson: 0.6402, Spearman: 0.5992
2025-11-14 17:16:35,703 - INFO - Fold 1 Train Epoch 23/200, Batch 70, Loss: 7.3529, Pearson: 0.6461, Spearman: 0.6022
2025-11-14 17:16:45,880 - INFO - Fold 1 Train Epoch 23/200, Batch 80, Loss: 7.4067, Pearson: 0.6447, Spearman: 0.6045
2025-11-14 17:16:50,722 - INFO - Fold 1 Train Epoch 23/200, Train Loss: 7.2603, Pearson Mean: 0.6415, Spearman Mean: 0.6053
2025-11-14 17:16:50,722 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4442, 'spearman_mean_genewise': 0.4006, 'l1_error_mean': 1.896, 'l2_errors_mean': 7.2606, 'r2_scores_mean': 0.2107, 'pearson_std': 0.1171, 'l2_error_q1': 4.9543, 'l2_error_q2': 6.9727, 'l2_error_q3': 9.2614, 'r2_score_q1': 0.1294, 'r2_score_q2': 0.1816, 'r2_score_q3': 0.2602, 'mape_mean': 58.3577, 'mape_std': 18.466, 'rmse_mean': 2.6499, 'rmse_std': 0.4885}
2025-11-14 17:16:51,060 - INFO - Fold 1 Val Epoch 23/200, Batch 0, Loss: 8.7621, Pearson: 0.5617, Spearman: 0.5469
2025-11-14 17:16:53,010 - INFO - Fold 1 Val Epoch 23/200, Batch 10, Loss: 7.4170, Pearson: 0.6125, Spearman: 0.6211
2025-11-14 17:16:54,957 - INFO - Fold 1 Val Epoch 23/200, Batch 20, Loss: 5.5013, Pearson: 0.5314, Spearman: 0.5376
2025-11-14 17:16:57,469 - INFO - Fold 1 Val Epoch 23/200, Val Loss: 7.4229, Pearson Mean: 0.5714, Spearman Mean: 0.5698
2025-11-14 17:16:57,469 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3274, 'spearman_mean_genewise': 0.2967, 'l1_error_mean': 1.9119, 'l2_errors_mean': 7.4466, 'r2_scores_mean': 0.1123, 'pearson_std': 0.1116, 'l2_error_q1': 4.5246, 'l2_error_q2': 6.8907, 'l2_error_q3': 10.2466, 'r2_score_q1': 0.0545, 'r2_score_q2': 0.0974, 'r2_score_q3': 0.1475, 'mape_mean': 63.9678, 'mape_std': 19.2131, 'rmse_mean': 2.6569, 'rmse_std': 0.6224}
2025-11-14 17:16:57,469 - INFO - Learning rate for epoch 23: 1e-05
2025-11-14 17:16:57,469 - INFO - No improvement in spearman genewise. Patience: 9/30
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6459
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6402
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.260624408721924
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 24 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        7.741366  6.643622  0.
 7.3361177 0.        8.588415 ]
Sample y_pred values (first sample, first 10 genes):
2025-11-14 17:16:58,381 - INFO - Fold 1 Train Epoch 24/200, Batch 0, Loss: 7.0601, Pearson: 0.6485, Spearman: 0.6156
2025-11-14 17:17:08,589 - INFO - Fold 1 Train Epoch 24/200, Batch 10, Loss: 7.2373, Pearson: 0.6407, Spearman: 0.6043
2025-11-14 17:17:18,789 - INFO - Fold 1 Train Epoch 24/200, Batch 20, Loss: 7.1649, Pearson: 0.6357, Spearman: 0.5982
2025-11-14 17:17:28,957 - INFO - Fold 1 Train Epoch 24/200, Batch 30, Loss: 7.1734, Pearson: 0.6396, Spearman: 0.6000
2025-11-14 17:17:39,134 - INFO - Fold 1 Train Epoch 24/200, Batch 40, Loss: 7.2687, Pearson: 0.6261, Spearman: 0.5928
2025-11-14 17:17:49,319 - INFO - Fold 1 Train Epoch 24/200, Batch 50, Loss: 7.3815, Pearson: 0.6437, Spearman: 0.6021
[0.9959033 4.388012  2.9522557 1.9822854 6.3014135 5.2493334 0.8993429
 3.5009084 3.5199099 8.013708 ]
y_true  -> mean=2.0900, std=3.4900, min=0.0000, max=12.7075
y_pred  -> mean=2.1024, std=2.2143, min=0.0000, max=13.4537
Batch 0 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])2025-11-14 17:17:59,521 - INFO - Fold 1 Train Epoch 24/200, Batch 60, Loss: 7.3469, Pearson: 0.6353, Spearman: 0.6047
2025-11-14 17:18:09,703 - INFO - Fold 1 Train Epoch 24/200, Batch 70, Loss: 7.5808, Pearson: 0.6350, Spearman: 0.6043
2025-11-14 17:18:19,905 - INFO - Fold 1 Train Epoch 24/200, Batch 80, Loss: 7.0038, Pearson: 0.6482, Spearman: 0.6051
2025-11-14 17:18:24,743 - INFO - Fold 1 Train Epoch 24/200, Train Loss: 7.2566, Pearson Mean: 0.6414, Spearman Mean: 0.6055
2025-11-14 17:18:24,743 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4444, 'spearman_mean_genewise': 0.4007, 'l1_error_mean': 1.8959, 'l2_errors_mean': 7.2593, 'r2_scores_mean': 0.2109, 'pearson_std': 0.1169, 'l2_error_q1': 4.9396, 'l2_error_q2': 6.9755, 'l2_error_q3': 9.2542, 'r2_score_q1': 0.1297, 'r2_score_q2': 0.1827, 'r2_score_q3': 0.2606, 'mape_mean': 58.3504, 'mape_std': 18.452, 'rmse_mean': 2.6497, 'rmse_std': 0.4884}
2025-11-14 17:18:25,075 - INFO - Fold 1 Val Epoch 24/200, Batch 0, Loss: 8.7256, Pearson: 0.5648, Spearman: 0.5506
2025-11-14 17:18:27,044 - INFO - Fold 1 Val Epoch 24/200, Batch 10, Loss: 7.4107, Pearson: 0.6117, Spearman: 0.6208
2025-11-14 17:18:29,040 - INFO - Fold 1 Val Epoch 24/200, Batch 20, Loss: 5.5268, Pearson: 0.5269, Spearman: 0.5344
2025-11-14 17:18:31,547 - INFO - Fold 1 Val Epoch 24/200, Val Loss: 7.4400, Pearson Mean: 0.5696, Spearman Mean: 0.5684
2025-11-14 17:18:31,547 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3268, 'spearman_mean_genewise': 0.2961, 'l1_error_mean': 1.9329, 'l2_errors_mean': 7.4629, 'r2_scores_mean': 0.1107, 'pearson_std': 0.1107, 'l2_error_q1': 4.522, 'l2_error_q2': 6.9273, 'l2_error_q3': 10.3174, 'r2_score_q1': 0.0547, 'r2_score_q2': 0.0957, 'r2_score_q3': 0.1459, 'mape_mean': 63.1386, 'mape_std': 19.1285, 'rmse_mean': 2.6598, 'rmse_std': 0.6234}
2025-11-14 17:18:31,547 - INFO - Learning rate for epoch 24: 1e-05
2025-11-14 17:18:31,547 - INFO - No improvement in spearman genewise. Patience: 10/30
2025-11-14 17:18:32,467 - INFO - Fold 1 Train Epoch 25/200, Batch 0, Loss: 7.4071, Pearson: 0.6372, Spearman: 0.6091
2025-11-14 17:18:42,678 - INFO - Fold 1 Train Epoch 25/200, Batch 10, Loss: 7.2731, Pearson: 0.6355, Spearman: 0.6051
2025-11-14 17:18:52,882 - INFO - Fold 1 Train Epoch 25/200, Batch 20, Loss: 7.4227, Pearson: 0.6427, Spearman: 0.6035

Batch 57 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6475
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6224
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.259341716766357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 25 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        8.236046  0.        0.
 7.5431633 0.        7.5431633]
Sample y_pred values (first sample, first 10 genes):
[0.8077867 0.        1.73893   1.9616842 3.1971757 2.2322288 1.011915
 3.46376   3.7152436 6.761956 ]
y_true  -> mean=2.1564, std=3.5305, min=0.0000, max=12.7939
y_pred  -> mean=2.1080, std=2.2085, min=0.0000, max=13.2538
Batch 0 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6415
2025-11-14 17:19:03,054 - INFO - Fold 1 Train Epoch 25/200, Batch 30, Loss: 7.2446, Pearson: 0.6463, Spearman: 0.6083
2025-11-14 17:19:13,248 - INFO - Fold 1 Train Epoch 25/200, Batch 40, Loss: 7.0028, Pearson: 0.6430, Spearman: 0.6018
2025-11-14 17:19:23,403 - INFO - Fold 1 Train Epoch 25/200, Batch 50, Loss: 7.1771, Pearson: 0.6419, Spearman: 0.6061
2025-11-14 17:19:33,589 - INFO - Fold 1 Train Epoch 25/200, Batch 60, Loss: 7.0463, Pearson: 0.6518, Spearman: 0.6078
2025-11-14 17:19:43,753 - INFO - Fold 1 Train Epoch 25/200, Batch 70, Loss: 7.3403, Pearson: 0.6389, Spearman: 0.6013
2025-11-14 17:19:53,934 - INFO - Fold 1 Train Epoch 25/200, Batch 80, Loss: 7.1390, Pearson: 0.6445, Spearman: 0.6052
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6389
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6143
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
2025-11-14 17:19:58,861 - INFO - Fold 1 Train Epoch 25/200, Train Loss: 7.2547, Pearson Mean: 0.6422, Spearman Mean: 0.6063
2025-11-14 17:19:58,861 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4459, 'spearman_mean_genewise': 0.402, 'l1_error_mean': 1.8934, 'l2_errors_mean': 7.2453, 'r2_scores_mean': 0.2122, 'pearson_std': 0.1169, 'l2_error_q1': 4.9246, 'l2_error_q2': 6.9617, 'l2_error_q3': 9.251, 'r2_score_q1': 0.1309, 'r2_score_q2': 0.1838, 'r2_score_q3': 0.262, 'mape_mean': 58.2654, 'mape_std': 18.4486, 'rmse_mean': 2.6471, 'rmse_std': 0.4878}
2025-11-14 17:19:59,232 - INFO - Fold 1 Val Epoch 25/200, Batch 0, Loss: 8.7367, Pearson: 0.5630, Spearman: 0.5486
2025-11-14 17:20:01,302 - INFO - Fold 1 Val Epoch 25/200, Batch 10, Loss: 7.4569, Pearson: 0.6122, Spearman: 0.6209
2025-11-14 17:20:03,111 - INFO - Fold 1 Val Epoch 25/200, Batch 20, Loss: 5.5108, Pearson: 0.5305, Spearman: 0.5365
2025-11-14 17:20:05,545 - INFO - Fold 1 Val Epoch 25/200, Val Loss: 7.4299, Pearson Mean: 0.5708, Spearman Mean: 0.5691
2025-11-14 17:20:05,546 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3266, 'spearman_mean_genewise': 0.296, 'l1_error_mean': 1.9108, 'l2_errors_mean': 7.4529, 'r2_scores_mean': 0.1118, 'pearson_std': 0.1116, 'l2_error_q1': 4.5226, 'l2_error_q2': 6.8881, 'l2_error_q3': 10.2627, 'r2_score_q1': 0.0545, 'r2_score_q2': 0.0973, 'r2_score_q3': 0.1472, 'mape_mean': 64.1937, 'mape_std': 19.0869, 'rmse_mean': 2.6579, 'rmse_std': 0.6231}
2025-11-14 17:20:05,546 - INFO - Learning rate for epoch 25: 1e-05
2025-11-14 17:20:05,546 - INFO - No improvement in spearman genewise. Patience: 11/30
2025-11-14 17:20:06,428 - INFO - Fold 1 Train Epoch 26/200, Batch 0, Loss: 7.3131, Pearson: 0.6383, Spearman: 0.6068
2025-11-14 17:20:16,398 - INFO - Fold 1 Train Epoch 26/200, Batch 10, Loss: 7.1259, Pearson: 0.6485, Spearman: 0.6105
2025-11-14 17:20:26,591 - INFO - Fold 1 Train Epoch 26/200, Batch 20, Loss: 7.5089, Pearson: 0.6503, Spearman: 0.6121
2025-11-14 17:20:36,761 - INFO - Fold 1 Train Epoch 26/200, Batch 30, Loss: 7.2475, Pearson: 0.6332, Spearman: 0.5969
2025-11-14 17:20:46,957 - INFO - Fold 1 Train Epoch 26/200, Batch 40, Loss: 7.2879, Pearson: 0.6426, Spearman: 0.6074
2025-11-14 17:20:57,127 - INFO - Fold 1 Train Epoch 26/200, Batch 50, Loss: 7.2222, Pearson: 0.6528, Spearman: 0.6145
========================= 7.245289325714111
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 26 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 0.        7.9353356 9.544487 ]
Sample y_pred values (first sample, first 10 genes):
[0.33033273 0.         1.7104821  0.36834833 3.8955908  0.8392923
 0.41742963 1.0775506  3.5368264  8.359154  ]
y_true  -> mean=2.1260, std=3.5127, min=0.0000, max=12.5442
y_pred  -> mean=2.1078, std=2.2105, min=0.0000, max=13.0623
Batch 0 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 17:21:07,316 - INFO - Fold 1 Train Epoch 26/200, Batch 60, Loss: 7.2066, Pearson: 0.6343, Spearman: 0.6051
2025-11-14 17:21:17,496 - INFO - Fold 1 Train Epoch 26/200, Batch 70, Loss: 7.3183, Pearson: 0.6486, Spearman: 0.6094
2025-11-14 17:21:27,672 - INFO - Fold 1 Train Epoch 26/200, Batch 80, Loss: 7.1515, Pearson: 0.6323, Spearman: 0.6006
2025-11-14 17:21:32,617 - INFO - Fold 1 Train Epoch 26/200, Train Loss: 7.2472, Pearson Mean: 0.6424, Spearman Mean: 0.6063
2025-11-14 17:21:32,617 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4459, 'spearman_mean_genewise': 0.402, 'l1_error_mean': 1.8956, 'l2_errors_mean': 7.2462, 'r2_scores_mean': 0.2122, 'pearson_std': 0.1168, 'l2_error_q1': 4.9277, 'l2_error_q2': 6.9627, 'l2_error_q3': 9.227, 'r2_score_q1': 0.1309, 'r2_score_q2': 0.1841, 'r2_score_q3': 0.2607, 'mape_mean': 58.2349, 'mape_std': 18.4537, 'rmse_mean': 2.6473, 'rmse_std': 0.4878}
2025-11-14 17:21:32,926 - INFO - Fold 1 Val Epoch 26/200, Batch 0, Loss: 8.7650, Pearson: 0.5617, Spearman: 0.5470
2025-11-14 17:21:34,922 - INFO - Fold 1 Val Epoch 26/200, Batch 10, Loss: 7.4158, Pearson: 0.6115, Spearman: 0.6203
2025-11-14 17:21:37,014 - INFO - Fold 1 Val Epoch 26/200, Batch 20, Loss: 5.5096, Pearson: 0.5294, Spearman: 0.5359
2025-11-14 17:21:39,431 - INFO - Fold 1 Val Epoch 26/200, Val Loss: 7.4363, Pearson Mean: 0.5700, Spearman Mean: 0.5684
2025-11-14 17:21:39,432 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3266, 'spearman_mean_genewise': 0.2957, 'l1_error_mean': 1.9229, 'l2_errors_mean': 7.4598, 'r2_scores_mean': 0.111, 'pearson_std': 0.1111, 'l2_error_q1': 4.5214, 'l2_error_q2': 6.9083, 'l2_error_q3': 10.2811, 'r2_score_q1': 0.0545, 'r2_score_q2': 0.0968, 'r2_score_q3': 0.1467, 'mape_mean': 63.723, 'mape_std': 19.2071, 'rmse_mean': 2.6592, 'rmse_std': 0.6233}
2025-11-14 17:21:39,432 - INFO - Learning rate for epoch 26: 1e-05
2025-11-14 17:21:39,432 - INFO - No improvement in spearman genewise. Patience: 12/30
2025-11-14 17:21:40,420 - INFO - Fold 1 Train Epoch 27/200, Batch 0, Loss: 7.1551, Pearson: 0.6417, Spearman: 0.6053
2025-11-14 17:21:50,467 - INFO - Fold 1 Train Epoch 27/200, Batch 10, Loss: 7.1297, Pearson: 0.6481, Spearman: 0.6140
2025-11-14 17:22:00,660 - INFO - Fold 1 Train Epoch 27/200, Batch 20, Loss: 7.2528, Pearson: 0.6483, Spearman: 0.6128
Batch 54 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6403
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6369
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.246192932128906
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 27 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.639538]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.79089427 0.         1.7688663  0.53336215
 0.         0.44175977 1.477767   6.486723  ]
y_true  -> mean=2.0706, std=3.4873, min=0.0000, max=12.4991
y_pred  -> mean=2.1083, std=2.2430, min=0.0000, max=14.5950
Batch 0 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6405
2025-11-14 17:22:10,872 - INFO - Fold 1 Train Epoch 27/200, Batch 30, Loss: 7.3688, Pearson: 0.6333, Spearman: 0.5964
2025-11-14 17:22:21,066 - INFO - Fold 1 Train Epoch 27/200, Batch 40, Loss: 7.2422, Pearson: 0.6471, Spearman: 0.6108
2025-11-14 17:22:31,263 - INFO - Fold 1 Train Epoch 27/200, Batch 50, Loss: 7.0691, Pearson: 0.6474, Spearman: 0.5991
2025-11-14 17:22:41,434 - INFO - Fold 1 Train Epoch 27/200, Batch 60, Loss: 7.1670, Pearson: 0.6497, Spearman: 0.6120
2025-11-14 17:22:51,613 - INFO - Fold 1 Train Epoch 27/200, Batch 70, Loss: 7.2240, Pearson: 0.6426, Spearman: 0.6108
2025-11-14 17:23:01,809 - INFO - Fold 1 Train Epoch 27/200, Batch 80, Loss: 7.0758, Pearson: 0.6450, Spearman: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6420
2025-11-14 17:23:06,663 - INFO - Fold 1 Train Epoch 27/200, Train Loss: 7.2385, Pearson Mean: 0.6430, Spearman Mean: 0.6069
2025-11-14 17:23:06,663 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4468, 'spearman_mean_genewise': 0.4027, 'l1_error_mean': 1.8926, 'l2_errors_mean': 7.2373, 'r2_scores_mean': 0.213, 'pearson_std': 0.1168, 'l2_error_q1': 4.9269, 'l2_error_q2': 6.9556, 'l2_error_q3': 9.2143, 'r2_score_q1': 0.132, 'r2_score_q2': 0.1846, 'r2_score_q3': 0.2633, 'mape_mean': 58.2328, 'mape_std': 18.4486, 'rmse_mean': 2.6457, 'rmse_std': 0.4873}
2025-11-14 17:23:07,001 - INFO - Fold 1 Val Epoch 27/200, Batch 0, Loss: 8.7251, Pearson: 0.5637, Spearman: 0.5496
2025-11-14 17:23:09,043 - INFO - Fold 1 Val Epoch 27/200, Batch 10, Loss: 7.4460, Pearson: 0.6119, Spearman: 0.6206
2025-11-14 17:23:10,872 - INFO - Fold 1 Val Epoch 27/200, Batch 20, Loss: 5.5101, Pearson: 0.5304, Spearman: 0.5364
2025-11-14 17:23:13,384 - INFO - Fold 1 Val Epoch 27/200, Val Loss: 7.4313, Pearson Mean: 0.5706, Spearman Mean: 0.5690
2025-11-14 17:23:13,385 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3259, 'spearman_mean_genewise': 0.2953, 'l1_error_mean': 1.9143, 'l2_errors_mean': 7.4549, 'r2_scores_mean': 0.1116, 'pearson_std': 0.1114, 'l2_error_q1': 4.5236, 'l2_error_q2': 6.8891, 'l2_error_q3': 10.281, 'r2_score_q1': 0.054, 'r2_score_q2': 0.0969, 'r2_score_q3': 0.1466, 'mape_mean': 64.2335, 'mape_std': 19.1659, 'rmse_mean': 2.6583, 'rmse_std': 0.6234}
2025-11-14 17:23:13,385 - INFO - Learning rate for epoch 27: 1.0000000000000002e-06
2025-11-14 17:23:13,385 - INFO - No improvement in spearman genewise. Patience: 13/30
2025-11-14 17:23:14,386 - INFO - Fold 1 Train Epoch 28/200, Batch 0, Loss: 7.4515, Pearson: 0.6314, Spearman: 0.6002
2025-11-14 17:23:24,604 - INFO - Fold 1 Train Epoch 28/200, Batch 10, Loss: 7.3417, Pearson: 0.6479, Spearman: 0.6152
2025-11-14 17:23:34,739 - INFO - Fold 1 Train Epoch 28/200, Batch 20, Loss: 6.9101, Pearson: 0.6555, Spearman: 0.6130
2025-11-14 17:23:44,941 - INFO - Fold 1 Train Epoch 28/200, Batch 30, Loss: 7.4516, Pearson: 0.6338, Spearman: 0.6072
2025-11-14 17:23:55,123 - INFO - Fold 1 Train Epoch 28/200, Batch 40, Loss: 7.2054, Pearson: 0.6375, Spearman: 0.6005
2025-11-14 17:24:05,292 - INFO - Fold 1 Train Epoch 28/200, Batch 50, Loss: 7.4113, Pearson: 0.6393, Spearman: 0.6045
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6455
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6512
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.237313747406006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 28 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 0.        7.9898496 7.9898496]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         1.2497196  0.7103065  2.532465   1.6347741
 0.19283119 1.2407069  1.6763988  8.142469  ]
y_true  -> mean=2.1279, std=3.5200, min=0.0000, max=12.8889
y_pred  -> mean=2.1182, std=2.2183, min=0.0000, max=13.5871
Batch 0 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6393
2025-11-14 17:24:15,514 - INFO - Fold 1 Train Epoch 28/200, Batch 60, Loss: 7.1582, Pearson: 0.6362, Spearman: 0.6013
2025-11-14 17:24:25,702 - INFO - Fold 1 Train Epoch 28/200, Batch 70, Loss: 6.9900, Pearson: 0.6385, Spearman: 0.5987
2025-11-14 17:24:35,895 - INFO - Fold 1 Train Epoch 28/200, Batch 80, Loss: 7.2773, Pearson: 0.6485, Spearman: 0.6159
2025-11-14 17:24:40,813 - INFO - Fold 1 Train Epoch 28/200, Train Loss: 7.2365, Pearson Mean: 0.6431, Spearman Mean: 0.6074
2025-11-14 17:24:40,813 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4474, 'spearman_mean_genewise': 0.4034, 'l1_error_mean': 1.8932, 'l2_errors_mean': 7.2328, 'r2_scores_mean': 0.2135, 'pearson_std': 0.1167, 'l2_error_q1': 4.9256, 'l2_error_q2': 6.9536, 'l2_error_q3': 9.2173, 'r2_score_q1': 0.1318, 'r2_score_q2': 0.1845, 'r2_score_q3': 0.2629, 'mape_mean': 58.182, 'mape_std': 18.4369, 'rmse_mean': 2.6449, 'rmse_std': 0.4871}
2025-11-14 17:24:41,180 - INFO - Fold 1 Val Epoch 28/200, Batch 0, Loss: 8.7175, Pearson: 0.5649, Spearman: 0.5504
2025-11-14 17:24:43,198 - INFO - Fold 1 Val Epoch 28/200, Batch 10, Loss: 7.4384, Pearson: 0.6122, Spearman: 0.6213
2025-11-14 17:24:44,938 - INFO - Fold 1 Val Epoch 28/200, Batch 20, Loss: 5.5062, Pearson: 0.5302, Spearman: 0.5368
2025-11-14 17:24:47,450 - INFO - Fold 1 Val Epoch 28/200, Val Loss: 7.4311, Pearson Mean: 0.5704, Spearman Mean: 0.5690
2025-11-14 17:24:47,450 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3269, 'spearman_mean_genewise': 0.296, 'l1_error_mean': 1.9209, 'l2_errors_mean': 7.4544, 'r2_scores_mean': 0.1115, 'pearson_std': 0.1113, 'l2_error_q1': 4.5191, 'l2_error_q2': 6.9046, 'l2_error_q3': 10.2722, 'r2_score_q1': 0.0538, 'r2_score_q2': 0.0965, 'r2_score_q3': 0.1475, 'mape_mean': 63.5437, 'mape_std': 18.996, 'rmse_mean': 2.6582, 'rmse_std': 0.623}
2025-11-14 17:24:47,451 - INFO - Learning rate for epoch 28: 1.0000000000000002e-06
2025-11-14 17:24:47,451 - INFO - No improvement in spearman genewise. Patience: 14/30
2025-11-14 17:24:48,484 - INFO - Fold 1 Train Epoch 29/200, Batch 0, Loss: 7.0889, Pearson: 0.6529, Spearman: 0.6162
2025-11-14 17:24:58,679 - INFO - Fold 1 Train Epoch 29/200, Batch 10, Loss: 7.0430, Pearson: 0.6508, Spearman: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6458
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6286
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.2327985763549805
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 29 =====================
Sample y_true values (first sample, first 10 genes):
[0.        7.0582767 0.        8.156315  0.        7.0582767 0.
 7.0582767 7.0582767 9.003449 ]
Sample y_pred values (first sample, first 10 genes):
[0.59561145 3.003951   1.9559431  1.2531135  4.2503643  2.7474506
 0.52683693 2.0961812  2.7566779  8.7574415 ]
y_true  -> mean=2.1446, std=3.5146, min=0.0000, max=12.7169
y_pred  -> mean=2.1187, std=2.2477, min=0.0000, max=13.4361
Batch 0 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6401
2025-11-14 17:25:08,874 - INFO - Fold 1 Train Epoch 29/200, Batch 20, Loss: 7.0651, Pearson: 0.6514, Spearman: 0.6110
2025-11-14 17:25:19,047 - INFO - Fold 1 Train Epoch 29/200, Batch 30, Loss: 7.2651, Pearson: 0.6356, Spearman: 0.6030
2025-11-14 17:25:29,248 - INFO - Fold 1 Train Epoch 29/200, Batch 40, Loss: 7.1578, Pearson: 0.6398, Spearman: 0.6054
2025-11-14 17:25:39,414 - INFO - Fold 1 Train Epoch 29/200, Batch 50, Loss: 7.1175, Pearson: 0.6435, Spearman: 0.5989
2025-11-14 17:25:49,620 - INFO - Fold 1 Train Epoch 29/200, Batch 60, Loss: 7.5128, Pearson: 0.6330, Spearman: 0.6058
2025-11-14 17:25:59,809 - INFO - Fold 1 Train Epoch 29/200, Batch 70, Loss: 7.1205, Pearson: 0.6543, Spearman: 0.6145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6492
2025-11-14 17:26:10,004 - INFO - Fold 1 Train Epoch 29/200, Batch 80, Loss: 7.4626, Pearson: 0.6372, Spearman: 0.6077
2025-11-14 17:26:14,892 - INFO - Fold 1 Train Epoch 29/200, Train Loss: 7.2270, Pearson Mean: 0.6432, Spearman Mean: 0.6071
2025-11-14 17:26:14,892 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4479, 'spearman_mean_genewise': 0.4036, 'l1_error_mean': 1.8916, 'l2_errors_mean': 7.229, 'r2_scores_mean': 0.2139, 'pearson_std': 0.1166, 'l2_error_q1': 4.9268, 'l2_error_q2': 6.949, 'l2_error_q3': 9.2299, 'r2_score_q1': 0.1325, 'r2_score_q2': 0.1848, 'r2_score_q3': 0.2637, 'mape_mean': 58.1406, 'mape_std': 18.4263, 'rmse_mean': 2.6442, 'rmse_std': 0.487}
2025-11-14 17:26:15,255 - INFO - Fold 1 Val Epoch 29/200, Batch 0, Loss: 8.7205, Pearson: 0.5652, Spearman: 0.5508
2025-11-14 17:26:17,297 - INFO - Fold 1 Val Epoch 29/200, Batch 10, Loss: 7.3982, Pearson: 0.6126, Spearman: 0.6215
2025-11-14 17:26:19,330 - INFO - Fold 1 Val Epoch 29/200, Batch 20, Loss: 5.5241, Pearson: 0.5287, Spearman: 0.5353
2025-11-14 17:26:21,741 - INFO - Fold 1 Val Epoch 29/200, Val Loss: 7.4300, Pearson Mean: 0.5706, Spearman Mean: 0.5691
2025-11-14 17:26:21,741 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3274, 'spearman_mean_genewise': 0.2965, 'l1_error_mean': 1.926, 'l2_errors_mean': 7.4513, 'r2_scores_mean': 0.1118, 'pearson_std': 0.1113, 'l2_error_q1': 4.5189, 'l2_error_q2': 6.8885, 'l2_error_q3': 10.2888, 'r2_score_q1': 0.0551, 'r2_score_q2': 0.097, 'r2_score_q3': 0.1472, 'mape_mean': 63.3112, 'mape_std': 19.1972, 'rmse_mean': 2.6577, 'rmse_std': 0.6228}
2025-11-14 17:26:21,741 - INFO - Learning rate for epoch 29: 1.0000000000000002e-06
2025-11-14 17:26:21,741 - INFO - No improvement in spearman genewise. Patience: 15/30
2025-11-14 17:26:22,748 - INFO - Fold 1 Train Epoch 30/200, Batch 0, Loss: 7.2453, Pearson: 0.6459, Spearman: 0.6093
2025-11-14 17:26:32,841 - INFO - Fold 1 Train Epoch 30/200, Batch 10, Loss: 7.1226, Pearson: 0.6514, Spearman: 0.6096
2025-11-14 17:26:43,028 - INFO - Fold 1 Train Epoch 30/200, Batch 20, Loss: 7.2588, Pearson: 0.6371, Spearman: 0.6041
2025-11-14 17:26:53,207 - INFO - Fold 1 Train Epoch 30/200, Batch 30, Loss: 7.0355, Pearson: 0.6518, Spearman: 0.6140
2025-11-14 17:27:03,404 - INFO - Fold 1 Train Epoch 30/200, Batch 40, Loss: 7.0580, Pearson: 0.6397, Spearman: 0.5959
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6419
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6171
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.2290191650390625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 30 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       8.142478 0.       8.142478
 8.142478 0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.09171    0.         0.93512154 0.         1.8053108  0.7071699
 0.12914956 0.7883673  1.4864106  5.8232083 ]
y_true  -> mean=2.1726, std=3.5244, min=0.0000, max=12.4743
y_pred  -> mean=2.1121, std=2.2210, min=0.0000, max=12.9860
Batch 0 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6396
2025-11-14 17:27:13,579 - INFO - Fold 1 Train Epoch 30/200, Batch 50, Loss: 7.3272, Pearson: 0.6412, Spearman: 0.6120
2025-11-14 17:27:23,749 - INFO - Fold 1 Train Epoch 30/200, Batch 60, Loss: 7.1249, Pearson: 0.6401, Spearman: 0.5996
2025-11-14 17:27:33,946 - INFO - Fold 1 Train Epoch 30/200, Batch 70, Loss: 7.2333, Pearson: 0.6494, Spearman: 0.6102
2025-11-14 17:27:44,136 - INFO - Fold 1 Train Epoch 30/200, Batch 80, Loss: 7.2185, Pearson: 0.6363, Spearman: 0.6058
2025-11-14 17:27:49,051 - INFO - Fold 1 Train Epoch 30/200, Train Loss: 7.2325, Pearson Mean: 0.6433, Spearman Mean: 0.6075
2025-11-14 17:27:49,051 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4477, 'spearman_mean_genewise': 0.4036, 'l1_error_mean': 1.8922, 'l2_errors_mean': 7.2298, 'r2_scores_mean': 0.2138, 'pearson_std': 0.1167, 'l2_error_q1': 4.923, 'l2_error_q2': 6.9621, 'l2_error_q3': 9.2211, 'r2_score_q1': 0.1331, 'r2_score_q2': 0.1848, 'r2_score_q3': 0.2629, 'mape_mean': 58.1682, 'mape_std': 18.4407, 'rmse_mean': 2.6444, 'rmse_std': 0.487}
2025-11-14 17:27:49,430 - INFO - Fold 1 Val Epoch 30/200, Batch 0, Loss: 8.7333, Pearson: 0.5643, Spearman: 0.5500
2025-11-14 17:27:51,399 - INFO - Fold 1 Val Epoch 30/200, Batch 10, Loss: 7.3976, Pearson: 0.6131, Spearman: 0.6220
2025-11-14 17:27:53,433 - INFO - Fold 1 Val Epoch 30/200, Batch 20, Loss: 5.5173, Pearson: 0.5295, Spearman: 0.5361
2025-11-14 17:27:55,867 - INFO - Fold 1 Val Epoch 30/200, Val Loss: 7.4274, Pearson Mean: 0.5709, Spearman Mean: 0.5693
2025-11-14 17:27:55,868 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3276, 'spearman_mean_genewise': 0.2966, 'l1_error_mean': 1.9229, 'l2_errors_mean': 7.4487, 'r2_scores_mean': 0.1121, 'pearson_std': 0.1113, 'l2_error_q1': 4.5214, 'l2_error_q2': 6.8677, 'l2_error_q3': 10.2706, 'r2_score_q1': 0.0549, 'r2_score_q2': 0.0968, 'r2_score_q3': 0.148, 'mape_mean': 63.3438, 'mape_std': 19.1568, 'rmse_mean': 2.6573, 'rmse_std': 0.6224}
2025-11-14 17:27:55,868 - INFO - Learning rate for epoch 30: 1.0000000000000002e-06
2025-11-14 17:27:55,868 - INFO - No improvement in spearman genewise. Patience: 16/30
2025-11-14 17:27:56,807 - INFO - Fold 1 Train Epoch 31/200, Batch 0, Loss: 7.0704, Pearson: 0.6399, Spearman: 0.6063
2025-11-14 17:28:06,822 - INFO - Fold 1 Train Epoch 31/200, Batch 10, Loss: 7.2297, Pearson: 0.6423, Spearman: 0.6044
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6444
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6334
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.22981071472168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 31 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       8.052207 0.       0.       7.359378 0.       0.
 7.359378 7.359378]
Sample y_pred values (first sample, first 10 genes):
[1.1399144  1.8885084  5.636632   0.35574713 6.3682947  4.2076693
 0.63038206 2.9679637  6.148311   7.1847925 ]
y_true  -> mean=1.9868, std=3.4562, min=0.0000, max=12.4292
y_pred  -> mean=2.1137, std=2.2435, min=0.0000, max=13.2660
Batch 0 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6406
2025-11-14 17:28:17,004 - INFO - Fold 1 Train Epoch 31/200, Batch 20, Loss: 7.1150, Pearson: 0.6394, Spearman: 0.6046
2025-11-14 17:28:27,186 - INFO - Fold 1 Train Epoch 31/200, Batch 30, Loss: 7.4476, Pearson: 0.6459, Spearman: 0.6057
2025-11-14 17:28:37,361 - INFO - Fold 1 Train Epoch 31/200, Batch 40, Loss: 7.3271, Pearson: 0.6412, Spearman: 0.6034
2025-11-14 17:28:47,577 - INFO - Fold 1 Train Epoch 31/200, Batch 50, Loss: 7.2850, Pearson: 0.6366, Spearman: 0.5996
2025-11-14 17:28:57,747 - INFO - Fold 1 Train Epoch 31/200, Batch 60, Loss: 7.4012, Pearson: 0.6295, Spearman: 0.5987
2025-11-14 17:29:07,935 - INFO - Fold 1 Train Epoch 31/200, Batch 70, Loss: 7.1625, Pearson: 0.6494, Spearman: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6393
2025-11-14 17:29:18,120 - INFO - Fold 1 Train Epoch 31/200, Batch 80, Loss: 7.2583, Pearson: 0.6440, Spearman: 0.6094
2025-11-14 17:29:23,043 - INFO - Fold 1 Train Epoch 31/200, Train Loss: 7.2379, Pearson Mean: 0.6431, Spearman Mean: 0.6072
2025-11-14 17:29:23,043 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4474, 'spearman_mean_genewise': 0.4033, 'l1_error_mean': 1.8933, 'l2_errors_mean': 7.2334, 'r2_scores_mean': 0.2135, 'pearson_std': 0.1166, 'l2_error_q1': 4.9383, 'l2_error_q2': 6.9525, 'l2_error_q3': 9.2069, 'r2_score_q1': 0.1326, 'r2_score_q2': 0.1846, 'r2_score_q3': 0.2629, 'mape_mean': 58.2034, 'mape_std': 18.4445, 'rmse_mean': 2.645, 'rmse_std': 0.4873}
2025-11-14 17:29:23,396 - INFO - Fold 1 Val Epoch 31/200, Batch 0, Loss: 8.7249, Pearson: 0.5645, Spearman: 0.5502
2025-11-14 17:29:25,460 - INFO - Fold 1 Val Epoch 31/200, Batch 10, Loss: 7.4302, Pearson: 0.6122, Spearman: 0.6211
2025-11-14 17:29:27,382 - INFO - Fold 1 Val Epoch 31/200, Batch 20, Loss: 5.5086, Pearson: 0.5289, Spearman: 0.5357
2025-11-14 17:29:29,995 - INFO - Fold 1 Val Epoch 31/200, Val Loss: 7.4254, Pearson Mean: 0.5709, Spearman Mean: 0.5694
2025-11-14 17:29:29,996 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.327, 'spearman_mean_genewise': 0.296, 'l1_error_mean': 1.9132, 'l2_errors_mean': 7.4479, 'r2_scores_mean': 0.1122, 'pearson_std': 0.1115, 'l2_error_q1': 4.5263, 'l2_error_q2': 6.8663, 'l2_error_q3': 10.2614, 'r2_score_q1': 0.0546, 'r2_score_q2': 0.0973, 'r2_score_q3': 0.1476, 'mape_mean': 63.8929, 'mape_std': 19.2084, 'rmse_mean': 2.6572, 'rmse_std': 0.6223}
2025-11-14 17:29:29,996 - INFO - Learning rate for epoch 31: 1.0000000000000002e-06
2025-11-14 17:29:29,996 - INFO - No improvement in spearman genewise. Patience: 17/30
2025-11-14 17:29:31,015 - INFO - Fold 1 Train Epoch 32/200, Batch 0, Loss: 7.0135, Pearson: 0.6484, Spearman: 0.6127
2025-11-14 17:29:41,088 - INFO - Fold 1 Train Epoch 32/200, Batch 10, Loss: 7.3825, Pearson: 0.6434, Spearman: 0.6127
2025-11-14 17:29:51,271 - INFO - Fold 1 Train Epoch 32/200, Batch 20, Loss: 7.1679, Pearson: 0.6372, Spearman: 0.6090
2025-11-14 17:30:01,438 - INFO - Fold 1 Train Epoch 32/200, Batch 30, Loss: 7.0947, Pearson: 0.6508, Spearman: 0.6144
2025-11-14 17:30:11,639 - INFO - Fold 1 Train Epoch 32/200, Batch 40, Loss: 7.1210, Pearson: 0.6433, Spearman: 0.6028
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6434
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6302
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.233362674713135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 32 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.18794224 0.         0.91473234 0.         0.838538   0.25619596
 0.         0.2171649  0.722902   4.023231  ]
y_true  -> mean=2.0714, std=3.4783, min=0.0000, max=12.6105
y_pred  -> mean=2.1080, std=2.2291, min=0.0000, max=13.9249
Batch 0 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6426
2025-11-14 17:30:21,821 - INFO - Fold 1 Train Epoch 32/200, Batch 50, Loss: 7.1932, Pearson: 0.6438, Spearman: 0.6055
2025-11-14 17:30:32,009 - INFO - Fold 1 Train Epoch 32/200, Batch 60, Loss: 7.2625, Pearson: 0.6380, Spearman: 0.6043
2025-11-14 17:30:42,206 - INFO - Fold 1 Train Epoch 32/200, Batch 70, Loss: 7.1727, Pearson: 0.6323, Spearman: 0.5937
2025-11-14 17:30:52,393 - INFO - Fold 1 Train Epoch 32/200, Batch 80, Loss: 7.1850, Pearson: 0.6389, Spearman: 0.6060
2025-11-14 17:30:57,241 - INFO - Fold 1 Train Epoch 32/200, Train Loss: 7.2323, Pearson Mean: 0.6432, Spearman Mean: 0.6074
2025-11-14 17:30:57,241 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4477, 'spearman_mean_genewise': 0.4035, 'l1_error_mean': 1.8923, 'l2_errors_mean': 7.2308, 'r2_scores_mean': 0.2137, 'pearson_std': 0.1166, 'l2_error_q1': 4.9172, 'l2_error_q2': 6.958, 'l2_error_q3': 9.2273, 'r2_score_q1': 0.1328, 'r2_score_q2': 0.184, 'r2_score_q3': 0.2633, 'mape_mean': 58.179, 'mape_std': 18.4379, 'rmse_mean': 2.6445, 'rmse_std': 0.4872}
2025-11-14 17:30:57,520 - INFO - Fold 1 Val Epoch 32/200, Batch 0, Loss: 8.7114, Pearson: 0.5654, Spearman: 0.5511
2025-11-14 17:30:59,467 - INFO - Fold 1 Val Epoch 32/200, Batch 10, Loss: 7.4111, Pearson: 0.6122, Spearman: 0.6209
2025-11-14 17:31:01,538 - INFO - Fold 1 Val Epoch 32/200, Batch 20, Loss: 5.5092, Pearson: 0.5302, Spearman: 0.5365
2025-11-14 17:31:04,058 - INFO - Fold 1 Val Epoch 32/200, Val Loss: 7.4263, Pearson Mean: 0.5708, Spearman Mean: 0.5692
2025-11-14 17:31:04,059 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3274, 'spearman_mean_genewise': 0.2964, 'l1_error_mean': 1.9193, 'l2_errors_mean': 7.4489, 'r2_scores_mean': 0.1122, 'pearson_std': 0.1115, 'l2_error_q1': 4.5241, 'l2_error_q2': 6.8832, 'l2_error_q3': 10.2588, 'r2_score_q1': 0.0548, 'r2_score_q2': 0.0974, 'r2_score_q3': 0.1481, 'mape_mean': 63.5902, 'mape_std': 19.1882, 'rmse_mean': 2.6573, 'rmse_std': 0.6227}
2025-11-14 17:31:04,059 - INFO - Learning rate for epoch 32: 1.0000000000000002e-06
2025-11-14 17:31:04,059 - INFO - No improvement in spearman genewise. Patience: 18/30
2025-11-14 17:31:04,917 - INFO - Fold 1 Train Epoch 33/200, Batch 0, Loss: 7.3849, Pearson: 0.6411, Spearman: 0.6068
2025-11-14 17:31:15,160 - INFO - Fold 1 Train Epoch 33/200, Batch 10, Loss: 7.2079, Pearson: 0.6441, Spearman: 0.6068
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6376
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6197
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.230825901031494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 33 =====================
Sample y_true values (first sample, first 10 genes):
[6.958947 6.958947 6.958947 0.       7.651619 8.056926 0.       0.
 0.       8.749914]
Sample y_pred values (first sample, first 10 genes):
[0.8104961  4.0414824  3.2481978  0.36116785 4.283772   4.32358
 0.49748406 1.4694748  3.1749     7.106917  ]
y_true  -> mean=2.1914, std=3.5393, min=0.0000, max=12.5034
y_pred  -> mean=2.1182, std=2.2251, min=0.0000, max=13.2473
Batch 0 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6401
2025-11-14 17:31:25,352 - INFO - Fold 1 Train Epoch 33/200, Batch 20, Loss: 7.5274, Pearson: 0.6354, Spearman: 0.6076
2025-11-14 17:31:35,528 - INFO - Fold 1 Train Epoch 33/200, Batch 30, Loss: 7.2943, Pearson: 0.6419, Spearman: 0.6109
2025-11-14 17:31:45,729 - INFO - Fold 1 Train Epoch 33/200, Batch 40, Loss: 7.2254, Pearson: 0.6371, Spearman: 0.6048
2025-11-14 17:31:55,927 - INFO - Fold 1 Train Epoch 33/200, Batch 50, Loss: 7.2443, Pearson: 0.6472, Spearman: 0.6114
2025-11-14 17:32:06,129 - INFO - Fold 1 Train Epoch 33/200, Batch 60, Loss: 7.1158, Pearson: 0.6509, Spearman: 0.6059
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6479
2025-11-14 17:32:16,296 - INFO - Fold 1 Train Epoch 33/200, Batch 70, Loss: 7.5864, Pearson: 0.6324, Spearman: 0.6059
2025-11-14 17:32:26,494 - INFO - Fold 1 Train Epoch 33/200, Batch 80, Loss: 7.2732, Pearson: 0.6354, Spearman: 0.6049
2025-11-14 17:32:31,430 - INFO - Fold 1 Train Epoch 33/200, Train Loss: 7.2261, Pearson Mean: 0.6435, Spearman Mean: 0.6075
2025-11-14 17:32:31,431 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4479, 'spearman_mean_genewise': 0.4037, 'l1_error_mean': 1.8925, 'l2_errors_mean': 7.2289, 'r2_scores_mean': 0.2139, 'pearson_std': 0.1167, 'l2_error_q1': 4.9237, 'l2_error_q2': 6.9432, 'l2_error_q3': 9.2277, 'r2_score_q1': 0.133, 'r2_score_q2': 0.1852, 'r2_score_q3': 0.2636, 'mape_mean': 58.1837, 'mape_std': 18.4422, 'rmse_mean': 2.6442, 'rmse_std': 0.487}
2025-11-14 17:32:31,769 - INFO - Fold 1 Val Epoch 33/200, Batch 0, Loss: 8.7647, Pearson: 0.5620, Spearman: 0.5475
2025-11-14 17:32:33,748 - INFO - Fold 1 Val Epoch 33/200, Batch 10, Loss: 7.4075, Pearson: 0.6120, Spearman: 0.6211
2025-11-14 17:32:35,614 - INFO - Fold 1 Val Epoch 33/200, Batch 20, Loss: 5.5279, Pearson: 0.5298, Spearman: 0.5363
2025-11-14 17:32:38,126 - INFO - Fold 1 Val Epoch 33/200, Val Loss: 7.4466, Pearson Mean: 0.5700, Spearman Mean: 0.5684
2025-11-14 17:32:38,127 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.326, 'spearman_mean_genewise': 0.2958, 'l1_error_mean': 1.9346, 'l2_errors_mean': 7.4688, 'r2_scores_mean': 0.11, 'pearson_std': 0.1113, 'l2_error_q1': 4.5167, 'l2_error_q2': 6.9157, 'l2_error_q3': 10.3222, 'r2_score_q1': 0.0535, 'r2_score_q2': 0.0944, 'r2_score_q3': 0.146, 'mape_mean': 63.1438, 'mape_std': 19.22, 'rmse_mean': 2.6607, 'rmse_std': 0.6241}
2025-11-14 17:32:38,127 - INFO - Learning rate for epoch 33: 1.0000000000000002e-07
2025-11-14 17:32:38,127 - INFO - No improvement in spearman genewise. Patience: 19/30
2025-11-14 17:32:39,162 - INFO - Fold 1 Train Epoch 34/200, Batch 0, Loss: 7.1386, Pearson: 0.6381, Spearman: 0.6028
2025-11-14 17:32:49,352 - INFO - Fold 1 Train Epoch 34/200, Batch 10, Loss: 7.1160, Pearson: 0.6461, Spearman: 0.6121
2025-11-14 17:32:59,533 - INFO - Fold 1 Train Epoch 34/200, Batch 20, Loss: 7.2602, Pearson: 0.6493, Spearman: 0.6131
2025-11-14 17:33:09,746 - INFO - Fold 1 Train Epoch 34/200, Batch 30, Loss: 7.3578, Pearson: 0.6413, Spearman: 0.6074
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6305
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6461
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.228916168212891
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 34 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       9.721226 0.       9.721226 0.       0.       0.
 0.       0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.02299535 0.         0.5389568  0.         0.99732405 0.
 0.         0.32770568 1.0602329  4.348032  ]
y_true  -> mean=2.0375, std=3.4689, min=0.0000, max=12.5628
y_pred  -> mean=2.1073, std=2.2218, min=0.0000, max=13.0477
Batch 0 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6368
2025-11-14 17:33:19,922 - INFO - Fold 1 Train Epoch 34/200, Batch 40, Loss: 7.1014, Pearson: 0.6489, Spearman: 0.6161
2025-11-14 17:33:30,138 - INFO - Fold 1 Train Epoch 34/200, Batch 50, Loss: 7.1543, Pearson: 0.6462, Spearman: 0.6096
2025-11-14 17:33:40,347 - INFO - Fold 1 Train Epoch 34/200, Batch 60, Loss: 6.9899, Pearson: 0.6390, Spearman: 0.5946
2025-11-14 17:33:50,571 - INFO - Fold 1 Train Epoch 34/200, Batch 70, Loss: 7.0549, Pearson: 0.6367, Spearman: 0.6041
2025-11-14 17:34:00,759 - INFO - Fold 1 Train Epoch 34/200, Batch 80, Loss: 7.4807, Pearson: 0.6495, Spearman: 0.6134
2025-11-14 17:34:05,715 - INFO - Fold 1 Train Epoch 34/200, Train Loss: 7.2306, Pearson Mean: 0.6434, Spearman Mean: 0.6074
2025-11-14 17:34:05,715 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4478, 'spearman_mean_genewise': 0.4035, 'l1_error_mean': 1.8921, 'l2_errors_mean': 7.2299, 'r2_scores_mean': 0.2138, 'pearson_std': 0.1165, 'l2_error_q1': 4.9208, 'l2_error_q2': 6.9565, 'l2_error_q3': 9.2211, 'r2_score_q1': 0.1321, 'r2_score_q2': 0.1849, 'r2_score_q3': 0.2634, 'mape_mean': 58.1662, 'mape_std': 18.442, 'rmse_mean': 2.6444, 'rmse_std': 0.487}
2025-11-14 17:34:06,017 - INFO - Fold 1 Val Epoch 34/200, Batch 0, Loss: 8.7341, Pearson: 0.5634, Spearman: 0.5493
2025-11-14 17:34:07,866 - INFO - Fold 1 Val Epoch 34/200, Batch 10, Loss: 7.4323, Pearson: 0.6127, Spearman: 0.6214
2025-11-14 17:34:09,737 - INFO - Fold 1 Val Epoch 34/200, Batch 20, Loss: 5.4959, Pearson: 0.5304, Spearman: 0.5371
2025-11-14 17:34:12,205 - INFO - Fold 1 Val Epoch 34/200, Val Loss: 7.4243, Pearson Mean: 0.5709, Spearman Mean: 0.5694
2025-11-14 17:34:12,206 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3269, 'spearman_mean_genewise': 0.2959, 'l1_error_mean': 1.9088, 'l2_errors_mean': 7.4476, 'r2_scores_mean': 0.1122, 'pearson_std': 0.1114, 'l2_error_q1': 4.5229, 'l2_error_q2': 6.8632, 'l2_error_q3': 10.2721, 'r2_score_q1': 0.0542, 'r2_score_q2': 0.0976, 'r2_score_q3': 0.1476, 'mape_mean': 64.1087, 'mape_std': 19.1086, 'rmse_mean': 2.6572, 'rmse_std': 0.6222}
2025-11-14 17:34:12,206 - INFO - Learning rate for epoch 34: 1.0000000000000002e-07
2025-11-14 17:34:12,206 - INFO - No improvement in spearman genewise. Patience: 20/30
2025-11-14 17:34:13,252 - INFO - Fold 1 Train Epoch 35/200, Batch 0, Loss: 7.1885, Pearson: 0.6469, Spearman: 0.6068
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6464
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6435
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.229886054992676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 35 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        7.9694176 0.        0.
 7.9694176 8.662392  8.662392 ]
Sample y_pred values (first sample, first 10 genes):
[0.06958438 0.         1.7609477  0.         2.702987   1.1290674
 0.06527656 0.8196787  2.2682471  8.265433  ]
y_true  -> mean=2.1501, std=3.5155, min=0.0000, max=12.6005
y_pred  -> mean=2.1157, std=2.2610, min=0.0000, max=12.9293
Batch 0 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6486
2025-11-14 17:34:23,468 - INFO - Fold 1 Train Epoch 35/200, Batch 10, Loss: 7.3627, Pearson: 0.6300, Spearman: 0.6029
2025-11-14 17:34:33,682 - INFO - Fold 1 Train Epoch 35/200, Batch 20, Loss: 7.1120, Pearson: 0.6450, Spearman: 0.6026
2025-11-14 17:34:43,851 - INFO - Fold 1 Train Epoch 35/200, Batch 30, Loss: 7.1081, Pearson: 0.6494, Spearman: 0.6086
2025-11-14 17:34:54,062 - INFO - Fold 1 Train Epoch 35/200, Batch 40, Loss: 7.3999, Pearson: 0.6390, Spearman: 0.6059
2025-11-14 17:35:04,251 - INFO - Fold 1 Train Epoch 35/200, Batch 50, Loss: 7.5225, Pearson: 0.6384, Spearman: 0.6030
2025-11-14 17:35:14,452 - INFO - Fold 1 Train Epoch 35/200, Batch 60, Loss: 7.2707, Pearson: 0.6432, Spearman: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6407
2025-11-14 17:35:24,671 - INFO - Fold 1 Train Epoch 35/200, Batch 70, Loss: 7.1448, Pearson: 0.6483, Spearman: 0.6113
2025-11-14 17:35:34,844 - INFO - Fold 1 Train Epoch 35/200, Batch 80, Loss: 7.2262, Pearson: 0.6524, Spearman: 0.6106
2025-11-14 17:35:39,318 - INFO - Fold 1 Train Epoch 35/200, Train Loss: 7.2256, Pearson Mean: 0.6436, Spearman Mean: 0.6076
2025-11-14 17:35:39,318 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4479, 'spearman_mean_genewise': 0.4036, 'l1_error_mean': 1.8918, 'l2_errors_mean': 7.2284, 'r2_scores_mean': 0.2139, 'pearson_std': 0.1166, 'l2_error_q1': 4.9136, 'l2_error_q2': 6.9516, 'l2_error_q3': 9.2206, 'r2_score_q1': 0.1322, 'r2_score_q2': 0.1852, 'r2_score_q3': 0.2634, 'mape_mean': 58.1657, 'mape_std': 18.4491, 'rmse_mean': 2.6441, 'rmse_std': 0.4868}
2025-11-14 17:35:39,686 - INFO - Fold 1 Val Epoch 35/200, Batch 0, Loss: 8.7553, Pearson: 0.5626, Spearman: 0.5483
2025-11-14 17:35:41,757 - INFO - Fold 1 Val Epoch 35/200, Batch 10, Loss: 7.4028, Pearson: 0.6131, Spearman: 0.6215
2025-11-14 17:35:43,407 - INFO - Fold 1 Val Epoch 35/200, Batch 20, Loss: 5.5141, Pearson: 0.5306, Spearman: 0.5368
2025-11-14 17:35:45,852 - INFO - Fold 1 Val Epoch 35/200, Val Loss: 7.4278, Pearson Mean: 0.5711, Spearman Mean: 0.5694
2025-11-14 17:35:45,853 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3267, 'spearman_mean_genewise': 0.2959, 'l1_error_mean': 1.9162, 'l2_errors_mean': 7.4499, 'r2_scores_mean': 0.112, 'pearson_std': 0.1115, 'l2_error_q1': 4.523, 'l2_error_q2': 6.8663, 'l2_error_q3': 10.2754, 'r2_score_q1': 0.0547, 'r2_score_q2': 0.0972, 'r2_score_q3': 0.1479, 'mape_mean': 63.9147, 'mape_std': 19.2882, 'rmse_mean': 2.6575, 'rmse_std': 0.6226}
2025-11-14 17:35:45,853 - INFO - Learning rate for epoch 35: 1.0000000000000002e-07
2025-11-14 17:35:45,853 - INFO - No improvement in spearman genewise. Patience: 21/30
2025-11-14 17:35:46,875 - INFO - Fold 1 Train Epoch 36/200, Batch 0, Loss: 7.0538, Pearson: 0.6397, Spearman: 0.6056
2025-11-14 17:35:56,982 - INFO - Fold 1 Train Epoch 36/200, Batch 10, Loss: 7.4391, Pearson: 0.6333, Spearman: 0.6094
2025-11-14 17:36:07,174 - INFO - Fold 1 Train Epoch 36/200, Batch 20, Loss: 7.2333, Pearson: 0.6375, Spearman: 0.5988
2025-11-14 17:36:17,368 - INFO - Fold 1 Train Epoch 36/200, Batch 30, Loss: 7.1804, Pearson: 0.6411, Spearman: 0.6067
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6452
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6597
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.2284111976623535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 36 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.902991]
Sample y_pred values (first sample, first 10 genes):
[0.24095032 0.         2.1669898  0.         3.283029   1.437694
 0.1652877  0.92316484 2.3878348  8.275057  ]
y_true  -> mean=1.9925, std=3.4519, min=0.0000, max=12.6014
y_pred  -> mean=2.1108, std=2.2308, min=0.0000, max=13.4496
Batch 0 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6447
2025-11-14 17:36:27,577 - INFO - Fold 1 Train Epoch 36/200, Batch 40, Loss: 7.1994, Pearson: 0.6445, Spearman: 0.6081
2025-11-14 17:36:37,757 - INFO - Fold 1 Train Epoch 36/200, Batch 50, Loss: 7.0558, Pearson: 0.6517, Spearman: 0.6094
2025-11-14 17:36:47,962 - INFO - Fold 1 Train Epoch 36/200, Batch 60, Loss: 7.2647, Pearson: 0.6419, Spearman: 0.6026
2025-11-14 17:36:58,147 - INFO - Fold 1 Train Epoch 36/200, Batch 70, Loss: 7.2398, Pearson: 0.6456, Spearman: 0.6065
2025-11-14 17:37:08,350 - INFO - Fold 1 Train Epoch 36/200, Batch 80, Loss: 7.1332, Pearson: 0.6420, Spearman: 0.6061
2025-11-14 17:37:12,920 - INFO - Fold 1 Train Epoch 36/200, Train Loss: 7.2290, Pearson Mean: 0.6435, Spearman Mean: 0.6075
2025-11-14 17:37:12,920 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4481, 'spearman_mean_genewise': 0.4038, 'l1_error_mean': 1.8917, 'l2_errors_mean': 7.2267, 'r2_scores_mean': 0.2141, 'pearson_std': 0.1167, 'l2_error_q1': 4.9014, 'l2_error_q2': 6.9489, 'l2_error_q3': 9.2014, 'r2_score_q1': 0.1331, 'r2_score_q2': 0.1853, 'r2_score_q3': 0.2638, 'mape_mean': 58.1548, 'mape_std': 18.4406, 'rmse_mean': 2.6438, 'rmse_std': 0.4869}
2025-11-14 17:37:13,272 - INFO - Fold 1 Val Epoch 36/200, Batch 0, Loss: 8.7335, Pearson: 0.5638, Spearman: 0.5495
2025-11-14 17:37:15,348 - INFO - Fold 1 Val Epoch 36/200, Batch 10, Loss: 7.4039, Pearson: 0.6125, Spearman: 0.6211
2025-11-14 17:37:16,898 - INFO - Fold 1 Val Epoch 36/200, Batch 20, Loss: 5.5139, Pearson: 0.5297, Spearman: 0.5362
2025-11-14 17:37:19,436 - INFO - Fold 1 Val Epoch 36/200, Val Loss: 7.4235, Pearson Mean: 0.5711, Spearman Mean: 0.5694
2025-11-14 17:37:19,437 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3273, 'spearman_mean_genewise': 0.2964, 'l1_error_mean': 1.9166, 'l2_errors_mean': 7.4458, 'r2_scores_mean': 0.1124, 'pearson_std': 0.1115, 'l2_error_q1': 4.5226, 'l2_error_q2': 6.8622, 'l2_error_q3': 10.2564, 'r2_score_q1': 0.0552, 'r2_score_q2': 0.0981, 'r2_score_q3': 0.1477, 'mape_mean': 63.7745, 'mape_std': 19.2285, 'rmse_mean': 2.6568, 'rmse_std': 0.6223}
2025-11-14 17:37:19,437 - INFO - Learning rate for epoch 36: 1.0000000000000002e-07
2025-11-14 17:37:19,437 - INFO - No improvement in spearman genewise. Patience: 22/30
2025-11-14 17:37:20,465 - INFO - Fold 1 Train Epoch 37/200, Batch 0, Loss: 7.3969, Pearson: 0.6387, Spearman: 0.6039
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6410
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6320
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.226706027984619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 37 =====================
Sample y_true values (first sample, first 10 genes):
[0.        6.388266  7.7732987 6.388266  7.7732987 6.388266  0.
 0.        0.        8.689337 ]
Sample y_pred values (first sample, first 10 genes):
[1.1061579 4.902113  4.0977993 2.7228577 6.800585  5.4538484 1.3256156
 1.8125823 4.3620057 7.820586 ]
y_true  -> mean=2.1617, std=3.5340, min=0.0000, max=12.6199
y_pred  -> mean=2.1164, std=2.2351, min=0.0000, max=13.1783
Batch 0 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6442
2025-11-14 17:37:30,727 - INFO - Fold 1 Train Epoch 37/200, Batch 10, Loss: 7.1921, Pearson: 0.6477, Spearman: 0.6116
2025-11-14 17:37:40,886 - INFO - Fold 1 Train Epoch 37/200, Batch 20, Loss: 7.1657, Pearson: 0.6517, Spearman: 0.6113
2025-11-14 17:37:51,075 - INFO - Fold 1 Train Epoch 37/200, Batch 30, Loss: 7.2825, Pearson: 0.6365, Spearman: 0.6016
2025-11-14 17:38:01,277 - INFO - Fold 1 Train Epoch 37/200, Batch 40, Loss: 7.2150, Pearson: 0.6491, Spearman: 0.6119
2025-11-14 17:38:11,477 - INFO - Fold 1 Train Epoch 37/200, Batch 50, Loss: 7.3448, Pearson: 0.6451, Spearman: 0.6124
2025-11-14 17:38:21,669 - INFO - Fold 1 Train Epoch 37/200, Batch 60, Loss: 7.3847, Pearson: 0.6396, Spearman: 0.6021
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6420
2025-11-14 17:38:31,861 - INFO - Fold 1 Train Epoch 37/200, Batch 70, Loss: 7.3039, Pearson: 0.6341, Spearman: 0.6033
2025-11-14 17:38:42,067 - INFO - Fold 1 Train Epoch 37/200, Batch 80, Loss: 6.9947, Pearson: 0.6502, Spearman: 0.6130
2025-11-14 17:38:46,197 - INFO - Fold 1 Train Epoch 37/200, Train Loss: 7.2312, Pearson Mean: 0.6434, Spearman Mean: 0.6075
2025-11-14 17:38:46,197 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4477, 'spearman_mean_genewise': 0.4035, 'l1_error_mean': 1.8927, 'l2_errors_mean': 7.2313, 'r2_scores_mean': 0.2137, 'pearson_std': 0.1165, 'l2_error_q1': 4.9118, 'l2_error_q2': 6.9587, 'l2_error_q3': 9.2284, 'r2_score_q1': 0.133, 'r2_score_q2': 0.1844, 'r2_score_q3': 0.2634, 'mape_mean': 58.1868, 'mape_std': 18.444, 'rmse_mean': 2.6446, 'rmse_std': 0.4871}
2025-11-14 17:38:46,567 - INFO - Fold 1 Val Epoch 37/200, Batch 0, Loss: 8.7647, Pearson: 0.5622, Spearman: 0.5476
2025-11-14 17:38:48,371 - INFO - Fold 1 Val Epoch 37/200, Batch 10, Loss: 7.4037, Pearson: 0.6120, Spearman: 0.6208
2025-11-14 17:38:49,620 - INFO - Fold 1 Val Epoch 37/200, Batch 20, Loss: 5.5347, Pearson: 0.5292, Spearman: 0.5356
2025-11-14 17:38:52,098 - INFO - Fold 1 Val Epoch 37/200, Val Loss: 7.4408, Pearson Mean: 0.5703, Spearman Mean: 0.5686
2025-11-14 17:38:52,099 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3265, 'spearman_mean_genewise': 0.296, 'l1_error_mean': 1.9306, 'l2_errors_mean': 7.4626, 'r2_scores_mean': 0.1108, 'pearson_std': 0.1113, 'l2_error_q1': 4.5204, 'l2_error_q2': 6.9058, 'l2_error_q3': 10.301, 'r2_score_q1': 0.0547, 'r2_score_q2': 0.0956, 'r2_score_q3': 0.1465, 'mape_mean': 63.3008, 'mape_std': 19.2742, 'rmse_mean': 2.6596, 'rmse_std': 0.6237}
2025-11-14 17:38:52,099 - INFO - Learning rate for epoch 37: 1.0000000000000002e-07
2025-11-14 17:38:52,099 - INFO - No improvement in spearman genewise. Patience: 23/30
2025-11-14 17:38:52,992 - INFO - Fold 1 Train Epoch 38/200, Batch 0, Loss: 7.3162, Pearson: 0.6406, Spearman: 0.6094
2025-11-14 17:39:03,005 - INFO - Fold 1 Train Epoch 38/200, Batch 10, Loss: 7.2282, Pearson: 0.6465, Spearman: 0.6069
2025-11-14 17:39:13,161 - INFO - Fold 1 Train Epoch 38/200, Batch 20, Loss: 7.1602, Pearson: 0.6361, Spearman: 0.6015
2025-11-14 17:39:23,366 - INFO - Fold 1 Train Epoch 38/200, Batch 30, Loss: 6.9585, Pearson: 0.6347, Spearman: 0.5974
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6396
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6444
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.231295585632324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 38 =====================
Sample y_true values (first sample, first 10 genes):
[6.3340425 0.        0.        0.        0.        6.3340425 0.
 0.        7.026302  8.124322 ]
Sample y_pred values (first sample, first 10 genes):
[0.        1.367176  1.2782335 1.3637645 5.1993284 2.689918  0.9532449
 2.0014026 5.9897995 8.847473 ]
y_true  -> mean=2.1421, std=3.5222, min=0.0000, max=12.6548
y_pred  -> mean=2.1137, std=2.2323, min=0.0000, max=13.7862
Batch 0 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6347
2025-11-14 17:39:33,564 - INFO - Fold 1 Train Epoch 38/200, Batch 40, Loss: 7.1470, Pearson: 0.6422, Spearman: 0.6006
2025-11-14 17:39:43,770 - INFO - Fold 1 Train Epoch 38/200, Batch 50, Loss: 7.2709, Pearson: 0.6413, Spearman: 0.6106
2025-11-14 17:39:53,956 - INFO - Fold 1 Train Epoch 38/200, Batch 60, Loss: 7.3119, Pearson: 0.6397, Spearman: 0.6043
2025-11-14 17:40:04,153 - INFO - Fold 1 Train Epoch 38/200, Batch 70, Loss: 7.2503, Pearson: 0.6441, Spearman: 0.6103
2025-11-14 17:40:14,332 - INFO - Fold 1 Train Epoch 38/200, Batch 80, Loss: 7.1812, Pearson: 0.6341, Spearman: 0.5973
2025-11-14 17:40:18,405 - INFO - Fold 1 Train Epoch 38/200, Train Loss: 7.2282, Pearson Mean: 0.6433, Spearman Mean: 0.6074
2025-11-14 17:40:18,406 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4479, 'spearman_mean_genewise': 0.4036, 'l1_error_mean': 1.8925, 'l2_errors_mean': 7.2298, 'r2_scores_mean': 0.2138, 'pearson_std': 0.1165, 'l2_error_q1': 4.9202, 'l2_error_q2': 6.9518, 'l2_error_q3': 9.2301, 'r2_score_q1': 0.1329, 'r2_score_q2': 0.1844, 'r2_score_q3': 0.2623, 'mape_mean': 58.1812, 'mape_std': 18.4381, 'rmse_mean': 2.6443, 'rmse_std': 0.4872}
2025-11-14 17:40:18,777 - INFO - Fold 1 Val Epoch 38/200, Batch 0, Loss: 8.7659, Pearson: 0.5620, Spearman: 0.5476
2025-11-14 17:40:20,788 - INFO - Fold 1 Val Epoch 38/200, Batch 10, Loss: 7.3905, Pearson: 0.6121, Spearman: 0.6209
2025-11-14 17:40:22,016 - INFO - Fold 1 Val Epoch 38/200, Batch 20, Loss: 5.5428, Pearson: 0.5272, Spearman: 0.5345
2025-11-14 17:40:24,469 - INFO - Fold 1 Val Epoch 38/200, Val Loss: 7.4494, Pearson Mean: 0.5695, Spearman Mean: 0.5681
2025-11-14 17:40:24,469 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3257, 'spearman_mean_genewise': 0.2953, 'l1_error_mean': 1.9385, 'l2_errors_mean': 7.4718, 'r2_scores_mean': 0.1097, 'pearson_std': 0.1109, 'l2_error_q1': 4.5219, 'l2_error_q2': 6.9297, 'l2_error_q3': 10.3358, 'r2_score_q1': 0.0545, 'r2_score_q2': 0.0942, 'r2_score_q3': 0.1455, 'mape_mean': 63.1206, 'mape_std': 19.2574, 'rmse_mean': 2.6613, 'rmse_std': 0.6239}
2025-11-14 17:40:24,469 - INFO - Learning rate for epoch 38: 1.0000000000000002e-07
2025-11-14 17:40:24,469 - INFO - No improvement in spearman genewise. Patience: 24/30
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6462
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6277
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.229753017425537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 39 =====================
Sample y_true values (first sample, first 10 genes):
[0.       6.905762 0.       0.       8.291305 0.       0.       7.598408
 8.291305 7.598408]
Sample y_pred values (first sample, first 10 genes):
[0.8349822 2.6997514 1.9654839 2.0367057 4.936608  2.8643136 1.1818076
 2.009729  3.1937666 9.057486 ]
2025-11-14 17:40:25,381 - INFO - Fold 1 Train Epoch 39/200, Batch 0, Loss: 7.4858, Pearson: 0.6463, Spearman: 0.6154
2025-11-14 17:40:35,607 - INFO - Fold 1 Train Epoch 39/200, Batch 10, Loss: 7.3340, Pearson: 0.6421, Spearman: 0.6034
2025-11-14 17:40:45,807 - INFO - Fold 1 Train Epoch 39/200, Batch 20, Loss: 7.2024, Pearson: 0.6500, Spearman: 0.6140
2025-11-14 17:40:55,993 - INFO - Fold 1 Train Epoch 39/200, Batch 30, Loss: 7.4083, Pearson: 0.6379, Spearman: 0.6041
2025-11-14 17:41:06,175 - INFO - Fold 1 Train Epoch 39/200, Batch 40, Loss: 7.2172, Pearson: 0.6495, Spearman: 0.6064
2025-11-14 17:41:16,370 - INFO - Fold 1 Train Epoch 39/200, Batch 50, Loss: 6.9603, Pearson: 0.6448, Spearman: 0.6039
y_true  -> mean=2.3110, std=3.5744, min=0.0000, max=12.4292
y_pred  -> mean=2.1186, std=2.2147, min=0.0000, max=13.4092
Batch 0 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6558
2025-11-14 17:41:26,588 - INFO - Fold 1 Train Epoch 39/200, Batch 60, Loss: 7.4302, Pearson: 0.6372, Spearman: 0.6048
2025-11-14 17:41:36,742 - INFO - Fold 1 Train Epoch 39/200, Batch 70, Loss: 7.1380, Pearson: 0.6325, Spearman: 0.5977
2025-11-14 17:41:46,935 - INFO - Fold 1 Train Epoch 39/200, Batch 80, Loss: 7.1338, Pearson: 0.6469, Spearman: 0.6092
2025-11-14 17:41:51,060 - INFO - Fold 1 Train Epoch 39/200, Train Loss: 7.2262, Pearson Mean: 0.6435, Spearman Mean: 0.6075
2025-11-14 17:41:51,061 - INFO - Training Metrics: {'pearson_mean_genewise': 0.448, 'spearman_mean_genewise': 0.4039, 'l1_error_mean': 1.8922, 'l2_errors_mean': 7.2277, 'r2_scores_mean': 0.214, 'pearson_std': 0.1165, 'l2_error_q1': 4.9177, 'l2_error_q2': 6.9537, 'l2_error_q3': 9.204, 'r2_score_q1': 0.1324, 'r2_score_q2': 0.185, 'r2_score_q3': 0.2634, 'mape_mean': 58.171, 'mape_std': 18.4409, 'rmse_mean': 2.644, 'rmse_std': 0.4868}
2025-11-14 17:41:51,374 - INFO - Fold 1 Val Epoch 39/200, Batch 0, Loss: 8.7413, Pearson: 0.5632, Spearman: 0.5489
2025-11-14 17:41:53,377 - INFO - Fold 1 Val Epoch 39/200, Batch 10, Loss: 7.4041, Pearson: 0.6126, Spearman: 0.6215
2025-11-14 17:41:54,587 - INFO - Fold 1 Val Epoch 39/200, Batch 20, Loss: 5.5204, Pearson: 0.5282, Spearman: 0.5350
2025-11-14 17:41:57,042 - INFO - Fold 1 Val Epoch 39/200, Val Loss: 7.4310, Pearson Mean: 0.5703, Spearman Mean: 0.5688
2025-11-14 17:41:57,042 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.327, 'spearman_mean_genewise': 0.2962, 'l1_error_mean': 1.9254, 'l2_errors_mean': 7.4537, 'r2_scores_mean': 0.1114, 'pearson_std': 0.1112, 'l2_error_q1': 4.5179, 'l2_error_q2': 6.9012, 'l2_error_q3': 10.3003, 'r2_score_q1': 0.0553, 'r2_score_q2': 0.0963, 'r2_score_q3': 0.1471, 'mape_mean': 63.4866, 'mape_std': 19.1061, 'rmse_mean': 2.6583, 'rmse_std': 0.6224}
2025-11-14 17:41:57,042 - INFO - Learning rate for epoch 39: 1.0000000000000004e-08
2025-11-14 17:41:57,042 - INFO - No improvement in spearman genewise. Patience: 25/30
2025-11-14 17:41:57,966 - INFO - Fold 1 Train Epoch 40/200, Batch 0, Loss: 7.2526, Pearson: 0.6418, Spearman: 0.6084
2025-11-14 17:42:08,162 - INFO - Fold 1 Train Epoch 40/200, Batch 10, Loss: 7.0923, Pearson: 0.6430, Spearman: 0.6064
2025-11-14 17:42:18,379 - INFO - Fold 1 Train Epoch 40/200, Batch 20, Loss: 7.3927, Pearson: 0.6435, Spearman: 0.6109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6501
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6431
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.227715015411377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 40 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        7.4010243 0.        8.093866  0.
 7.4010243 7.4010243 7.4010243]
Sample y_pred values (first sample, first 10 genes):
[1.0027989  2.1431904  4.2293177  0.61407053 5.2994165  4.710694
 1.4032708  2.313632   4.3261805  5.7791758 ]
y_true  -> mean=2.1167, std=3.5114, min=0.0000, max=12.6166
y_pred  -> mean=2.1087, std=2.2206, min=0.0000, max=13.6738
Batch 0 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6468
2025-11-14 17:42:28,552 - INFO - Fold 1 Train Epoch 40/200, Batch 30, Loss: 7.2079, Pearson: 0.6491, Spearman: 0.6105
2025-11-14 17:42:38,736 - INFO - Fold 1 Train Epoch 40/200, Batch 40, Loss: 7.1378, Pearson: 0.6493, Spearman: 0.6123
2025-11-14 17:42:48,929 - INFO - Fold 1 Train Epoch 40/200, Batch 50, Loss: 7.2948, Pearson: 0.6455, Spearman: 0.6009
2025-11-14 17:42:59,124 - INFO - Fold 1 Train Epoch 40/200, Batch 60, Loss: 7.0806, Pearson: 0.6467, Spearman: 0.6102
2025-11-14 17:43:09,302 - INFO - Fold 1 Train Epoch 40/200, Batch 70, Loss: 7.3322, Pearson: 0.6376, Spearman: 0.6036
2025-11-14 17:43:19,532 - INFO - Fold 1 Train Epoch 40/200, Batch 80, Loss: 7.4000, Pearson: 0.6442, Spearman: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6505
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6360
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
2025-11-14 17:43:23,620 - INFO - Fold 1 Train Epoch 40/200, Train Loss: 7.2262, Pearson Mean: 0.6435, Spearman Mean: 0.6075
2025-11-14 17:43:23,620 - INFO - Training Metrics: {'pearson_mean_genewise': 0.448, 'spearman_mean_genewise': 0.4038, 'l1_error_mean': 1.8922, 'l2_errors_mean': 7.2283, 'r2_scores_mean': 0.214, 'pearson_std': 0.1166, 'l2_error_q1': 4.9201, 'l2_error_q2': 6.9554, 'l2_error_q3': 9.2194, 'r2_score_q1': 0.1329, 'r2_score_q2': 0.185, 'r2_score_q3': 0.2626, 'mape_mean': 58.1699, 'mape_std': 18.4362, 'rmse_mean': 2.6441, 'rmse_std': 0.487}
2025-11-14 17:43:23,979 - INFO - Fold 1 Val Epoch 40/200, Batch 0, Loss: 8.7370, Pearson: 0.5633, Spearman: 0.5490
2025-11-14 17:43:25,979 - INFO - Fold 1 Val Epoch 40/200, Batch 10, Loss: 7.4287, Pearson: 0.6118, Spearman: 0.6204
2025-11-14 17:43:27,205 - INFO - Fold 1 Val Epoch 40/200, Batch 20, Loss: 5.5033, Pearson: 0.5301, Spearman: 0.5365
2025-11-14 17:43:29,690 - INFO - Fold 1 Val Epoch 40/200, Val Loss: 7.4271, Pearson Mean: 0.5708, Spearman Mean: 0.5692
2025-11-14 17:43:29,691 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3265, 'spearman_mean_genewise': 0.2955, 'l1_error_mean': 1.9126, 'l2_errors_mean': 7.4504, 'r2_scores_mean': 0.112, 'pearson_std': 0.1115, 'l2_error_q1': 4.5203, 'l2_error_q2': 6.8772, 'l2_error_q3': 10.2614, 'r2_score_q1': 0.054, 'r2_score_q2': 0.0975, 'r2_score_q3': 0.1472, 'mape_mean': 64.1747, 'mape_std': 19.2335, 'rmse_mean': 2.6575, 'rmse_std': 0.6228}
2025-11-14 17:43:29,691 - INFO - Learning rate for epoch 40: 1.0000000000000004e-08
2025-11-14 17:43:29,691 - INFO - No improvement in spearman genewise. Patience: 26/30
2025-11-14 17:43:30,717 - INFO - Fold 1 Train Epoch 41/200, Batch 0, Loss: 7.2443, Pearson: 0.6316, Spearman: 0.6063
2025-11-14 17:43:40,826 - INFO - Fold 1 Train Epoch 41/200, Batch 10, Loss: 7.3337, Pearson: 0.6429, Spearman: 0.6116
2025-11-14 17:43:51,014 - INFO - Fold 1 Train Epoch 41/200, Batch 20, Loss: 7.3676, Pearson: 0.6381, Spearman: 0.6047
2025-11-14 17:44:01,198 - INFO - Fold 1 Train Epoch 41/200, Batch 30, Loss: 7.1767, Pearson: 0.6372, Spearman: 0.6059
2025-11-14 17:44:11,397 - INFO - Fold 1 Train Epoch 41/200, Batch 40, Loss: 7.5821, Pearson: 0.6426, Spearman: 0.6095
2025-11-14 17:44:21,607 - INFO - Fold 1 Train Epoch 41/200, Batch 50, Loss: 7.2808, Pearson: 0.6454, Spearman: 0.6130
========================= 7.228252410888672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 41 =====================
Sample y_true values (first sample, first 10 genes):
[0.       6.99532  6.303088 0.       0.       0.       6.99532  0.
 0.       8.786315]
Sample y_pred values (first sample, first 10 genes):
[0.70715845 3.5424156  3.2523675  1.689234   4.7896266  3.3617535
 0.74175787 2.8804727  3.7078478  9.232478  ]
y_true  -> mean=1.9899, std=3.4683, min=0.0000, max=12.7969
y_pred  -> mean=2.1073, std=2.1876, min=0.0000, max=13.7477
Batch 0 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 17:44:31,816 - INFO - Fold 1 Train Epoch 41/200, Batch 60, Loss: 7.4058, Pearson: 0.6399, Spearman: 0.6095
2025-11-14 17:44:41,995 - INFO - Fold 1 Train Epoch 41/200, Batch 70, Loss: 7.3399, Pearson: 0.6349, Spearman: 0.5990
2025-11-14 17:44:52,231 - INFO - Fold 1 Train Epoch 41/200, Batch 80, Loss: 6.9985, Pearson: 0.6506, Spearman: 0.6085
2025-11-14 17:44:56,467 - INFO - Fold 1 Train Epoch 41/200, Train Loss: 7.2301, Pearson Mean: 0.6434, Spearman Mean: 0.6074
2025-11-14 17:44:56,468 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4479, 'spearman_mean_genewise': 0.4036, 'l1_error_mean': 1.8926, 'l2_errors_mean': 7.2292, 'r2_scores_mean': 0.2139, 'pearson_std': 0.1165, 'l2_error_q1': 4.9265, 'l2_error_q2': 6.9497, 'l2_error_q3': 9.2052, 'r2_score_q1': 0.1326, 'r2_score_q2': 0.1847, 'r2_score_q3': 0.2642, 'mape_mean': 58.1855, 'mape_std': 18.4484, 'rmse_mean': 2.6443, 'rmse_std': 0.4869}
2025-11-14 17:44:56,796 - INFO - Fold 1 Val Epoch 41/200, Batch 0, Loss: 8.7467, Pearson: 0.5626, Spearman: 0.5483
2025-11-14 17:44:58,593 - INFO - Fold 1 Val Epoch 41/200, Batch 10, Loss: 7.3964, Pearson: 0.6132, Spearman: 0.6217
2025-11-14 17:44:59,847 - INFO - Fold 1 Val Epoch 41/200, Batch 20, Loss: 5.5119, Pearson: 0.5301, Spearman: 0.5365
2025-11-14 17:45:02,378 - INFO - Fold 1 Val Epoch 41/200, Val Loss: 7.4323, Pearson Mean: 0.5706, Spearman Mean: 0.5687
2025-11-14 17:45:02,378 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3266, 'spearman_mean_genewise': 0.2959, 'l1_error_mean': 1.9188, 'l2_errors_mean': 7.454, 'r2_scores_mean': 0.1116, 'pearson_std': 0.1114, 'l2_error_q1': 4.521, 'l2_error_q2': 6.8685, 'l2_error_q3': 10.2625, 'r2_score_q1': 0.0547, 'r2_score_q2': 0.0965, 'r2_score_q3': 0.1469, 'mape_mean': 63.8536, 'mape_std': 19.239, 'rmse_mean': 2.6582, 'rmse_std': 0.6227}
2025-11-14 17:45:02,378 - INFO - Learning rate for epoch 41: 1.0000000000000004e-08
2025-11-14 17:45:02,378 - INFO - No improvement in spearman genewise. Patience: 27/30
2025-11-14 17:45:03,252 - INFO - Fold 1 Train Epoch 42/200, Batch 0, Loss: 7.2085, Pearson: 0.6391, Spearman: 0.6078
2025-11-14 17:45:13,238 - INFO - Fold 1 Train Epoch 42/200, Batch 10, Loss: 7.2654, Pearson: 0.6418, Spearman: 0.6109
2025-11-14 17:45:23,460 - INFO - Fold 1 Train Epoch 42/200, Batch 20, Loss: 7.3670, Pearson: 0.6418, Spearman: 0.6029
Batch 54 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6513
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6383
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.22914981842041
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 42 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.155716 0.       0.       7.155716 0.       0.
 7.155716 7.155716]
Sample y_pred values (first sample, first 10 genes):
[1.2312664 0.        1.7644027 1.2022789 3.6870513 1.9970908 1.2128848
 3.348709  4.074168  7.4749174]
y_true  -> mean=2.0662, std=3.4904, min=0.0000, max=12.4524
y_pred  -> mean=2.1069, std=2.2154, min=0.0000, max=12.9782
Batch 0 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6423
2025-11-14 17:45:33,643 - INFO - Fold 1 Train Epoch 42/200, Batch 30, Loss: 7.4140, Pearson: 0.6437, Spearman: 0.6050
2025-11-14 17:45:43,843 - INFO - Fold 1 Train Epoch 42/200, Batch 40, Loss: 7.1920, Pearson: 0.6409, Spearman: 0.6095
2025-11-14 17:45:54,036 - INFO - Fold 1 Train Epoch 42/200, Batch 50, Loss: 7.3062, Pearson: 0.6398, Spearman: 0.6038
2025-11-14 17:46:04,257 - INFO - Fold 1 Train Epoch 42/200, Batch 60, Loss: 7.2057, Pearson: 0.6491, Spearman: 0.6051
2025-11-14 17:46:14,447 - INFO - Fold 1 Train Epoch 42/200, Batch 70, Loss: 7.2257, Pearson: 0.6384, Spearman: 0.6060
2025-11-14 17:46:24,647 - INFO - Fold 1 Train Epoch 42/200, Batch 80, Loss: 7.3876, Pearson: 0.6412, Spearman: 0.6047
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6464
2025-11-14 17:46:28,901 - INFO - Fold 1 Train Epoch 42/200, Train Loss: 7.2275, Pearson Mean: 0.6434, Spearman Mean: 0.6076
2025-11-14 17:46:28,901 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4479, 'spearman_mean_genewise': 0.4037, 'l1_error_mean': 1.892, 'l2_errors_mean': 7.2278, 'r2_scores_mean': 0.2139, 'pearson_std': 0.1167, 'l2_error_q1': 4.9309, 'l2_error_q2': 6.9515, 'l2_error_q3': 9.2239, 'r2_score_q1': 0.1321, 'r2_score_q2': 0.1857, 'r2_score_q3': 0.2636, 'mape_mean': 58.1707, 'mape_std': 18.4535, 'rmse_mean': 2.644, 'rmse_std': 0.4869}
2025-11-14 17:46:29,244 - INFO - Fold 1 Val Epoch 42/200, Batch 0, Loss: 8.7518, Pearson: 0.5630, Spearman: 0.5485
2025-11-14 17:46:30,958 - INFO - Fold 1 Val Epoch 42/200, Batch 10, Loss: 7.4117, Pearson: 0.6126, Spearman: 0.6215
2025-11-14 17:46:32,219 - INFO - Fold 1 Val Epoch 42/200, Batch 20, Loss: 5.5307, Pearson: 0.5290, Spearman: 0.5356
2025-11-14 17:46:34,716 - INFO - Fold 1 Val Epoch 42/200, Val Loss: 7.4353, Pearson Mean: 0.5706, Spearman Mean: 0.5691
2025-11-14 17:46:34,716 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3265, 'spearman_mean_genewise': 0.2961, 'l1_error_mean': 1.9256, 'l2_errors_mean': 7.4573, 'r2_scores_mean': 0.1112, 'pearson_std': 0.1112, 'l2_error_q1': 4.5209, 'l2_error_q2': 6.9064, 'l2_error_q3': 10.2981, 'r2_score_q1': 0.0549, 'r2_score_q2': 0.0966, 'r2_score_q3': 0.1474, 'mape_mean': 63.4215, 'mape_std': 19.2173, 'rmse_mean': 2.6588, 'rmse_std': 0.6231}
2025-11-14 17:46:34,716 - INFO - Learning rate for epoch 42: 1.0000000000000004e-08
2025-11-14 17:46:34,716 - INFO - No improvement in spearman genewise. Patience: 28/30
2025-11-14 17:46:35,668 - INFO - Fold 1 Train Epoch 43/200, Batch 0, Loss: 7.0528, Pearson: 0.6423, Spearman: 0.6087
2025-11-14 17:46:45,890 - INFO - Fold 1 Train Epoch 43/200, Batch 10, Loss: 7.2795, Pearson: 0.6380, Spearman: 0.6068
2025-11-14 17:46:56,067 - INFO - Fold 1 Train Epoch 43/200, Batch 20, Loss: 7.3291, Pearson: 0.6425, Spearman: 0.6103
2025-11-14 17:47:06,236 - INFO - Fold 1 Train Epoch 43/200, Batch 30, Loss: 7.0911, Pearson: 0.6560, Spearman: 0.6102
2025-11-14 17:47:16,430 - INFO - Fold 1 Train Epoch 43/200, Batch 40, Loss: 7.5404, Pearson: 0.6498, Spearman: 0.6135
2025-11-14 17:47:26,644 - INFO - Fold 1 Train Epoch 43/200, Batch 50, Loss: 7.3791, Pearson: 0.6417, Spearman: 0.6072
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6501
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6276
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.227835178375244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 43 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.8536057 0.        7.8536057 7.1608467 0.
 0.        8.258942  9.1060915]
Sample y_pred values (first sample, first 10 genes):
[1.0970346  1.5093774  2.7173166  0.7143905  5.9963264  3.4430594
 0.80929786 1.7202522  3.612897   9.965769  ]
y_true  -> mean=2.0019, std=3.4618, min=0.0000, max=12.9120
y_pred  -> mean=2.1143, std=2.2362, min=0.0000, max=13.8241
Batch 0 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6417
2025-11-14 17:47:36,795 - INFO - Fold 1 Train Epoch 43/200, Batch 60, Loss: 7.1862, Pearson: 0.6454, Spearman: 0.6071
2025-11-14 17:47:46,989 - INFO - Fold 1 Train Epoch 43/200, Batch 70, Loss: 7.1387, Pearson: 0.6404, Spearman: 0.5984
2025-11-14 17:47:57,187 - INFO - Fold 1 Train Epoch 43/200, Batch 80, Loss: 7.1092, Pearson: 0.6364, Spearman: 0.6003
2025-11-14 17:48:01,152 - INFO - Fold 1 Train Epoch 43/200, Train Loss: 7.2318, Pearson Mean: 0.6433, Spearman Mean: 0.6074
2025-11-14 17:48:01,152 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4477, 'spearman_mean_genewise': 0.4035, 'l1_error_mean': 1.8928, 'l2_errors_mean': 7.2305, 'r2_scores_mean': 0.2137, 'pearson_std': 0.1165, 'l2_error_q1': 4.9325, 'l2_error_q2': 6.9493, 'l2_error_q3': 9.2288, 'r2_score_q1': 0.1321, 'r2_score_q2': 0.1853, 'r2_score_q3': 0.2636, 'mape_mean': 58.1917, 'mape_std': 18.4468, 'rmse_mean': 2.6445, 'rmse_std': 0.487}
2025-11-14 17:48:01,501 - INFO - Fold 1 Val Epoch 43/200, Batch 0, Loss: 8.7394, Pearson: 0.5648, Spearman: 0.5504
2025-11-14 17:48:03,000 - INFO - Fold 1 Val Epoch 43/200, Batch 10, Loss: 7.3842, Pearson: 0.6122, Spearman: 0.6213
2025-11-14 17:48:04,291 - INFO - Fold 1 Val Epoch 43/200, Batch 20, Loss: 5.5496, Pearson: 0.5281, Spearman: 0.5351
2025-11-14 17:48:06,860 - INFO - Fold 1 Val Epoch 43/200, Val Loss: 7.4525, Pearson Mean: 0.5696, Spearman Mean: 0.5681
2025-11-14 17:48:06,860 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3265, 'spearman_mean_genewise': 0.2959, 'l1_error_mean': 1.9459, 'l2_errors_mean': 7.4737, 'r2_scores_mean': 0.1095, 'pearson_std': 0.1111, 'l2_error_q1': 4.5152, 'l2_error_q2': 6.9247, 'l2_error_q3': 10.3326, 'r2_score_q1': 0.0541, 'r2_score_q2': 0.0935, 'r2_score_q3': 0.1446, 'mape_mean': 62.5807, 'mape_std': 19.1848, 'rmse_mean': 2.6615, 'rmse_std': 0.6244}
2025-11-14 17:48:06,861 - INFO - Learning rate for epoch 43: 1.0000000000000004e-08
2025-11-14 17:48:06,861 - INFO - No improvement in spearman genewise. Patience: 29/30
2025-11-14 17:48:07,739 - INFO - Fold 1 Train Epoch 44/200, Batch 0, Loss: 7.3984, Pearson: 0.6445, Spearman: 0.6092
2025-11-14 17:48:17,722 - INFO - Fold 1 Train Epoch 44/200, Batch 10, Loss: 7.2571, Pearson: 0.6431, Spearman: 0.6065
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6433
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6235
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.23054313659668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 44 =====================
Sample y_true values (first sample, first 10 genes):
[5.8288956 6.925545  5.8288956 0.        6.520571  7.6182013 0.
 5.8288956 5.8288956 9.610207 ]
Sample y_pred values (first sample, first 10 genes):
[ 2.1938043  6.1692843  4.861353   3.441082   7.4628105  5.8903513
  2.242533   5.045829   5.649486  10.106091 ]
y_true  -> mean=2.2438, std=3.5528, min=0.0000, max=12.4923
y_pred  -> mean=2.1196, std=2.2295, min=0.0000, max=13.5914
Batch 0 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6393
2025-11-14 17:48:27,895 - INFO - Fold 1 Train Epoch 44/200, Batch 20, Loss: 7.4424, Pearson: 0.6460, Spearman: 0.6082
2025-11-14 17:48:38,117 - INFO - Fold 1 Train Epoch 44/200, Batch 30, Loss: 7.1000, Pearson: 0.6528, Spearman: 0.6087
2025-11-14 17:48:48,343 - INFO - Fold 1 Train Epoch 44/200, Batch 40, Loss: 7.0572, Pearson: 0.6497, Spearman: 0.6130
2025-11-14 17:48:58,543 - INFO - Fold 1 Train Epoch 44/200, Batch 50, Loss: 7.1666, Pearson: 0.6317, Spearman: 0.6015
2025-11-14 17:49:08,738 - INFO - Fold 1 Train Epoch 44/200, Batch 60, Loss: 7.2434, Pearson: 0.6387, Spearman: 0.6055
2025-11-14 17:49:18,898 - INFO - Fold 1 Train Epoch 44/200, Batch 70, Loss: 7.4413, Pearson: 0.6413, Spearman: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6467
2025-11-14 17:49:29,113 - INFO - Fold 1 Train Epoch 44/200, Batch 80, Loss: 7.1197, Pearson: 0.6495, Spearman: 0.6078
2025-11-14 17:49:32,902 - INFO - Fold 1 Train Epoch 44/200, Train Loss: 7.2278, Pearson Mean: 0.6436, Spearman Mean: 0.6076
2025-11-14 17:49:32,902 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4483, 'spearman_mean_genewise': 0.404, 'l1_error_mean': 1.8914, 'l2_errors_mean': 7.2249, 'r2_scores_mean': 0.2142, 'pearson_std': 0.1167, 'l2_error_q1': 4.9145, 'l2_error_q2': 6.9557, 'l2_error_q3': 9.2057, 'r2_score_q1': 0.1326, 'r2_score_q2': 0.1855, 'r2_score_q3': 0.2645, 'mape_mean': 58.1529, 'mape_std': 18.4442, 'rmse_mean': 2.6435, 'rmse_std': 0.4867}
2025-11-14 17:49:33,256 - INFO - Fold 1 Val Epoch 44/200, Batch 0, Loss: 8.7537, Pearson: 0.5621, Spearman: 0.5474
2025-11-14 17:49:34,867 - INFO - Fold 1 Val Epoch 44/200, Batch 10, Loss: 7.4332, Pearson: 0.6123, Spearman: 0.6211
2025-11-14 17:49:36,109 - INFO - Fold 1 Val Epoch 44/200, Batch 20, Loss: 5.5033, Pearson: 0.5301, Spearman: 0.5366
2025-11-14 17:49:38,609 - INFO - Fold 1 Val Epoch 44/200, Val Loss: 7.4322, Pearson Mean: 0.5704, Spearman Mean: 0.5688
2025-11-14 17:49:38,609 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3264, 'spearman_mean_genewise': 0.2956, 'l1_error_mean': 1.9162, 'l2_errors_mean': 7.4552, 'r2_scores_mean': 0.1115, 'pearson_std': 0.1112, 'l2_error_q1': 4.522, 'l2_error_q2': 6.8989, 'l2_error_q3': 10.2717, 'r2_score_q1': 0.0542, 'r2_score_q2': 0.0969, 'r2_score_q3': 0.1476, 'mape_mean': 63.9817, 'mape_std': 19.1198, 'rmse_mean': 2.6584, 'rmse_std': 0.6229}
2025-11-14 17:49:38,610 - INFO - Learning rate for epoch 44: 1.0000000000000004e-08
2025-11-14 17:49:38,610 - INFO - No improvement in spearman genewise. Patience: 30/30
2025-11-14 17:49:38,610 - INFO - Early stopping triggered. Breaking training loop.
2025-11-14 17:49:38,610 - INFO - ===== Completed Fold 1/5 =====
2025-11-14 17:49:38,611 - INFO - 
===== Starting Fold 2/5 =====
2025-11-14 17:49:38,611 - INFO - Fold 2: Train=29, Val=7
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6388
image : torch.Size([34, 3, 224, 224]), y_true: torch.Size([34, 785]), y_pred: torch.Size([34, 785])
Batch 84 Pearson correlation: 0.6339
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.224948406219482
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A2.h5ad
 Loaded images: (325, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (325, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A3.h5ad
 Loaded images: (359, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (359, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A4.h5ad
 Loaded images: (343, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (343, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A6.h5ad
 Loaded images: (360, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (360, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B1.h5ad
 Loaded images: (295, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (295, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B2.h5ad
 Loaded images: (270, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (270, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B5.h5ad
 Loaded images: (289, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (289, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B6.h5ad
 Loaded images: (277, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (277, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C1.h5ad
 Loaded images: (176, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (176, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C2.h5ad
 Loaded images: (187, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (187, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C3.h5ad
 Loaded images: (180, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (180, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C4.h5ad
 Loaded images: (184, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (184, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C5.h5ad
 Loaded images: (181, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (181, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D1.h5ad
 Loaded images: (306, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (306, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D2.h5ad
 Loaded images: (303, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (303, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D3.h5ad
 Loaded images: (301, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (301, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D4.h5ad
 Loaded images: (302, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (302, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D5.h5ad
 Loaded images: (306, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (306, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D6.h5ad
 Loaded images: (315, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
2025-11-14 17:49:56,172 - INFO - Fold 2: Train=10868, Val=2752
2025-11-14 17:49:56,173 - INFO - train_datasets length:  10868
2025-11-14 17:49:56,173 - INFO - Number of train batches: 85
2025-11-14 17:49:56,173 - INFO - Initializing model...
2025-11-14 17:49:56,271 - INFO - Model
STNet(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (denseblock1): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition1): _Transition(
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock2): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition2): _Transition(
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock3): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer17): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer18): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer19): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer20): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer21): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer22): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer23): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer24): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition3): _Transition(
      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock4): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=1024, out_features=785, bias=True)
)
2025-11-14 17:49:56,275 - INFO - Using device: cuda
2025-11-14 17:49:57,242 - INFO - Fold 2 Train Epoch 1/200, Batch 0, Loss: 16.7824, Pearson: -0.0136, Spearman: -0.0071
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (315, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E1.h5ad
 Loaded images: (587, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (587, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E2.h5ad
 Loaded images: (572, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (572, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E3.h5ad
 Loaded images: (570, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (570, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F1.h5ad
 Loaded images: (691, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (691, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F2.h5ad
 Loaded images: (695, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (695, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G1.h5ad
 Loaded images: (441, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (441, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G2.h5ad
 Loaded images: (467, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (467, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G3.h5ad
 Loaded images: (463, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (463, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H1.h5ad
 Loaded images: (613, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (613, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H3.h5ad
 Loaded images: (510, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (510, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A1.h5ad
 Loaded images: (346, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (346, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A5.h5ad
 Loaded images: (332, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (332, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B3.h5ad
 Loaded images: (298, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (298, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B4.h5ad
 Loaded images: (283, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (283, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C6.h5ad
 Loaded images: (178, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (178, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F3.h5ad
 Loaded images: (712, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (712, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H2.h5ad
 Loaded images: (603, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (603, 785)
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 1 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       7.91461  0.       7.221828
 0.       8.319953]
Sample y_pred values (first sample, first 10 genes):
[0.         0.13746297 1.3037622  0.         0.1036628  0.24305227
 0.         0.08173478 0.73370004 0.        ]
y_true  -> mean=2.2210, std=3.5305, min=0.0000, max=12.8087
y_pred  -> mean=0.1629, std=0.2423, min=0.0000, max=1.9478
Batch 0 Pearson correlation: -0.0136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.0049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.0165
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.0305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.0434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.0571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.0728
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.0822
2025-11-14 17:50:07,210 - INFO - Fold 2 Train Epoch 1/200, Batch 10, Loss: 15.8544, Pearson: 0.1298, Spearman: 0.0713
2025-11-14 17:50:17,347 - INFO - Fold 2 Train Epoch 1/200, Batch 20, Loss: 13.1697, Pearson: 0.2132, Spearman: 0.1137
2025-11-14 17:50:27,509 - INFO - Fold 2 Train Epoch 1/200, Batch 30, Loss: 13.2968, Pearson: 0.3008, Spearman: 0.2190
2025-11-14 17:50:37,636 - INFO - Fold 2 Train Epoch 1/200, Batch 40, Loss: 11.9508, Pearson: 0.3630, Spearman: 0.3227
2025-11-14 17:50:47,771 - INFO - Fold 2 Train Epoch 1/200, Batch 50, Loss: 10.8287, Pearson: 0.3837, Spearman: 0.3850
2025-11-14 17:50:55,686 - INFO - Fold 2 Train Epoch 1/200, Batch 60, Loss: 10.0188, Pearson: 0.4549, Spearman: 0.4391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.0968
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.1073
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.1298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.1282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.1394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.1640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.1735
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.1821
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.2000
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.1836
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.2231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.1935
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.2132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.2331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.2419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.2470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.2642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.2349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.2463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.2864
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.2755
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.2865
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.3008
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.2936
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.2792
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.3085
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.3122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.2953
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.3017
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.3375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.3081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.3306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.3630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.3175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.3708
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.3452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.3797
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.3573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.3697
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.3689
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.3767
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.3901
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.3837
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.3646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.4027
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.4001
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.4004
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.4453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.4026
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.4203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.4327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.4379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.4549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.4739
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.4596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.4855
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.4514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.4700
2025-11-14 17:51:03,870 - INFO - Fold 2 Train Epoch 1/200, Batch 70, Loss: 9.8501, Pearson: 0.4735, Spearman: 0.4666
2025-11-14 17:51:14,024 - INFO - Fold 2 Train Epoch 1/200, Batch 80, Loss: 9.1232, Pearson: 0.5102, Spearman: 0.4930
2025-11-14 17:51:19,301 - INFO - Fold 2 Train Epoch 1/200, Train Loss: 12.0822, Pearson Mean: 0.3237, Spearman Mean: 0.2902
2025-11-14 17:51:19,301 - INFO - Training Metrics: {'pearson_mean_genewise': 0.1764, 'spearman_mean_genewise': 0.1589, 'l1_error_mean': 2.2661, 'l2_errors_mean': 12.0858, 'r2_scores_mean': -0.2149, 'pearson_std': 0.0706, 'l2_error_q1': 5.7197, 'l2_error_q2': 8.8074, 'l2_error_q3': 14.7872, 'r2_score_q1': -0.1212, 'r2_score_q2': -0.0229, 'r2_score_q3': 0.0094, 'mape_mean': 80.4741, 'mape_std': 6.5682, 'rmse_mean': 3.2819, 'rmse_std': 1.1467}
2025-11-14 17:51:19,584 - INFO - Fold 2 Val Epoch 1/200, Batch 0, Loss: 10.1790, Pearson: 0.4457, Spearman: 0.4525
2025-11-14 17:51:21,352 - INFO - Fold 2 Val Epoch 1/200, Batch 10, Loss: 11.7863, Pearson: 0.5085, Spearman: 0.5064
2025-11-14 17:51:23,109 - INFO - Fold 2 Val Epoch 1/200, Batch 20, Loss: 8.4521, Pearson: 0.3109, Spearman: 0.3394
2025-11-14 17:51:25,552 - INFO - Fold 2 Val Epoch 1/200, Val Loss: 9.8002, Pearson Mean: 0.4318, Spearman Mean: 0.4213
2025-11-14 17:51:25,553 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2552, 'spearman_mean_genewise': 0.2349, 'l1_error_mean': 2.128, 'l2_errors_mean': 9.8033, 'r2_scores_mean': -0.0094, 'pearson_std': 0.1113, 'l2_error_q1': 5.3707, 'l2_error_q2': 8.3065, 'l2_error_q3': 12.8226, 'r2_score_q1': -0.0048, 'r2_score_q2': 0.0248, 'r2_score_q3': 0.057, 'mape_mean': 73.5559, 'mape_std': 12.4676, 'rmse_mean': 3.0087, 'rmse_std': 0.8667}
2025-11-14 17:51:25,553 - INFO - Learning rate for epoch 1: 0.0001
2025-11-14 17:51:25,597 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 17:51:26,547 - INFO - Fold 2 Train Epoch 2/200, Batch 0, Loss: 9.3929, Pearson: 0.4894, Spearman: 0.4875
2025-11-14 17:51:36,730 - INFO - Fold 2 Train Epoch 2/200, Batch 10, Loss: 8.9808, Pearson: 0.5326, Spearman: 0.5087
2025-11-14 17:51:46,927 - INFO - Fold 2 Train Epoch 2/200, Batch 20, Loss: 8.7095, Pearson: 0.5421, Spearman: 0.5174
2025-11-14 17:51:57,110 - INFO - Fold 2 Train Epoch 2/200, Batch 30, Loss: 8.9379, Pearson: 0.5334, Spearman: 0.5210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.4636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.4662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.4826
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.4659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.4735
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.4772
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.4902
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.4740
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.4749
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.4750
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.4870
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.4914
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.4903
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.4883
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.4913
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.4810
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.4992
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.4926
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 12.085782051086426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 2 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       6.658281 0.       6.658281
 0.       7.350786]
Sample y_pred values (first sample, first 10 genes):
[0.16077441 0.80945617 1.208478   0.89315724 3.5754967  1.2802185
 0.3728485  1.3878751  3.064322   4.753956  ]
y_true  -> mean=2.1059, std=3.4917, min=0.0000, max=12.7741
y_pred  -> mean=1.7942, std=1.5552, min=0.0000, max=8.4953
Batch 0 Pearson correlation: 0.4894
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5002
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5093
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.4888
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5130
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.4998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5030
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5121
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5068
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5356
2025-11-14 17:52:07,287 - INFO - Fold 2 Train Epoch 2/200, Batch 40, Loss: 9.1610, Pearson: 0.5443, Spearman: 0.5363
2025-11-14 17:52:17,446 - INFO - Fold 2 Train Epoch 2/200, Batch 50, Loss: 8.9267, Pearson: 0.5396, Spearman: 0.5232
2025-11-14 17:52:25,449 - INFO - Fold 2 Train Epoch 2/200, Batch 60, Loss: 8.9154, Pearson: 0.5632, Spearman: 0.5346
2025-11-14 17:52:33,757 - INFO - Fold 2 Train Epoch 2/200, Batch 70, Loss: 8.6076, Pearson: 0.5498, Spearman: 0.5264
2025-11-14 17:52:43,945 - INFO - Fold 2 Train Epoch 2/200, Batch 80, Loss: 8.4542, Pearson: 0.5602, Spearman: 0.5323
2025-11-14 17:52:49,196 - INFO - Fold 2 Train Epoch 2/200, Train Loss: 8.7663, Pearson Mean: 0.5364, Spearman Mean: 0.5172
2025-11-14 17:52:49,196 - INFO - Training Metrics: {'pearson_mean_genewise': 0.2959, 'spearman_mean_genewise': 0.2739, 'l1_error_mean': 2.1848, 'l2_errors_mean': 8.7665, 'r2_scores_mean': 0.0605, 'pearson_std': 0.0949, 'l2_error_q1': 5.342, 'l2_error_q2': 7.8534, 'l2_error_q3': 11.754, 'r2_score_q1': 0.0427, 'r2_score_q2': 0.0731, 'r2_score_q3': 0.1121, 'mape_mean': 67.241, 'mape_std': 15.5122, 'rmse_mean': 2.8868, 'rmse_std': 0.6581}
2025-11-14 17:52:49,556 - INFO - Fold 2 Val Epoch 2/200, Batch 0, Loss: 10.0550, Pearson: 0.4726, Spearman: 0.4797
2025-11-14 17:52:51,370 - INFO - Fold 2 Val Epoch 2/200, Batch 10, Loss: 10.4017, Pearson: 0.5323, Spearman: 0.5302
2025-11-14 17:52:53,119 - INFO - Fold 2 Val Epoch 2/200, Batch 20, Loss: 7.2380, Pearson: 0.4381, Spearman: 0.4515
2025-11-14 17:52:55,575 - INFO - Fold 2 Val Epoch 2/200, Val Loss: 8.9130, Pearson Mean: 0.4897, Spearman Mean: 0.4822
2025-11-14 17:52:55,575 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2753, 'spearman_mean_genewise': 0.2587, 'l1_error_mean': 2.145, 'l2_errors_mean': 8.9235, 'r2_scores_mean': 0.0544, 'pearson_std': 0.1055, 'l2_error_q1': 5.2711, 'l2_error_q2': 8.2538, 'l2_error_q3': 12.3071, 'r2_score_q1': 0.0239, 'r2_score_q2': 0.0496, 'r2_score_q3': 0.0838, 'mape_mean': 69.0495, 'mape_std': 16.1595, 'rmse_mean': 2.9026, 'rmse_std': 0.706}
2025-11-14 17:52:55,576 - INFO - Learning rate for epoch 2: 0.0001
2025-11-14 17:52:55,637 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 17:52:56,616 - INFO - Fold 2 Train Epoch 3/200, Batch 0, Loss: 8.1295, Pearson: 0.5513, Spearman: 0.5381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5688
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5482
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.5314
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.766496658325195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 3 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.085719]
Sample y_pred values (first sample, first 10 genes):
[0.12568219 0.65727717 1.0412614  0.4856356  2.2558978  1.0868076
 0.1641758  0.81942993 2.1917653  4.9753428 ]
y_true  -> mean=1.9531, std=3.4166, min=0.0000, max=12.7220
y_pred  -> mean=1.9561, std=1.8247, min=0.0000, max=12.4686
Batch 0 Pearson correlation: 0.5513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5812
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5665
2025-11-14 17:53:06,793 - INFO - Fold 2 Train Epoch 3/200, Batch 10, Loss: 8.3617, Pearson: 0.5754, Spearman: 0.5359
2025-11-14 17:53:16,992 - INFO - Fold 2 Train Epoch 3/200, Batch 20, Loss: 8.1247, Pearson: 0.5704, Spearman: 0.5533
2025-11-14 17:53:27,191 - INFO - Fold 2 Train Epoch 3/200, Batch 30, Loss: 8.8486, Pearson: 0.5598, Spearman: 0.5389
2025-11-14 17:53:37,368 - INFO - Fold 2 Train Epoch 3/200, Batch 40, Loss: 8.1155, Pearson: 0.5611, Spearman: 0.5436
2025-11-14 17:53:47,566 - INFO - Fold 2 Train Epoch 3/200, Batch 50, Loss: 7.9045, Pearson: 0.5834, Spearman: 0.5536
2025-11-14 17:53:55,557 - INFO - Fold 2 Train Epoch 3/200, Batch 60, Loss: 8.2217, Pearson: 0.5717, Spearman: 0.5439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5681
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5744
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5754
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5663
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5767
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5695
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5704
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5773
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5695
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5788
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5744
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5718
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5748
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5690
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5809
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5720
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5739
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5741
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5731
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5732
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5784
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5834
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5713
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5807
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5717
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5767
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5725
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5717
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5830
2025-11-14 17:54:03,796 - INFO - Fold 2 Train Epoch 3/200, Batch 70, Loss: 8.1280, Pearson: 0.5820, Spearman: 0.5501
2025-11-14 17:54:13,969 - INFO - Fold 2 Train Epoch 3/200, Batch 80, Loss: 8.1464, Pearson: 0.5627, Spearman: 0.5466
2025-11-14 17:54:19,216 - INFO - Fold 2 Train Epoch 3/200, Train Loss: 8.2861, Pearson Mean: 0.5690, Spearman Mean: 0.5433
2025-11-14 17:54:19,216 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3297, 'spearman_mean_genewise': 0.3022, 'l1_error_mean': 2.1108, 'l2_errors_mean': 8.286, 'r2_scores_mean': 0.1036, 'pearson_std': 0.1002, 'l2_error_q1': 5.2764, 'l2_error_q2': 7.6785, 'l2_error_q3': 11.135, 'r2_score_q1': 0.0593, 'r2_score_q2': 0.0982, 'r2_score_q3': 0.1402, 'mape_mean': 64.8698, 'mape_std': 16.7833, 'rmse_mean': 2.8182, 'rmse_std': 0.5863}
2025-11-14 17:54:19,556 - INFO - Fold 2 Val Epoch 3/200, Batch 0, Loss: 10.2708, Pearson: 0.4779, Spearman: 0.4838
2025-11-14 17:54:21,370 - INFO - Fold 2 Val Epoch 3/200, Batch 10, Loss: 9.5416, Pearson: 0.5487, Spearman: 0.5490
2025-11-14 17:54:23,130 - INFO - Fold 2 Val Epoch 3/200, Batch 20, Loss: 6.8058, Pearson: 0.4918, Spearman: 0.4940
2025-11-14 17:54:25,530 - INFO - Fold 2 Val Epoch 3/200, Val Loss: 8.5867, Pearson Mean: 0.5178, Spearman Mean: 0.5087
2025-11-14 17:54:25,531 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2916, 'spearman_mean_genewise': 0.2711, 'l1_error_mean': 2.1877, 'l2_errors_mean': 8.6008, 'r2_scores_mean': 0.0781, 'pearson_std': 0.1108, 'l2_error_q1': 5.2489, 'l2_error_q2': 8.1418, 'l2_error_q3': 11.946, 'r2_score_q1': 0.0326, 'r2_score_q2': 0.0627, 'r2_score_q3': 0.1024, 'mape_mean': 66.1531, 'mape_std': 17.2274, 'rmse_mean': 2.859, 'rmse_std': 0.6534}
2025-11-14 17:54:25,531 - INFO - Learning rate for epoch 3: 0.0001
2025-11-14 17:54:25,598 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 17:54:26,556 - INFO - Fold 2 Train Epoch 4/200, Batch 0, Loss: 8.3203, Pearson: 0.5615, Spearman: 0.5369
2025-11-14 17:54:36,781 - INFO - Fold 2 Train Epoch 4/200, Batch 10, Loss: 7.8269, Pearson: 0.5978, Spearman: 0.5512
2025-11-14 17:54:46,967 - INFO - Fold 2 Train Epoch 4/200, Batch 20, Loss: 7.7899, Pearson: 0.5842, Spearman: 0.5492
2025-11-14 17:54:57,144 - INFO - Fold 2 Train Epoch 4/200, Batch 30, Loss: 8.1299, Pearson: 0.5652, Spearman: 0.5464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5772
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5702
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5770
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5837
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5764
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5967
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5693
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5820
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5761
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5771
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5738
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5736
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5765
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5776
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5770
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5726
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5862
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5759
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.5702
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.285968780517578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 4 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       7.461715
 0.       7.461715]
Sample y_pred values (first sample, first 10 genes):
[0.92083937 0.99131215 1.5290008  1.4994674  3.6362991  2.2238927
 0.6892397  2.38553    3.4696827  7.113696  ]
y_true  -> mean=2.0630, std=3.4845, min=0.0000, max=12.8135
y_pred  -> mean=1.9968, std=1.9071, min=0.0000, max=13.1709
Batch 0 Pearson correlation: 0.5615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5857
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5906
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5872
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5751
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5979
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5946
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5853
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5957
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5891
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5978
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5841
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5775
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5874
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5857
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5809
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5716
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5775
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5759
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5759
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5842
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5836
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5865
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5898
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5888
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5740
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5715
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5770
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5798
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5898
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5652
2025-11-14 17:55:07,336 - INFO - Fold 2 Train Epoch 4/200, Batch 40, Loss: 8.3136, Pearson: 0.5743, Spearman: 0.5543
2025-11-14 17:55:17,504 - INFO - Fold 2 Train Epoch 4/200, Batch 50, Loss: 7.9114, Pearson: 0.5814, Spearman: 0.5576
2025-11-14 17:55:25,540 - INFO - Fold 2 Train Epoch 4/200, Batch 60, Loss: 7.9238, Pearson: 0.5979, Spearman: 0.5722
2025-11-14 17:55:33,766 - INFO - Fold 2 Train Epoch 4/200, Batch 70, Loss: 8.0737, Pearson: 0.5786, Spearman: 0.5526
2025-11-14 17:55:43,942 - INFO - Fold 2 Train Epoch 4/200, Batch 80, Loss: 8.1408, Pearson: 0.6016, Spearman: 0.5685
2025-11-14 17:55:49,178 - INFO - Fold 2 Train Epoch 4/200, Train Loss: 8.0385, Pearson Mean: 0.5862, Spearman Mean: 0.5571
2025-11-14 17:55:49,179 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3505, 'spearman_mean_genewise': 0.3191, 'l1_error_mean': 2.0656, 'l2_errors_mean': 8.0386, 'r2_scores_mean': 0.1258, 'pearson_std': 0.1041, 'l2_error_q1': 5.2224, 'l2_error_q2': 7.5518, 'l2_error_q3': 10.6842, 'r2_score_q1': 0.0714, 'r2_score_q2': 0.1141, 'r2_score_q3': 0.1605, 'mape_mean': 63.4044, 'mape_std': 17.2681, 'rmse_mean': 2.78, 'rmse_std': 0.5571}
2025-11-14 17:55:49,477 - INFO - Fold 2 Val Epoch 4/200, Batch 0, Loss: 9.6174, Pearson: 0.4805, Spearman: 0.4861
2025-11-14 17:55:51,254 - INFO - Fold 2 Val Epoch 4/200, Batch 10, Loss: 9.2752, Pearson: 0.5591, Spearman: 0.5544
2025-11-14 17:55:52,989 - INFO - Fold 2 Val Epoch 4/200, Batch 20, Loss: 7.1473, Pearson: 0.4600, Spearman: 0.4719
2025-11-14 17:55:55,413 - INFO - Fold 2 Val Epoch 4/200, Val Loss: 8.7198, Pearson Mean: 0.4987, Spearman Mean: 0.4946
2025-11-14 17:55:55,414 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2932, 'spearman_mean_genewise': 0.2727, 'l1_error_mean': 2.1319, 'l2_errors_mean': 8.7151, 'r2_scores_mean': 0.074, 'pearson_std': 0.1054, 'l2_error_q1': 5.2083, 'l2_error_q2': 8.0886, 'l2_error_q3': 11.9915, 'r2_score_q1': 0.0347, 'r2_score_q2': 0.064, 'r2_score_q3': 0.1045, 'mape_mean': 66.568, 'mape_std': 17.3175, 'rmse_mean': 2.8708, 'rmse_std': 0.6881}
2025-11-14 17:55:55,414 - INFO - Learning rate for epoch 4: 0.0001
2025-11-14 17:55:55,476 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5785
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5784
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5931
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5975
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5768
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5932
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5893
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6000
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5939
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5743
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5839
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5853
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5874
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5887
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5965
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5953
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5870
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5848
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5925
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5814
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5952
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5987
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5814
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5858
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5859
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5880
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6061
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5966
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5822
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5979
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5964
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5920
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5814
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5905
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5827
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5897
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6008
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5765
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5786
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5811
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5903
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5802
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5913
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5722
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5841
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5728
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6030
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5903
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5707
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5954
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5773
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.5917
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.038555145263672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 5 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 7.026302 7.719005]
Sample y_pred values (first sample, first 10 genes):
[0.74857557 1.1638838  1.9965723  0.8395805  3.4141762  2.3003092
 0.48012984 1.7408922  3.1800685  7.5042057 ]
2025-11-14 17:55:56,428 - INFO - Fold 2 Train Epoch 5/200, Batch 0, Loss: 7.8515, Pearson: 0.6032, Spearman: 0.5782
2025-11-14 17:56:06,637 - INFO - Fold 2 Train Epoch 5/200, Batch 10, Loss: 7.8053, Pearson: 0.5898, Spearman: 0.5574
2025-11-14 17:56:16,816 - INFO - Fold 2 Train Epoch 5/200, Batch 20, Loss: 7.9488, Pearson: 0.5985, Spearman: 0.5681
2025-11-14 17:56:27,016 - INFO - Fold 2 Train Epoch 5/200, Batch 30, Loss: 7.6906, Pearson: 0.5963, Spearman: 0.5686
2025-11-14 17:56:37,201 - INFO - Fold 2 Train Epoch 5/200, Batch 40, Loss: 7.6638, Pearson: 0.6008, Spearman: 0.5591
2025-11-14 17:56:47,396 - INFO - Fold 2 Train Epoch 5/200, Batch 50, Loss: 8.1951, Pearson: 0.5986, Spearman: 0.5678
y_true  -> mean=2.2122, std=3.5123, min=0.0000, max=12.5140
y_pred  -> mean=2.1513, std=2.0936, min=0.0000, max=12.8078
Batch 0 Pearson correlation: 0.6032
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5994
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5928
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5845
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5911
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5742
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5919
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5856
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6062
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5898
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5983
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5943
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5944
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5902
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5961
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5966
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5950
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5985
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5897
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5883
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5857
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5894
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5916
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5956
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6064
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6011
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5934
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6042
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6013
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5814
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5930
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5931
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5912
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5961
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5975
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6008
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5924
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5986
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5923
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5928
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5927
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5986
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5928
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5970
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5882
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6041
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5946
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6076
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6078
2025-11-14 17:56:55,449 - INFO - Fold 2 Train Epoch 5/200, Batch 60, Loss: 7.7051, Pearson: 0.5954, Spearman: 0.5597
2025-11-14 17:57:03,697 - INFO - Fold 2 Train Epoch 5/200, Batch 70, Loss: 7.9547, Pearson: 0.6028, Spearman: 0.5717
2025-11-14 17:57:13,870 - INFO - Fold 2 Train Epoch 5/200, Batch 80, Loss: 7.8891, Pearson: 0.5870, Spearman: 0.5593
2025-11-14 17:57:19,112 - INFO - Fold 2 Train Epoch 5/200, Train Loss: 7.8608, Pearson Mean: 0.5977, Spearman Mean: 0.5656
2025-11-14 17:57:19,113 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3672, 'spearman_mean_genewise': 0.3335, 'l1_error_mean': 2.0191, 'l2_errors_mean': 7.8607, 'r2_scores_mean': 0.1416, 'pearson_std': 0.1077, 'l2_error_q1': 5.199, 'l2_error_q2': 7.4456, 'l2_error_q3': 10.3381, 'r2_score_q1': 0.0808, 'r2_score_q2': 0.1261, 'r2_score_q3': 0.1755, 'mape_mean': 62.3032, 'mape_std': 17.7201, 'rmse_mean': 2.7512, 'rmse_std': 0.5398}
2025-11-14 17:57:19,433 - INFO - Fold 2 Val Epoch 5/200, Batch 0, Loss: 10.1432, Pearson: 0.4808, Spearman: 0.4872
2025-11-14 17:57:21,225 - INFO - Fold 2 Val Epoch 5/200, Batch 10, Loss: 10.4351, Pearson: 0.5330, Spearman: 0.5353
2025-11-14 17:57:22,934 - INFO - Fold 2 Val Epoch 5/200, Batch 20, Loss: 6.5875, Pearson: 0.5133, Spearman: 0.5119
2025-11-14 17:57:25,352 - INFO - Fold 2 Val Epoch 5/200, Val Loss: 8.4775, Pearson Mean: 0.5288, Spearman Mean: 0.5199
2025-11-14 17:57:25,352 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.292, 'spearman_mean_genewise': 0.276, 'l1_error_mean': 2.1202, 'l2_errors_mean': 8.4923, 'r2_scores_mean': 0.0869, 'pearson_std': 0.115, 'l2_error_q1': 5.2213, 'l2_error_q2': 8.1024, 'l2_error_q3': 11.7405, 'r2_score_q1': 0.0359, 'r2_score_q2': 0.0658, 'r2_score_q3': 0.1089, 'mape_mean': 67.5456, 'mape_std': 17.9981, 'rmse_mean': 2.8423, 'rmse_std': 0.643}
2025-11-14 17:57:25,352 - INFO - Learning rate for epoch 5: 0.0001
2025-11-14 17:57:25,412 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 17:57:26,342 - INFO - Fold 2 Train Epoch 6/200, Batch 0, Loss: 7.5514, Pearson: 0.5903, Spearman: 0.5627
2025-11-14 17:57:36,579 - INFO - Fold 2 Train Epoch 6/200, Batch 10, Loss: 7.6427, Pearson: 0.6163, Spearman: 0.5780
2025-11-14 17:57:46,746 - INFO - Fold 2 Train Epoch 6/200, Batch 20, Loss: 7.7656, Pearson: 0.6002, Spearman: 0.5718
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5973
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5954
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5975
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5985
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5939
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6009
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5906
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5987
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6117
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5983
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6028
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6148
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6038
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6113
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6084
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6043
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5870
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5962
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6031
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5800
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.5936
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.860658645629883
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 6 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.975213 0.       0.       0.       0.       0.
 0.       9.073596]
Sample y_pred values (first sample, first 10 genes):
[0.5062624  1.5444655  2.0898817  0.5291302  3.679421   1.9184744
 0.32430503 1.3267633  2.9659133  7.59413   ]
y_true  -> mean=1.8906, std=3.3906, min=0.0000, max=13.8155
y_pred  -> mean=2.1332, std=2.0444, min=0.0000, max=13.7802
Batch 0 Pearson correlation: 0.5903
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6072
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6069
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6048
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6012
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5995
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6068
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6020
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5901
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6051
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5969
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5994
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6002
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5888
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5962
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5943
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6084
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6070
2025-11-14 17:57:56,904 - INFO - Fold 2 Train Epoch 6/200, Batch 30, Loss: 7.8266, Pearson: 0.6052, Spearman: 0.5714
2025-11-14 17:58:07,096 - INFO - Fold 2 Train Epoch 6/200, Batch 40, Loss: 7.7319, Pearson: 0.6078, Spearman: 0.5757
2025-11-14 17:58:17,294 - INFO - Fold 2 Train Epoch 6/200, Batch 50, Loss: 7.8214, Pearson: 0.6020, Spearman: 0.5664
2025-11-14 17:58:25,326 - INFO - Fold 2 Train Epoch 6/200, Batch 60, Loss: 7.7959, Pearson: 0.6016, Spearman: 0.5734
2025-11-14 17:58:33,468 - INFO - Fold 2 Train Epoch 6/200, Batch 70, Loss: 7.6523, Pearson: 0.6127, Spearman: 0.5775
2025-11-14 17:58:43,645 - INFO - Fold 2 Train Epoch 6/200, Batch 80, Loss: 7.7774, Pearson: 0.6131, Spearman: 0.5757
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5990
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6021
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6052
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5980
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6035
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6156
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6000
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6072
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6078
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6058
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6060
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6094
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6075
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6060
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6054
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6020
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6078
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6023
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6039
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6062
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5981
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6008
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6078
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6060
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6118
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6082
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5949
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6064
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6133
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.5960
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
2025-11-14 17:58:48,897 - INFO - Fold 2 Train Epoch 6/200, Train Loss: 7.7274, Pearson Mean: 0.6064, Spearman Mean: 0.5719
2025-11-14 17:58:48,897 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3813, 'spearman_mean_genewise': 0.3451, 'l1_error_mean': 1.9984, 'l2_errors_mean': 7.7275, 'r2_scores_mean': 0.1539, 'pearson_std': 0.1087, 'l2_error_q1': 5.1561, 'l2_error_q2': 7.305, 'l2_error_q3': 10.1118, 'r2_score_q1': 0.09, 'r2_score_q2': 0.1371, 'r2_score_q3': 0.1895, 'mape_mean': 61.424, 'mape_std': 17.7351, 'rmse_mean': 2.7291, 'rmse_std': 0.5284}
2025-11-14 17:58:49,242 - INFO - Fold 2 Val Epoch 6/200, Batch 0, Loss: 10.2175, Pearson: 0.4750, Spearman: 0.4836
2025-11-14 17:58:51,032 - INFO - Fold 2 Val Epoch 6/200, Batch 10, Loss: 10.0634, Pearson: 0.5381, Spearman: 0.5428
2025-11-14 17:58:52,767 - INFO - Fold 2 Val Epoch 6/200, Batch 20, Loss: 6.7267, Pearson: 0.4987, Spearman: 0.5033
2025-11-14 17:58:55,170 - INFO - Fold 2 Val Epoch 6/200, Val Loss: 8.5313, Pearson Mean: 0.5222, Spearman Mean: 0.5150
2025-11-14 17:58:55,171 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.302, 'spearman_mean_genewise': 0.2813, 'l1_error_mean': 2.059, 'l2_errors_mean': 8.539, 'r2_scores_mean': 0.0835, 'pearson_std': 0.1156, 'l2_error_q1': 5.2086, 'l2_error_q2': 8.112, 'l2_error_q3': 11.8, 'r2_score_q1': 0.0323, 'r2_score_q2': 0.0641, 'r2_score_q3': 0.1117, 'mape_mean': 66.4185, 'mape_std': 17.5891, 'rmse_mean': 2.8484, 'rmse_std': 0.6523}
2025-11-14 17:58:55,171 - INFO - Learning rate for epoch 6: 0.0001
2025-11-14 17:58:55,234 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 17:58:56,157 - INFO - Fold 2 Train Epoch 7/200, Batch 0, Loss: 7.6183, Pearson: 0.6156, Spearman: 0.5756
2025-11-14 17:59:06,387 - INFO - Fold 2 Train Epoch 7/200, Batch 10, Loss: 7.6696, Pearson: 0.6127, Spearman: 0.5797
2025-11-14 17:59:16,605 - INFO - Fold 2 Train Epoch 7/200, Batch 20, Loss: 7.7105, Pearson: 0.6158, Spearman: 0.5804
2025-11-14 17:59:26,793 - INFO - Fold 2 Train Epoch 7/200, Batch 30, Loss: 7.6992, Pearson: 0.6070, Spearman: 0.5753
2025-11-14 17:59:36,968 - INFO - Fold 2 Train Epoch 7/200, Batch 40, Loss: 7.4610, Pearson: 0.6192, Spearman: 0.5725
2025-11-14 17:59:47,135 - INFO - Fold 2 Train Epoch 7/200, Batch 50, Loss: 7.6888, Pearson: 0.6159, Spearman: 0.5825
========================= 7.727477073669434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 7 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       9.064194]
Sample y_pred values (first sample, first 10 genes):
[0.4067263  0.5147278  1.2280755  0.19751254 2.2307308  1.159595
 0.04700819 0.70420027 1.852592   6.8395405 ]
y_true  -> mean=2.1761, std=3.5017, min=0.0000, max=12.6839
y_pred  -> mean=2.1550, std=2.2080, min=0.0000, max=14.0537
Batch 0 Pearson correlation: 0.6156
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5976
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6118
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6085
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6078
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6032
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6062
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6013
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6033
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6089
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6029
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6070
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6073
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6153
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6097
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6165
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6151
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6026
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6165
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6038
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6075
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 17:59:55,210 - INFO - Fold 2 Train Epoch 7/200, Batch 60, Loss: 7.5705, Pearson: 0.6144, Spearman: 0.5800
2025-11-14 18:00:03,425 - INFO - Fold 2 Train Epoch 7/200, Batch 70, Loss: 7.4906, Pearson: 0.6161, Spearman: 0.5793
2025-11-14 18:00:13,597 - INFO - Fold 2 Train Epoch 7/200, Batch 80, Loss: 7.4672, Pearson: 0.6239, Spearman: 0.5864
2025-11-14 18:00:18,834 - INFO - Fold 2 Train Epoch 7/200, Train Loss: 7.6199, Pearson Mean: 0.6135, Spearman Mean: 0.5775
2025-11-14 18:00:18,834 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3913, 'spearman_mean_genewise': 0.3538, 'l1_error_mean': 1.9709, 'l2_errors_mean': 7.62, 'r2_scores_mean': 0.1632, 'pearson_std': 0.1113, 'l2_error_q1': 5.1331, 'l2_error_q2': 7.1844, 'l2_error_q3': 9.9629, 'r2_score_q1': 0.0946, 'r2_score_q2': 0.144, 'r2_score_q3': 0.2026, 'mape_mean': 60.6965, 'mape_std': 18.017, 'rmse_mean': 2.7112, 'rmse_std': 0.5189}
2025-11-14 18:00:19,155 - INFO - Fold 2 Val Epoch 7/200, Batch 0, Loss: 10.4579, Pearson: 0.4759, Spearman: 0.4860
2025-11-14 18:00:20,937 - INFO - Fold 2 Val Epoch 7/200, Batch 10, Loss: 10.1332, Pearson: 0.5305, Spearman: 0.5376
2025-11-14 18:00:22,668 - INFO - Fold 2 Val Epoch 7/200, Batch 20, Loss: 6.5149, Pearson: 0.5221, Spearman: 0.5226
2025-11-14 18:00:25,082 - INFO - Fold 2 Val Epoch 7/200, Val Loss: 8.3860, Pearson Mean: 0.5379, Spearman Mean: 0.5292
2025-11-14 18:00:25,082 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3022, 'spearman_mean_genewise': 0.2803, 'l1_error_mean': 2.0886, 'l2_errors_mean': 8.4036, 'r2_scores_mean': 0.0916, 'pearson_std': 0.1189, 'l2_error_q1': 5.225, 'l2_error_q2': 8.0402, 'l2_error_q3': 11.455, 'r2_score_q1': 0.0341, 'r2_score_q2': 0.0688, 'r2_score_q3': 0.121, 'mape_mean': 65.2853, 'mape_std': 18.3626, 'rmse_mean': 2.8302, 'rmse_std': 0.6272}
2025-11-14 18:00:25,082 - INFO - Learning rate for epoch 7: 0.0001
2025-11-14 18:00:25,082 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 18:00:26,048 - INFO - Fold 2 Train Epoch 8/200, Batch 0, Loss: 7.5664, Pearson: 0.6234, Spearman: 0.5839
2025-11-14 18:00:36,261 - INFO - Fold 2 Train Epoch 8/200, Batch 10, Loss: 7.5190, Pearson: 0.6123, Spearman: 0.5870
2025-11-14 18:00:46,438 - INFO - Fold 2 Train Epoch 8/200, Batch 20, Loss: 7.5250, Pearson: 0.6101, Spearman: 0.5804
Batch 54 Pearson correlation: 0.6136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6121
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6054
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6041
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6079
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5994
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6202
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6320
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.619995594024658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 8 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        6.778743  0.        8.164184  0.        0.
 0.        7.8765965 7.471321 ]
Sample y_pred values (first sample, first 10 genes):
[1.4293425 2.0723999 4.3153706 1.0531898 5.3087044 4.697871  0.743752
 2.839451  4.2620535 6.6098804]
y_true  -> mean=2.1552, std=3.5033, min=0.0000, max=12.5632
y_pred  -> mean=1.9329, std=2.0691, min=0.0000, max=13.9711
Batch 0 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6111
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6099
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6069
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6025
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6101
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6115
2025-11-14 18:00:56,628 - INFO - Fold 2 Train Epoch 8/200, Batch 30, Loss: 7.6843, Pearson: 0.6163, Spearman: 0.5884
2025-11-14 18:01:06,800 - INFO - Fold 2 Train Epoch 8/200, Batch 40, Loss: 7.5941, Pearson: 0.6245, Spearman: 0.5846
2025-11-14 18:01:17,004 - INFO - Fold 2 Train Epoch 8/200, Batch 50, Loss: 7.5892, Pearson: 0.6199, Spearman: 0.5826
2025-11-14 18:01:25,047 - INFO - Fold 2 Train Epoch 8/200, Batch 60, Loss: 7.5706, Pearson: 0.6183, Spearman: 0.5795
2025-11-14 18:01:33,306 - INFO - Fold 2 Train Epoch 8/200, Batch 70, Loss: 7.7609, Pearson: 0.6113, Spearman: 0.5850
2025-11-14 18:01:43,503 - INFO - Fold 2 Train Epoch 8/200, Batch 80, Loss: 7.5488, Pearson: 0.6166, Spearman: 0.5788
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6117
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6064
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5980
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6059
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6153
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6090
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6042
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6113
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6044
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6100
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6147
2025-11-14 18:01:48,743 - INFO - Fold 2 Train Epoch 8/200, Train Loss: 7.5574, Pearson Mean: 0.6176, Spearman Mean: 0.5812
2025-11-14 18:01:48,743 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3973, 'spearman_mean_genewise': 0.359, 'l1_error_mean': 1.956, 'l2_errors_mean': 7.5577, 'r2_scores_mean': 0.1687, 'pearson_std': 0.1126, 'l2_error_q1': 5.106, 'l2_error_q2': 7.1156, 'l2_error_q3': 9.854, 'r2_score_q1': 0.0982, 'r2_score_q2': 0.1481, 'r2_score_q3': 0.2097, 'mape_mean': 60.5586, 'mape_std': 17.9997, 'rmse_mean': 2.7007, 'rmse_std': 0.5139}
2025-11-14 18:01:49,076 - INFO - Fold 2 Val Epoch 8/200, Batch 0, Loss: 10.8601, Pearson: 0.4756, Spearman: 0.4849
2025-11-14 18:01:50,864 - INFO - Fold 2 Val Epoch 8/200, Batch 10, Loss: 10.1970, Pearson: 0.5277, Spearman: 0.5399
2025-11-14 18:01:52,558 - INFO - Fold 2 Val Epoch 8/200, Batch 20, Loss: 6.5013, Pearson: 0.5229, Spearman: 0.5234
2025-11-14 18:01:54,968 - INFO - Fold 2 Val Epoch 8/200, Val Loss: 8.3792, Pearson Mean: 0.5420, Spearman Mean: 0.5339
2025-11-14 18:01:54,969 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3034, 'spearman_mean_genewise': 0.2833, 'l1_error_mean': 2.0761, 'l2_errors_mean': 8.4009, 'r2_scores_mean': 0.0898, 'pearson_std': 0.1224, 'l2_error_q1': 5.2865, 'l2_error_q2': 8.0604, 'l2_error_q3': 11.4746, 'r2_score_q1': 0.0304, 'r2_score_q2': 0.0666, 'r2_score_q3': 0.1198, 'mape_mean': 64.784, 'mape_std': 18.592, 'rmse_mean': 2.8307, 'rmse_std': 0.623}
2025-11-14 18:01:54,969 - INFO - Learning rate for epoch 8: 0.0001
2025-11-14 18:01:55,031 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 18:01:56,005 - INFO - Fold 2 Train Epoch 9/200, Batch 0, Loss: 7.6406, Pearson: 0.6268, Spearman: 0.5907
2025-11-14 18:02:06,223 - INFO - Fold 2 Train Epoch 9/200, Batch 10, Loss: 7.2799, Pearson: 0.6188, Spearman: 0.5816
2025-11-14 18:02:16,406 - INFO - Fold 2 Train Epoch 9/200, Batch 20, Loss: 7.5175, Pearson: 0.6275, Spearman: 0.5842
2025-11-14 18:02:26,582 - INFO - Fold 2 Train Epoch 9/200, Batch 30, Loss: 7.4294, Pearson: 0.6253, Spearman: 0.5876
2025-11-14 18:02:36,780 - INFO - Fold 2 Train Epoch 9/200, Batch 40, Loss: 7.6123, Pearson: 0.6160, Spearman: 0.5789
2025-11-14 18:02:46,958 - INFO - Fold 2 Train Epoch 9/200, Batch 50, Loss: 7.4326, Pearson: 0.6255, Spearman: 0.5893
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6155
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6278
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.557703495025635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 9 =====================
Sample y_true values (first sample, first 10 genes):
[6.742574 0.       0.       7.435131 8.351068 6.742574 6.742574 6.742574
 0.       8.687472]
Sample y_pred values (first sample, first 10 genes):
[0.93535316 2.6426587  2.0273426  1.1103864  5.289774   3.1041105
 0.50099826 2.5578456  3.9818041  7.9712687 ]
y_true  -> mean=2.2344, std=3.5351, min=0.0000, max=12.4769
y_pred  -> mean=2.0274, std=2.1155, min=0.0000, max=13.8463
Batch 0 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6090
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6152
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6152
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6091
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6255
2025-11-14 18:02:55,043 - INFO - Fold 2 Train Epoch 9/200, Batch 60, Loss: 7.5895, Pearson: 0.6183, Spearman: 0.5881
2025-11-14 18:03:03,295 - INFO - Fold 2 Train Epoch 9/200, Batch 70, Loss: 7.5920, Pearson: 0.6271, Spearman: 0.5932
2025-11-14 18:03:13,478 - INFO - Fold 2 Train Epoch 9/200, Batch 80, Loss: 7.3447, Pearson: 0.6274, Spearman: 0.5843
2025-11-14 18:03:18,713 - INFO - Fold 2 Train Epoch 9/200, Train Loss: 7.4935, Pearson Mean: 0.6216, Spearman Mean: 0.5850
2025-11-14 18:03:18,713 - INFO - Training Metrics: {'pearson_mean_genewise': 0.403, 'spearman_mean_genewise': 0.3637, 'l1_error_mean': 1.9477, 'l2_errors_mean': 7.4936, 'r2_scores_mean': 0.1744, 'pearson_std': 0.1142, 'l2_error_q1': 5.0295, 'l2_error_q2': 7.0867, 'l2_error_q3': 9.7179, 'r2_score_q1': 0.1016, 'r2_score_q2': 0.1529, 'r2_score_q3': 0.2141, 'mape_mean': 60.2733, 'mape_std': 18.1383, 'rmse_mean': 2.6896, 'rmse_std': 0.5094}
2025-11-14 18:03:18,997 - INFO - Fold 2 Val Epoch 9/200, Batch 0, Loss: 10.3886, Pearson: 0.4736, Spearman: 0.4806
2025-11-14 18:03:20,802 - INFO - Fold 2 Val Epoch 9/200, Batch 10, Loss: 9.8162, Pearson: 0.5388, Spearman: 0.5453
2025-11-14 18:03:22,562 - INFO - Fold 2 Val Epoch 9/200, Batch 20, Loss: 6.5508, Pearson: 0.5221, Spearman: 0.5221
2025-11-14 18:03:24,956 - INFO - Fold 2 Val Epoch 9/200, Val Loss: 8.3375, Pearson Mean: 0.5418, Spearman Mean: 0.5337
2025-11-14 18:03:24,956 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3084, 'spearman_mean_genewise': 0.2881, 'l1_error_mean': 2.0977, 'l2_errors_mean': 8.3573, 'r2_scores_mean': 0.094, 'pearson_std': 0.1221, 'l2_error_q1': 5.2409, 'l2_error_q2': 8.0458, 'l2_error_q3': 11.3921, 'r2_score_q1': 0.0343, 'r2_score_q2': 0.0693, 'r2_score_q3': 0.1215, 'mape_mean': 64.2483, 'mape_std': 18.7799, 'rmse_mean': 2.8235, 'rmse_std': 0.6206}
2025-11-14 18:03:24,956 - INFO - Learning rate for epoch 9: 0.0001
2025-11-14 18:03:25,019 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 18:03:25,996 - INFO - Fold 2 Train Epoch 10/200, Batch 0, Loss: 7.4010, Pearson: 0.6238, Spearman: 0.5846
2025-11-14 18:03:36,215 - INFO - Fold 2 Train Epoch 10/200, Batch 10, Loss: 7.2171, Pearson: 0.6303, Spearman: 0.5859
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6094
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6165
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6107
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6173
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6291
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6224
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.493630409240723
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 10 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        8.663837  0.        0.
 7.2780614 7.9708633 8.376213 ]
Sample y_pred values (first sample, first 10 genes):
[1.3313029 1.8766683 4.6005745 0.9181002 4.9501867 4.670326  0.841339
 2.5923967 4.824875  6.408512 ]
y_true  -> mean=2.0940, std=3.4803, min=0.0000, max=12.5056
y_pred  -> mean=2.1260, std=2.2101, min=0.0000, max=14.1373
Batch 0 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6173
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6261
2025-11-14 18:03:46,412 - INFO - Fold 2 Train Epoch 10/200, Batch 20, Loss: 7.4859, Pearson: 0.6269, Spearman: 0.5882
2025-11-14 18:03:56,591 - INFO - Fold 2 Train Epoch 10/200, Batch 30, Loss: 7.3283, Pearson: 0.6385, Spearman: 0.5992
2025-11-14 18:04:06,766 - INFO - Fold 2 Train Epoch 10/200, Batch 40, Loss: 7.3666, Pearson: 0.6235, Spearman: 0.5901
2025-11-14 18:04:16,955 - INFO - Fold 2 Train Epoch 10/200, Batch 50, Loss: 8.0121, Pearson: 0.6166, Spearman: 0.5825
2025-11-14 18:04:24,969 - INFO - Fold 2 Train Epoch 10/200, Batch 60, Loss: 7.4002, Pearson: 0.6239, Spearman: 0.5911
2025-11-14 18:04:33,204 - INFO - Fold 2 Train Epoch 10/200, Batch 70, Loss: 7.4434, Pearson: 0.6319, Spearman: 0.5899
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6151
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6234
2025-11-14 18:04:43,386 - INFO - Fold 2 Train Epoch 10/200, Batch 80, Loss: 7.4394, Pearson: 0.6334, Spearman: 0.5810
2025-11-14 18:04:48,626 - INFO - Fold 2 Train Epoch 10/200, Train Loss: 7.4376, Pearson Mean: 0.6251, Spearman Mean: 0.5883
2025-11-14 18:04:48,626 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4087, 'spearman_mean_genewise': 0.3691, 'l1_error_mean': 1.9323, 'l2_errors_mean': 7.4378, 'r2_scores_mean': 0.1794, 'pearson_std': 0.115, 'l2_error_q1': 5.0186, 'l2_error_q2': 7.028, 'l2_error_q3': 9.6784, 'r2_score_q1': 0.1053, 'r2_score_q2': 0.1562, 'r2_score_q3': 0.2206, 'mape_mean': 59.8419, 'mape_std': 18.2292, 'rmse_mean': 2.68, 'rmse_std': 0.5054}
2025-11-14 18:04:48,917 - INFO - Fold 2 Val Epoch 10/200, Batch 0, Loss: 10.2429, Pearson: 0.4763, Spearman: 0.4839
2025-11-14 18:04:50,656 - INFO - Fold 2 Val Epoch 10/200, Batch 10, Loss: 9.5624, Pearson: 0.5482, Spearman: 0.5493
2025-11-14 18:04:52,457 - INFO - Fold 2 Val Epoch 10/200, Batch 20, Loss: 6.6883, Pearson: 0.5095, Spearman: 0.5125
2025-11-14 18:04:54,859 - INFO - Fold 2 Val Epoch 10/200, Val Loss: 8.3199, Pearson Mean: 0.5414, Spearman Mean: 0.5338
2025-11-14 18:04:54,859 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3119, 'spearman_mean_genewise': 0.2904, 'l1_error_mean': 2.0711, 'l2_errors_mean': 8.3371, 'r2_scores_mean': 0.0968, 'pearson_std': 0.1217, 'l2_error_q1': 5.2293, 'l2_error_q2': 8.0326, 'l2_error_q3': 11.3273, 'r2_score_q1': 0.0365, 'r2_score_q2': 0.073, 'r2_score_q3': 0.1257, 'mape_mean': 64.6843, 'mape_std': 18.7958, 'rmse_mean': 2.8198, 'rmse_std': 0.6213}
2025-11-14 18:04:54,859 - INFO - Learning rate for epoch 10: 0.0001
2025-11-14 18:04:54,922 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 18:04:55,908 - INFO - Fold 2 Train Epoch 11/200, Batch 0, Loss: 7.3311, Pearson: 0.6299, Spearman: 0.5942
2025-11-14 18:05:06,122 - INFO - Fold 2 Train Epoch 11/200, Batch 10, Loss: 7.2571, Pearson: 0.6282, Spearman: 0.5897
2025-11-14 18:05:16,328 - INFO - Fold 2 Train Epoch 11/200, Batch 20, Loss: 6.9740, Pearson: 0.6401, Spearman: 0.5944
2025-11-14 18:05:26,501 - INFO - Fold 2 Train Epoch 11/200, Batch 30, Loss: 7.3194, Pearson: 0.6309, Spearman: 0.6009
2025-11-14 18:05:36,693 - INFO - Fold 2 Train Epoch 11/200, Batch 40, Loss: 7.3527, Pearson: 0.6287, Spearman: 0.5968
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6313
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6225
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.437815189361572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 11 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       8.190568 0.       0.       0.
 8.478181 8.883576]
Sample y_pred values (first sample, first 10 genes):
[0.65675914 2.1368728  2.6711845  0.88748026 6.1796484  3.2376873
 0.68547595 2.4443722  4.752179   9.519813  ]
y_true  -> mean=2.0990, std=3.4842, min=0.0000, max=12.6417
y_pred  -> mean=2.1537, std=2.2667, min=0.0000, max=13.6130
Batch 0 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6073
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6171
2025-11-14 18:05:46,874 - INFO - Fold 2 Train Epoch 11/200, Batch 50, Loss: 7.1636, Pearson: 0.6314, Spearman: 0.5982
2025-11-14 18:05:54,980 - INFO - Fold 2 Train Epoch 11/200, Batch 60, Loss: 7.6218, Pearson: 0.6200, Spearman: 0.5866
2025-11-14 18:06:03,211 - INFO - Fold 2 Train Epoch 11/200, Batch 70, Loss: 7.4318, Pearson: 0.6246, Spearman: 0.5905
2025-11-14 18:06:13,397 - INFO - Fold 2 Train Epoch 11/200, Batch 80, Loss: 7.5525, Pearson: 0.6283, Spearman: 0.5878
2025-11-14 18:06:18,649 - INFO - Fold 2 Train Epoch 11/200, Train Loss: 7.4369, Pearson Mean: 0.6252, Spearman Mean: 0.5893
2025-11-14 18:06:18,649 - INFO - Training Metrics: {'pearson_mean_genewise': 0.409, 'spearman_mean_genewise': 0.3697, 'l1_error_mean': 1.9281, 'l2_errors_mean': 7.4372, 'r2_scores_mean': 0.1796, 'pearson_std': 0.115, 'l2_error_q1': 5.0149, 'l2_error_q2': 7.0152, 'l2_error_q3': 9.6663, 'r2_score_q1': 0.1061, 'r2_score_q2': 0.1558, 'r2_score_q3': 0.2218, 'mape_mean': 59.749, 'mape_std': 18.2638, 'rmse_mean': 2.6798, 'rmse_std': 0.506}
2025-11-14 18:06:18,990 - INFO - Fold 2 Val Epoch 11/200, Batch 0, Loss: 9.9764, Pearson: 0.4804, Spearman: 0.4891
2025-11-14 18:06:20,773 - INFO - Fold 2 Val Epoch 11/200, Batch 10, Loss: 9.8375, Pearson: 0.5373, Spearman: 0.5412
2025-11-14 18:06:22,509 - INFO - Fold 2 Val Epoch 11/200, Batch 20, Loss: 7.3232, Pearson: 0.4397, Spearman: 0.4505
2025-11-14 18:06:24,906 - INFO - Fold 2 Val Epoch 11/200, Val Loss: 8.5421, Pearson Mean: 0.5169, Spearman Mean: 0.5112
2025-11-14 18:06:24,906 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3056, 'spearman_mean_genewise': 0.2827, 'l1_error_mean': 2.0447, 'l2_errors_mean': 8.5565, 'r2_scores_mean': 0.0836, 'pearson_std': 0.1132, 'l2_error_q1': 5.2039, 'l2_error_q2': 8.0492, 'l2_error_q3': 11.8172, 'r2_score_q1': 0.0326, 'r2_score_q2': 0.0667, 'r2_score_q3': 0.1133, 'mape_mean': 66.3372, 'mape_std': 17.962, 'rmse_mean': 2.8501, 'rmse_std': 0.6585}
2025-11-14 18:06:24,907 - INFO - Learning rate for epoch 11: 0.0001
2025-11-14 18:06:24,907 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 18:06:25,895 - INFO - Fold 2 Train Epoch 12/200, Batch 0, Loss: 7.9838, Pearson: 0.6071, Spearman: 0.5827
2025-11-14 18:06:36,092 - INFO - Fold 2 Train Epoch 12/200, Batch 10, Loss: 7.4584, Pearson: 0.6227, Spearman: 0.5885
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6076
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6151
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6085
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6143
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6291
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.4371562004089355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 12 =====================
Sample y_true values (first sample, first 10 genes):
[0.        6.8272595 0.        0.        7.519865  0.        0.
 6.8272595 7.519865  8.618115 ]
Sample y_pred values (first sample, first 10 genes):
[0.31294116 1.6395453  1.8282454  1.4862322  4.940439   3.0884051
 1.1699319  1.6259973  5.1563067  8.656371  ]
y_true  -> mean=2.2274, std=3.5528, min=0.0000, max=12.6263
y_pred  -> mean=2.1102, std=2.1596, min=0.0000, max=13.9712
Batch 0 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6083
2025-11-14 18:06:46,306 - INFO - Fold 2 Train Epoch 12/200, Batch 20, Loss: 7.4057, Pearson: 0.6275, Spearman: 0.5924
2025-11-14 18:06:56,477 - INFO - Fold 2 Train Epoch 12/200, Batch 30, Loss: 7.3673, Pearson: 0.6339, Spearman: 0.5974
2025-11-14 18:07:06,658 - INFO - Fold 2 Train Epoch 12/200, Batch 40, Loss: 7.6266, Pearson: 0.6216, Spearman: 0.5904
2025-11-14 18:07:16,850 - INFO - Fold 2 Train Epoch 12/200, Batch 50, Loss: 7.6785, Pearson: 0.6264, Spearman: 0.5891
2025-11-14 18:07:24,910 - INFO - Fold 2 Train Epoch 12/200, Batch 60, Loss: 7.3187, Pearson: 0.6162, Spearman: 0.5835
2025-11-14 18:07:33,134 - INFO - Fold 2 Train Epoch 12/200, Batch 70, Loss: 7.7438, Pearson: 0.6115, Spearman: 0.5796
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6086
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6152
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6117
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6296
2025-11-14 18:07:43,325 - INFO - Fold 2 Train Epoch 12/200, Batch 80, Loss: 7.7501, Pearson: 0.6316, Spearman: 0.5915
2025-11-14 18:07:48,557 - INFO - Fold 2 Train Epoch 12/200, Train Loss: 7.4346, Pearson Mean: 0.6253, Spearman Mean: 0.5898
2025-11-14 18:07:48,557 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4096, 'spearman_mean_genewise': 0.3704, 'l1_error_mean': 1.9258, 'l2_errors_mean': 7.4349, 'r2_scores_mean': 0.18, 'pearson_std': 0.1148, 'l2_error_q1': 5.0084, 'l2_error_q2': 7.0001, 'l2_error_q3': 9.6485, 'r2_score_q1': 0.1067, 'r2_score_q2': 0.1567, 'r2_score_q3': 0.221, 'mape_mean': 59.8171, 'mape_std': 18.2114, 'rmse_mean': 2.6793, 'rmse_std': 0.5064}
2025-11-14 18:07:48,877 - INFO - Fold 2 Val Epoch 12/200, Batch 0, Loss: 11.0420, Pearson: 0.4774, Spearman: 0.4840
2025-11-14 18:07:50,684 - INFO - Fold 2 Val Epoch 12/200, Batch 10, Loss: 9.5010, Pearson: 0.5497, Spearman: 0.5534
2025-11-14 18:07:52,393 - INFO - Fold 2 Val Epoch 12/200, Batch 20, Loss: 6.6108, Pearson: 0.5186, Spearman: 0.5169
2025-11-14 18:07:54,842 - INFO - Fold 2 Val Epoch 12/200, Val Loss: 8.3682, Pearson Mean: 0.5453, Spearman Mean: 0.5369
2025-11-14 18:07:54,843 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3125, 'spearman_mean_genewise': 0.2912, 'l1_error_mean': 2.0945, 'l2_errors_mean': 8.3898, 'r2_scores_mean': 0.0884, 'pearson_std': 0.1245, 'l2_error_q1': 5.2688, 'l2_error_q2': 8.0945, 'l2_error_q3': 11.4276, 'r2_score_q1': 0.0269, 'r2_score_q2': 0.0645, 'r2_score_q3': 0.122, 'mape_mean': 62.3839, 'mape_std': 19.0572, 'rmse_mean': 2.8299, 'rmse_std': 0.6177}
2025-11-14 18:07:54,843 - INFO - Learning rate for epoch 12: 0.0001
2025-11-14 18:07:54,905 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 18:07:55,856 - INFO - Fold 2 Train Epoch 13/200, Batch 0, Loss: 7.4410, Pearson: 0.6256, Spearman: 0.5840
2025-11-14 18:08:06,084 - INFO - Fold 2 Train Epoch 13/200, Batch 10, Loss: 7.8087, Pearson: 0.6214, Spearman: 0.5885
2025-11-14 18:08:16,291 - INFO - Fold 2 Train Epoch 13/200, Batch 20, Loss: 7.3652, Pearson: 0.6201, Spearman: 0.5904
2025-11-14 18:08:26,489 - INFO - Fold 2 Train Epoch 13/200, Batch 30, Loss: 7.8066, Pearson: 0.6191, Spearman: 0.5883
2025-11-14 18:08:36,640 - INFO - Fold 2 Train Epoch 13/200, Batch 40, Loss: 7.4442, Pearson: 0.6343, Spearman: 0.5924
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6258
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6281
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.434898853302002
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 13 =====================
Sample y_true values (first sample, first 10 genes):
[0.        6.6171703 0.        7.714891  8.407815  6.6171703 0.
 0.        0.        9.013849 ]
Sample y_pred values (first sample, first 10 genes):
[0.7108005 4.4227457 3.6997178 2.316143  6.3023343 4.8660946 0.9360602
 2.2657435 4.001913  8.782494 ]
y_true  -> mean=2.1249, std=3.4962, min=0.0000, max=12.6369
y_pred  -> mean=2.1151, std=2.2204, min=0.0000, max=13.4011
Batch 0 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6065
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6151
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6113
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6297
2025-11-14 18:08:46,828 - INFO - Fold 2 Train Epoch 13/200, Batch 50, Loss: 7.4361, Pearson: 0.6177, Spearman: 0.5852
2025-11-14 18:08:54,894 - INFO - Fold 2 Train Epoch 13/200, Batch 60, Loss: 7.4711, Pearson: 0.6292, Spearman: 0.5918
2025-11-14 18:09:03,092 - INFO - Fold 2 Train Epoch 13/200, Batch 70, Loss: 7.1848, Pearson: 0.6299, Spearman: 0.5967
2025-11-14 18:09:13,284 - INFO - Fold 2 Train Epoch 13/200, Batch 80, Loss: 7.3558, Pearson: 0.6217, Spearman: 0.5906
2025-11-14 18:09:18,542 - INFO - Fold 2 Train Epoch 13/200, Train Loss: 7.4613, Pearson Mean: 0.6236, Spearman Mean: 0.5894
2025-11-14 18:09:18,542 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4067, 'spearman_mean_genewise': 0.3684, 'l1_error_mean': 1.9343, 'l2_errors_mean': 7.4612, 'r2_scores_mean': 0.1776, 'pearson_std': 0.1145, 'l2_error_q1': 5.0157, 'l2_error_q2': 7.024, 'l2_error_q3': 9.6862, 'r2_score_q1': 0.1042, 'r2_score_q2': 0.1546, 'r2_score_q3': 0.217, 'mape_mean': 59.8081, 'mape_std': 18.2555, 'rmse_mean': 2.6837, 'rmse_std': 0.5086}
2025-11-14 18:09:18,864 - INFO - Fold 2 Val Epoch 13/200, Batch 0, Loss: 10.0101, Pearson: 0.4772, Spearman: 0.4863
2025-11-14 18:09:20,681 - INFO - Fold 2 Val Epoch 13/200, Batch 10, Loss: 10.1015, Pearson: 0.5377, Spearman: 0.5405
2025-11-14 18:09:22,409 - INFO - Fold 2 Val Epoch 13/200, Batch 20, Loss: 6.4756, Pearson: 0.5275, Spearman: 0.5273
2025-11-14 18:09:24,816 - INFO - Fold 2 Val Epoch 13/200, Val Loss: 8.2828, Pearson Mean: 0.5443, Spearman Mean: 0.5364
2025-11-14 18:09:24,817 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3127, 'spearman_mean_genewise': 0.2911, 'l1_error_mean': 2.0302, 'l2_errors_mean': 8.3026, 'r2_scores_mean': 0.1006, 'pearson_std': 0.1233, 'l2_error_q1': 5.2041, 'l2_error_q2': 8.0494, 'l2_error_q3': 11.325, 'r2_score_q1': 0.0372, 'r2_score_q2': 0.0742, 'r2_score_q3': 0.1323, 'mape_mean': 65.9383, 'mape_std': 18.4879, 'rmse_mean': 2.8138, 'rmse_std': 0.6207}
2025-11-14 18:09:24,817 - INFO - Learning rate for epoch 13: 0.0001
2025-11-14 18:09:24,817 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 18:09:25,776 - INFO - Fold 2 Train Epoch 14/200, Batch 0, Loss: 7.4040, Pearson: 0.6315, Spearman: 0.5980
2025-11-14 18:09:36,004 - INFO - Fold 2 Train Epoch 14/200, Batch 10, Loss: 7.4723, Pearson: 0.6320, Spearman: 0.5950
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6166
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6201
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.461165904998779
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 14 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.337196]
Sample y_pred values (first sample, first 10 genes):
[0.16042015 0.         0.22612578 0.1953564  2.8017316  0.57343197
 0.13507064 0.67155176 3.4876676  7.223253  ]
y_true  -> mean=2.1865, std=3.5071, min=0.0000, max=12.4942
y_pred  -> mean=2.0880, std=2.2002, min=0.0000, max=13.0561
Batch 0 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6278
2025-11-14 18:09:46,190 - INFO - Fold 2 Train Epoch 14/200, Batch 20, Loss: 7.3526, Pearson: 0.6259, Spearman: 0.5956
2025-11-14 18:09:56,394 - INFO - Fold 2 Train Epoch 14/200, Batch 30, Loss: 7.5554, Pearson: 0.6200, Spearman: 0.5918
2025-11-14 18:10:06,580 - INFO - Fold 2 Train Epoch 14/200, Batch 40, Loss: 7.2887, Pearson: 0.6343, Spearman: 0.5965
2025-11-14 18:10:16,785 - INFO - Fold 2 Train Epoch 14/200, Batch 50, Loss: 7.2179, Pearson: 0.6367, Spearman: 0.5953
2025-11-14 18:10:24,818 - INFO - Fold 2 Train Epoch 14/200, Batch 60, Loss: 7.2095, Pearson: 0.6297, Spearman: 0.5931
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6165
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6382
2025-11-14 18:10:33,063 - INFO - Fold 2 Train Epoch 14/200, Batch 70, Loss: 7.2728, Pearson: 0.6279, Spearman: 0.5982
2025-11-14 18:10:43,275 - INFO - Fold 2 Train Epoch 14/200, Batch 80, Loss: 7.3402, Pearson: 0.6264, Spearman: 0.5908
2025-11-14 18:10:48,546 - INFO - Fold 2 Train Epoch 14/200, Train Loss: 7.3636, Pearson Mean: 0.6299, Spearman Mean: 0.5940
2025-11-14 18:10:48,546 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4168, 'spearman_mean_genewise': 0.3765, 'l1_error_mean': 1.9131, 'l2_errors_mean': 7.3636, 'r2_scores_mean': 0.1865, 'pearson_std': 0.116, 'l2_error_q1': 4.9886, 'l2_error_q2': 6.958, 'l2_error_q3': 9.5806, 'r2_score_q1': 0.1107, 'r2_score_q2': 0.1616, 'r2_score_q3': 0.2288, 'mape_mean': 59.3158, 'mape_std': 18.2907, 'rmse_mean': 2.6668, 'rmse_std': 0.502}
2025-11-14 18:10:48,889 - INFO - Fold 2 Val Epoch 14/200, Batch 0, Loss: 10.0262, Pearson: 0.4800, Spearman: 0.4875
2025-11-14 18:10:50,702 - INFO - Fold 2 Val Epoch 14/200, Batch 10, Loss: 10.2200, Pearson: 0.5370, Spearman: 0.5422
2025-11-14 18:10:52,458 - INFO - Fold 2 Val Epoch 14/200, Batch 20, Loss: 6.3482, Pearson: 0.5394, Spearman: 0.5390
2025-11-14 18:10:54,887 - INFO - Fold 2 Val Epoch 14/200, Val Loss: 8.2733, Pearson Mean: 0.5459, Spearman Mean: 0.5377
2025-11-14 18:10:54,887 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.312, 'spearman_mean_genewise': 0.2927, 'l1_error_mean': 2.0215, 'l2_errors_mean': 8.2928, 'r2_scores_mean': 0.1009, 'pearson_std': 0.1246, 'l2_error_q1': 5.2056, 'l2_error_q2': 8.015, 'l2_error_q3': 11.289, 'r2_score_q1': 0.039, 'r2_score_q2': 0.0756, 'r2_score_q3': 0.1319, 'mape_mean': 66.3573, 'mape_std': 18.6943, 'rmse_mean': 2.8124, 'rmse_std': 0.6189}
2025-11-14 18:10:54,887 - INFO - Learning rate for epoch 14: 0.0001
2025-11-14 18:10:54,955 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 18:10:55,868 - INFO - Fold 2 Train Epoch 15/200, Batch 0, Loss: 7.2491, Pearson: 0.6297, Spearman: 0.5896
2025-11-14 18:11:06,110 - INFO - Fold 2 Train Epoch 15/200, Batch 10, Loss: 7.4821, Pearson: 0.6403, Spearman: 0.6047
2025-11-14 18:11:16,271 - INFO - Fold 2 Train Epoch 15/200, Batch 20, Loss: 7.0529, Pearson: 0.6417, Spearman: 0.6047
2025-11-14 18:11:26,459 - INFO - Fold 2 Train Epoch 15/200, Batch 30, Loss: 7.1127, Pearson: 0.6285, Spearman: 0.5917
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6259
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6260
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.363575458526611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 15 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        7.6401243 0.        0.
 0.        0.        8.333031 ]
Sample y_pred values (first sample, first 10 genes):
[0.26125908 2.1033216  0.83799183 1.5892445  4.671161   2.4661949
 1.2619877  2.1343822  3.281376   7.410503  ]
y_true  -> mean=2.0402, std=3.4649, min=0.0000, max=12.7151
y_pred  -> mean=2.0976, std=2.2222, min=0.0000, max=13.3808
Batch 0 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6338
2025-11-14 18:11:36,656 - INFO - Fold 2 Train Epoch 15/200, Batch 40, Loss: 7.3115, Pearson: 0.6357, Spearman: 0.5955
2025-11-14 18:11:46,843 - INFO - Fold 2 Train Epoch 15/200, Batch 50, Loss: 7.2136, Pearson: 0.6334, Spearman: 0.5974
2025-11-14 18:11:54,869 - INFO - Fold 2 Train Epoch 15/200, Batch 60, Loss: 7.4077, Pearson: 0.6250, Spearman: 0.5920
2025-11-14 18:12:01,084 - INFO - Fold 2 Train Epoch 15/200, Batch 70, Loss: 7.1828, Pearson: 0.6455, Spearman: 0.6056
2025-11-14 18:12:07,345 - INFO - Fold 2 Train Epoch 15/200, Batch 80, Loss: 7.4058, Pearson: 0.6392, Spearman: 0.5952
2025-11-14 18:12:11,033 - INFO - Fold 2 Train Epoch 15/200, Train Loss: 7.3155, Pearson Mean: 0.6330, Spearman Mean: 0.5967
2025-11-14 18:12:11,033 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4221, 'spearman_mean_genewise': 0.381, 'l1_error_mean': 1.9002, 'l2_errors_mean': 7.3154, 'r2_scores_mean': 0.191, 'pearson_std': 0.116, 'l2_error_q1': 4.9619, 'l2_error_q2': 6.9093, 'l2_error_q3': 9.4538, 'r2_score_q1': 0.1143, 'r2_score_q2': 0.1667, 'r2_score_q3': 0.2347, 'mape_mean': 59.1812, 'mape_std': 18.3155, 'rmse_mean': 2.6583, 'rmse_std': 0.4989}
2025-11-14 18:12:11,312 - INFO - Fold 2 Val Epoch 15/200, Batch 0, Loss: 10.4349, Pearson: 0.4826, Spearman: 0.4902
2025-11-14 18:12:12,550 - INFO - Fold 2 Val Epoch 15/200, Batch 10, Loss: 10.0435, Pearson: 0.5409, Spearman: 0.5438
2025-11-14 18:12:13,772 - INFO - Fold 2 Val Epoch 15/200, Batch 20, Loss: 6.5949, Pearson: 0.5169, Spearman: 0.5190
2025-11-14 18:12:16,197 - INFO - Fold 2 Val Epoch 15/200, Val Loss: 8.3261, Pearson Mean: 0.5446, Spearman Mean: 0.5372
2025-11-14 18:12:16,197 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3061, 'spearman_mean_genewise': 0.2872, 'l1_error_mean': 2.0579, 'l2_errors_mean': 8.3491, 'r2_scores_mean': 0.0953, 'pearson_std': 0.124, 'l2_error_q1': 5.2389, 'l2_error_q2': 8.0599, 'l2_error_q3': 11.4055, 'r2_score_q1': 0.0335, 'r2_score_q2': 0.0693, 'r2_score_q3': 0.1259, 'mape_mean': 64.9723, 'mape_std': 19.2208, 'rmse_mean': 2.8217, 'rmse_std': 0.6223}
2025-11-14 18:12:16,198 - INFO - Learning rate for epoch 15: 0.0001
2025-11-14 18:12:16,198 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 18:12:17,169 - INFO - Fold 2 Train Epoch 16/200, Batch 0, Loss: 7.5766, Pearson: 0.6257, Spearman: 0.5931
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6275
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6351
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.315417766571045
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 16 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        6.939184  6.939184  0.
 6.939184  7.6318464 8.547847 ]
Sample y_pred values (first sample, first 10 genes):
[0.46019793 2.1260955  1.5735885  1.2262516  4.9747996  2.711401
 0.9015066  2.295366   3.0456762  8.433893  ]
y_true  -> mean=2.1704, std=3.5272, min=0.0000, max=12.5082
y_pred  -> mean=2.0902, std=2.2014, min=0.0000, max=13.4017
Batch 0 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6286
2025-11-14 18:12:27,349 - INFO - Fold 2 Train Epoch 16/200, Batch 10, Loss: 7.4711, Pearson: 0.6340, Spearman: 0.5995
2025-11-14 18:12:37,419 - INFO - Fold 2 Train Epoch 16/200, Batch 20, Loss: 7.3270, Pearson: 0.6395, Spearman: 0.6023
2025-11-14 18:12:47,540 - INFO - Fold 2 Train Epoch 16/200, Batch 30, Loss: 7.2641, Pearson: 0.6222, Spearman: 0.5931
2025-11-14 18:12:57,677 - INFO - Fold 2 Train Epoch 16/200, Batch 40, Loss: 7.2974, Pearson: 0.6285, Spearman: 0.5908
2025-11-14 18:13:07,745 - INFO - Fold 2 Train Epoch 16/200, Batch 50, Loss: 7.4493, Pearson: 0.6331, Spearman: 0.5979
2025-11-14 18:13:17,895 - INFO - Fold 2 Train Epoch 16/200, Batch 60, Loss: 7.4658, Pearson: 0.6340, Spearman: 0.6010
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6151
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6220
2025-11-14 18:13:28,019 - INFO - Fold 2 Train Epoch 16/200, Batch 70, Loss: 7.3433, Pearson: 0.6277, Spearman: 0.5905
2025-11-14 18:13:38,167 - INFO - Fold 2 Train Epoch 16/200, Batch 80, Loss: 7.3700, Pearson: 0.6257, Spearman: 0.5932
2025-11-14 18:13:42,631 - INFO - Fold 2 Train Epoch 16/200, Train Loss: 7.3591, Pearson Mean: 0.6301, Spearman Mean: 0.5952
2025-11-14 18:13:42,631 - INFO - Training Metrics: {'pearson_mean_genewise': 0.418, 'spearman_mean_genewise': 0.3775, 'l1_error_mean': 1.9079, 'l2_errors_mean': 7.359, 'r2_scores_mean': 0.1873, 'pearson_std': 0.1153, 'l2_error_q1': 4.9753, 'l2_error_q2': 6.9647, 'l2_error_q3': 9.4986, 'r2_score_q1': 0.112, 'r2_score_q2': 0.1631, 'r2_score_q3': 0.2293, 'mape_mean': 59.2233, 'mape_std': 18.2304, 'rmse_mean': 2.6658, 'rmse_std': 0.5026}
2025-11-14 18:13:42,973 - INFO - Fold 2 Val Epoch 16/200, Batch 0, Loss: 10.2530, Pearson: 0.4604, Spearman: 0.4666
2025-11-14 18:13:44,566 - INFO - Fold 2 Val Epoch 16/200, Batch 10, Loss: 9.4478, Pearson: 0.5507, Spearman: 0.5522
2025-11-14 18:13:45,812 - INFO - Fold 2 Val Epoch 16/200, Batch 20, Loss: 6.6123, Pearson: 0.5229, Spearman: 0.5228
2025-11-14 18:13:48,207 - INFO - Fold 2 Val Epoch 16/200, Val Loss: 8.3894, Pearson Mean: 0.5394, Spearman Mean: 0.5320
2025-11-14 18:13:48,208 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3029, 'spearman_mean_genewise': 0.2823, 'l1_error_mean': 2.1065, 'l2_errors_mean': 8.4159, 'r2_scores_mean': 0.0889, 'pearson_std': 0.1251, 'l2_error_q1': 5.2482, 'l2_error_q2': 8.0833, 'l2_error_q3': 11.5117, 'r2_score_q1': 0.0309, 'r2_score_q2': 0.0649, 'r2_score_q3': 0.1178, 'mape_mean': 64.2755, 'mape_std': 19.2554, 'rmse_mean': 2.8322, 'rmse_std': 0.628}
2025-11-14 18:13:48,208 - INFO - Learning rate for epoch 16: 0.0001
2025-11-14 18:13:48,208 - INFO - No improvement in spearman genewise. Patience: 2/30
2025-11-14 18:13:49,156 - INFO - Fold 2 Train Epoch 17/200, Batch 0, Loss: 7.2058, Pearson: 0.6383, Spearman: 0.5993
2025-11-14 18:13:59,367 - INFO - Fold 2 Train Epoch 17/200, Batch 10, Loss: 7.2969, Pearson: 0.6314, Spearman: 0.5961
2025-11-14 18:14:09,524 - INFO - Fold 2 Train Epoch 17/200, Batch 20, Loss: 7.5155, Pearson: 0.6215, Spearman: 0.5945
2025-11-14 18:14:19,694 - INFO - Fold 2 Train Epoch 17/200, Batch 30, Loss: 7.4824, Pearson: 0.6257, Spearman: 0.5943
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6192
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6294
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.3589935302734375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 17 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.06449604 0.         1.6103109  0.         1.6811874  0.6092286
 0.         0.29714692 1.7902119  6.271792  ]
y_true  -> mean=2.1401, std=3.4861, min=0.0000, max=12.9358
y_pred  -> mean=2.0727, std=2.2145, min=0.0000, max=13.5809
Batch 0 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6153
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6290
2025-11-14 18:14:29,867 - INFO - Fold 2 Train Epoch 17/200, Batch 40, Loss: 7.3179, Pearson: 0.6333, Spearman: 0.5997
2025-11-14 18:14:40,055 - INFO - Fold 2 Train Epoch 17/200, Batch 50, Loss: 7.4321, Pearson: 0.6279, Spearman: 0.5945
2025-11-14 18:14:50,187 - INFO - Fold 2 Train Epoch 17/200, Batch 60, Loss: 7.1857, Pearson: 0.6312, Spearman: 0.5927
2025-11-14 18:15:00,345 - INFO - Fold 2 Train Epoch 17/200, Batch 70, Loss: 7.4369, Pearson: 0.6237, Spearman: 0.5905
2025-11-14 18:15:10,520 - INFO - Fold 2 Train Epoch 17/200, Batch 80, Loss: 7.3995, Pearson: 0.6334, Spearman: 0.6030
2025-11-14 18:15:15,022 - INFO - Fold 2 Train Epoch 17/200, Train Loss: 7.4092, Pearson Mean: 0.6270, Spearman Mean: 0.5935
2025-11-14 18:15:15,022 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4117, 'spearman_mean_genewise': 0.3724, 'l1_error_mean': 1.9171, 'l2_errors_mean': 7.4094, 'r2_scores_mean': 0.1822, 'pearson_std': 0.1161, 'l2_error_q1': 4.9976, 'l2_error_q2': 6.9994, 'l2_error_q3': 9.6271, 'r2_score_q1': 0.1057, 'r2_score_q2': 0.158, 'r2_score_q3': 0.2234, 'mape_mean': 59.7843, 'mape_std': 18.1411, 'rmse_mean': 2.6745, 'rmse_std': 0.5064}
2025-11-14 18:15:15,357 - INFO - Fold 2 Val Epoch 17/200, Batch 0, Loss: 10.2151, Pearson: 0.4632, Spearman: 0.4718
2025-11-14 18:15:16,799 - INFO - Fold 2 Val Epoch 17/200, Batch 10, Loss: 9.7575, Pearson: 0.5446, Spearman: 0.5543
2025-11-14 18:15:18,018 - INFO - Fold 2 Val Epoch 17/200, Batch 20, Loss: 6.4761, Pearson: 0.5300, Spearman: 0.5289
2025-11-14 18:15:20,419 - INFO - Fold 2 Val Epoch 17/200, Val Loss: 8.2980, Pearson Mean: 0.5439, Spearman Mean: 0.5364
2025-11-14 18:15:20,419 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.312, 'spearman_mean_genewise': 0.2908, 'l1_error_mean': 2.0581, 'l2_errors_mean': 8.3207, 'r2_scores_mean': 0.0956, 'pearson_std': 0.1272, 'l2_error_q1': 5.2307, 'l2_error_q2': 8.0195, 'l2_error_q3': 11.3435, 'r2_score_q1': 0.0322, 'r2_score_q2': 0.0685, 'r2_score_q3': 0.1294, 'mape_mean': 64.3241, 'mape_std': 18.7416, 'rmse_mean': 2.8182, 'rmse_std': 0.6153}
2025-11-14 18:15:20,419 - INFO - Learning rate for epoch 17: 0.0001
2025-11-14 18:15:20,419 - INFO - No improvement in spearman genewise. Patience: 3/30
2025-11-14 18:15:21,371 - INFO - Fold 2 Train Epoch 18/200, Batch 0, Loss: 7.1868, Pearson: 0.6401, Spearman: 0.6012
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6148
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6204
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6137
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6356
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6266
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.409407138824463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 18 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       8.948106 0.       0.       0.       0.
 0.       8.948106]
Sample y_pred values (first sample, first 10 genes):
[0.         0.28569707 0.9891133  0.         0.5732913  0.33089626
 0.         0.6428653  0.63580334 7.1274767 ]
y_true  -> mean=2.1238, std=3.4893, min=0.0000, max=12.4193
y_pred  -> mean=2.1130, std=2.2528, min=0.0000, max=13.5836
Batch 0 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6302
2025-11-14 18:15:31,575 - INFO - Fold 2 Train Epoch 18/200, Batch 10, Loss: 7.1162, Pearson: 0.6369, Spearman: 0.6035
2025-11-14 18:15:41,743 - INFO - Fold 2 Train Epoch 18/200, Batch 20, Loss: 7.5261, Pearson: 0.6227, Spearman: 0.5859
2025-11-14 18:15:51,936 - INFO - Fold 2 Train Epoch 18/200, Batch 30, Loss: 7.4071, Pearson: 0.6281, Spearman: 0.5981
2025-11-14 18:16:02,079 - INFO - Fold 2 Train Epoch 18/200, Batch 40, Loss: 7.2301, Pearson: 0.6375, Spearman: 0.5993
2025-11-14 18:16:12,244 - INFO - Fold 2 Train Epoch 18/200, Batch 50, Loss: 7.0756, Pearson: 0.6372, Spearman: 0.5974
2025-11-14 18:16:22,440 - INFO - Fold 2 Train Epoch 18/200, Batch 60, Loss: 7.2268, Pearson: 0.6314, Spearman: 0.5955
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6345
2025-11-14 18:16:32,608 - INFO - Fold 2 Train Epoch 18/200, Batch 70, Loss: 7.5940, Pearson: 0.6340, Spearman: 0.6007
2025-11-14 18:16:42,774 - INFO - Fold 2 Train Epoch 18/200, Batch 80, Loss: 7.4718, Pearson: 0.6311, Spearman: 0.5991
2025-11-14 18:16:47,314 - INFO - Fold 2 Train Epoch 18/200, Train Loss: 7.2859, Pearson Mean: 0.6348, Spearman Mean: 0.5992
2025-11-14 18:16:47,314 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4243, 'spearman_mean_genewise': 0.3828, 'l1_error_mean': 1.8983, 'l2_errors_mean': 7.286, 'r2_scores_mean': 0.1936, 'pearson_std': 0.1177, 'l2_error_q1': 4.9465, 'l2_error_q2': 6.9015, 'l2_error_q3': 9.3795, 'r2_score_q1': 0.1145, 'r2_score_q2': 0.1681, 'r2_score_q3': 0.2372, 'mape_mean': 58.973, 'mape_std': 18.4003, 'rmse_mean': 2.6528, 'rmse_std': 0.4986}
2025-11-14 18:16:47,644 - INFO - Fold 2 Val Epoch 18/200, Batch 0, Loss: 10.3617, Pearson: 0.4719, Spearman: 0.4785
2025-11-14 18:16:49,056 - INFO - Fold 2 Val Epoch 18/200, Batch 10, Loss: 9.5692, Pearson: 0.5491, Spearman: 0.5537
2025-11-14 18:16:50,314 - INFO - Fold 2 Val Epoch 18/200, Batch 20, Loss: 6.4240, Pearson: 0.5364, Spearman: 0.5340
2025-11-14 18:16:52,727 - INFO - Fold 2 Val Epoch 18/200, Val Loss: 8.2453, Pearson Mean: 0.5489, Spearman Mean: 0.5398
2025-11-14 18:16:52,727 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.316, 'spearman_mean_genewise': 0.2954, 'l1_error_mean': 2.0589, 'l2_errors_mean': 8.2688, 'r2_scores_mean': 0.1007, 'pearson_std': 0.1274, 'l2_error_q1': 5.2258, 'l2_error_q2': 8.0063, 'l2_error_q3': 11.2742, 'r2_score_q1': 0.0357, 'r2_score_q2': 0.0747, 'r2_score_q3': 0.1355, 'mape_mean': 63.9121, 'mape_std': 19.2561, 'rmse_mean': 2.8096, 'rmse_std': 0.6124}
2025-11-14 18:16:52,727 - INFO - Learning rate for epoch 18: 0.0001
2025-11-14 18:16:52,786 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_2/best_model.pth
2025-11-14 18:16:53,820 - INFO - Fold 2 Train Epoch 19/200, Batch 0, Loss: 7.2149, Pearson: 0.6407, Spearman: 0.6057
2025-11-14 18:17:03,992 - INFO - Fold 2 Train Epoch 19/200, Batch 10, Loss: 7.2155, Pearson: 0.6509, Spearman: 0.6124
2025-11-14 18:17:14,195 - INFO - Fold 2 Train Epoch 19/200, Batch 20, Loss: 7.4527, Pearson: 0.6402, Spearman: 0.6109
2025-11-14 18:17:24,369 - INFO - Fold 2 Train Epoch 19/200, Batch 30, Loss: 7.1844, Pearson: 0.6477, Spearman: 0.6043
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6380
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6280
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.286016941070557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 19 =====================
Sample y_true values (first sample, first 10 genes):
[0.        7.5025144 5.7135096 0.        7.5025144 6.405005  0.
 5.7135096 5.7135096 9.206811 ]
Sample y_pred values (first sample, first 10 genes):
[2.152335  6.513054  4.9422774 4.2091494 7.996423  6.7578015 2.3471503
 5.6643953 5.736161  8.864481 ]
y_true  -> mean=2.1549, std=3.4976, min=0.0000, max=12.6049
y_pred  -> mean=2.1097, std=2.2149, min=0.0000, max=13.6028
Batch 0 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6477
2025-11-14 18:17:34,564 - INFO - Fold 2 Train Epoch 19/200, Batch 40, Loss: 7.3148, Pearson: 0.6434, Spearman: 0.6063
2025-11-14 18:17:44,708 - INFO - Fold 2 Train Epoch 19/200, Batch 50, Loss: 7.2552, Pearson: 0.6403, Spearman: 0.6043
2025-11-14 18:17:54,889 - INFO - Fold 2 Train Epoch 19/200, Batch 60, Loss: 7.4179, Pearson: 0.6369, Spearman: 0.5999
2025-11-14 18:18:05,053 - INFO - Fold 2 Train Epoch 19/200, Batch 70, Loss: 7.1876, Pearson: 0.6321, Spearman: 0.5995
2025-11-14 18:18:15,210 - INFO - Fold 2 Train Epoch 19/200, Batch 80, Loss: 7.0778, Pearson: 0.6424, Spearman: 0.5981
2025-11-14 18:18:19,700 - INFO - Fold 2 Train Epoch 19/200, Train Loss: 7.2114, Pearson Mean: 0.6395, Spearman Mean: 0.6030
2025-11-14 18:18:19,700 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4326, 'spearman_mean_genewise': 0.3896, 'l1_error_mean': 1.888, 'l2_errors_mean': 7.2114, 'r2_scores_mean': 0.2008, 'pearson_std': 0.1175, 'l2_error_q1': 4.9134, 'l2_error_q2': 6.8213, 'l2_error_q3': 9.2705, 'r2_score_q1': 0.1211, 'r2_score_q2': 0.1758, 'r2_score_q3': 0.2461, 'mape_mean': 58.4817, 'mape_std': 18.4074, 'rmse_mean': 2.6396, 'rmse_std': 0.4938}
2025-11-14 18:18:20,036 - INFO - Fold 2 Val Epoch 19/200, Batch 0, Loss: 10.5233, Pearson: 0.4736, Spearman: 0.4804
2025-11-14 18:18:21,483 - INFO - Fold 2 Val Epoch 19/200, Batch 10, Loss: 9.6157, Pearson: 0.5480, Spearman: 0.5530
2025-11-14 18:18:22,690 - INFO - Fold 2 Val Epoch 19/200, Batch 20, Loss: 6.4116, Pearson: 0.5387, Spearman: 0.5361
2025-11-14 18:18:25,090 - INFO - Fold 2 Val Epoch 19/200, Val Loss: 8.2672, Pearson Mean: 0.5493, Spearman Mean: 0.5402
2025-11-14 18:18:25,090 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3151, 'spearman_mean_genewise': 0.2941, 'l1_error_mean': 2.0589, 'l2_errors_mean': 8.2914, 'r2_scores_mean': 0.0979, 'pearson_std': 0.1277, 'l2_error_q1': 5.229, 'l2_error_q2': 7.9919, 'l2_error_q3': 11.2884, 'r2_score_q1': 0.034, 'r2_score_q2': 0.0723, 'r2_score_q3': 0.1312, 'mape_mean': 63.5608, 'mape_std': 19.4388, 'rmse_mean': 2.8136, 'rmse_std': 0.6123}
2025-11-14 18:18:25,090 - INFO - Learning rate for epoch 19: 0.0001
2025-11-14 18:18:25,090 - INFO - No improvement in spearman genewise. Patience: 1/30
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6418
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6298
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.211441516876221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 20 =====================
Sample y_true values (first sample, first 10 genes):
[0.        7.3720083 0.        0.        7.3720083 0.        0.
 0.        0.        8.980943 ]
Sample y_pred values (first sample, first 10 genes):
[0.25514823 0.64497375 2.3299255  0.         4.4532943  1.8667643
 0.38316846 1.6858678  4.260006   8.732676  ]
2025-11-14 18:18:26,024 - INFO - Fold 2 Train Epoch 20/200, Batch 0, Loss: 7.0849, Pearson: 0.6325, Spearman: 0.5991
2025-11-14 18:18:36,260 - INFO - Fold 2 Train Epoch 20/200, Batch 10, Loss: 7.3813, Pearson: 0.6497, Spearman: 0.6042
2025-11-14 18:18:46,398 - INFO - Fold 2 Train Epoch 20/200, Batch 20, Loss: 7.0623, Pearson: 0.6416, Spearman: 0.5968
2025-11-14 18:18:56,592 - INFO - Fold 2 Train Epoch 20/200, Batch 30, Loss: 7.4016, Pearson: 0.6358, Spearman: 0.5984
2025-11-14 18:19:06,752 - INFO - Fold 2 Train Epoch 20/200, Batch 40, Loss: 6.9421, Pearson: 0.6439, Spearman: 0.6052
2025-11-14 18:19:16,911 - INFO - Fold 2 Train Epoch 20/200, Batch 50, Loss: 7.1288, Pearson: 0.6389, Spearman: 0.6040
y_true  -> mean=1.9559, std=3.4312, min=0.0000, max=12.5937
y_pred  -> mean=2.0929, std=2.2272, min=0.0000, max=13.0226
Batch 0 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6452
2025-11-14 18:19:27,091 - INFO - Fold 2 Train Epoch 20/200, Batch 60, Loss: 7.3480, Pearson: 0.6388, Spearman: 0.6023
2025-11-14 18:19:37,268 - INFO - Fold 2 Train Epoch 20/200, Batch 70, Loss: 7.1063, Pearson: 0.6461, Spearman: 0.6043
2025-11-14 18:19:47,432 - INFO - Fold 2 Train Epoch 20/200, Batch 80, Loss: 6.8654, Pearson: 0.6432, Spearman: 0.6004
2025-11-14 18:19:51,880 - INFO - Fold 2 Train Epoch 20/200, Train Loss: 7.1705, Pearson Mean: 0.6422, Spearman Mean: 0.6057
2025-11-14 18:19:51,880 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4374, 'spearman_mean_genewise': 0.3937, 'l1_error_mean': 1.8796, 'l2_errors_mean': 7.1703, 'r2_scores_mean': 0.2048, 'pearson_std': 0.1172, 'l2_error_q1': 4.8969, 'l2_error_q2': 6.7916, 'l2_error_q3': 9.2295, 'r2_score_q1': 0.1247, 'r2_score_q2': 0.179, 'r2_score_q3': 0.2504, 'mape_mean': 58.3919, 'mape_std': 18.3928, 'rmse_mean': 2.6323, 'rmse_std': 0.4914}
2025-11-14 18:19:52,242 - INFO - Fold 2 Val Epoch 20/200, Batch 0, Loss: 10.1002, Pearson: 0.4773, Spearman: 0.4849
2025-11-14 18:19:53,759 - INFO - Fold 2 Val Epoch 20/200, Batch 10, Loss: 9.4175, Pearson: 0.5531, Spearman: 0.5580
2025-11-14 18:19:55,005 - INFO - Fold 2 Val Epoch 20/200, Batch 20, Loss: 6.4582, Pearson: 0.5333, Spearman: 0.5315
2025-11-14 18:19:57,433 - INFO - Fold 2 Val Epoch 20/200, Val Loss: 8.2258, Pearson Mean: 0.5491, Spearman Mean: 0.5402
2025-11-14 18:19:57,434 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3182, 'spearman_mean_genewise': 0.2949, 'l1_error_mean': 2.0442, 'l2_errors_mean': 8.2483, 'r2_scores_mean': 0.1041, 'pearson_std': 0.126, 'l2_error_q1': 5.187, 'l2_error_q2': 7.9544, 'l2_error_q3': 11.278, 'r2_score_q1': 0.0393, 'r2_score_q2': 0.0784, 'r2_score_q3': 0.1347, 'mape_mean': 64.1489, 'mape_std': 19.1199, 'rmse_mean': 2.8054, 'rmse_std': 0.6149}
2025-11-14 18:19:57,434 - INFO - Learning rate for epoch 20: 0.0001
2025-11-14 18:19:57,434 - INFO - No improvement in spearman genewise. Patience: 2/30
2025-11-14 18:19:58,450 - INFO - Fold 2 Train Epoch 21/200, Batch 0, Loss: 7.1718, Pearson: 0.6438, Spearman: 0.6090
2025-11-14 18:20:08,622 - INFO - Fold 2 Train Epoch 21/200, Batch 10, Loss: 7.5099, Pearson: 0.6404, Spearman: 0.6038
2025-11-14 18:20:18,802 - INFO - Fold 2 Train Epoch 21/200, Batch 20, Loss: 6.8327, Pearson: 0.6451, Spearman: 0.6025
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6375
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6429
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.170349597930908
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 21 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       7.546942 0.       0.       0.
 0.       7.546942]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.30461782 0.14113337 2.4877355  0.670401
 0.3760097  0.7385579  3.2508988  7.3171086 ]
y_true  -> mean=2.1438, std=3.4992, min=0.0000, max=12.7270
y_pred  -> mean=2.0923, std=2.2485, min=0.0000, max=13.0502
Batch 0 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6388
2025-11-14 18:20:28,974 - INFO - Fold 2 Train Epoch 21/200, Batch 30, Loss: 7.0799, Pearson: 0.6506, Spearman: 0.6115
2025-11-14 18:20:39,138 - INFO - Fold 2 Train Epoch 21/200, Batch 40, Loss: 7.1048, Pearson: 0.6477, Spearman: 0.6118
2025-11-14 18:20:49,313 - INFO - Fold 2 Train Epoch 21/200, Batch 50, Loss: 7.0932, Pearson: 0.6490, Spearman: 0.6110
2025-11-14 18:20:59,497 - INFO - Fold 2 Train Epoch 21/200, Batch 60, Loss: 7.0862, Pearson: 0.6529, Spearman: 0.6117
2025-11-14 18:21:09,677 - INFO - Fold 2 Train Epoch 21/200, Batch 70, Loss: 7.0444, Pearson: 0.6414, Spearman: 0.6076
2025-11-14 18:21:19,861 - INFO - Fold 2 Train Epoch 21/200, Batch 80, Loss: 7.0569, Pearson: 0.6373, Spearman: 0.5982
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6404
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6443
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
2025-11-14 18:21:24,320 - INFO - Fold 2 Train Epoch 21/200, Train Loss: 7.1356, Pearson Mean: 0.6445, Spearman Mean: 0.6081
2025-11-14 18:21:24,320 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4415, 'spearman_mean_genewise': 0.3972, 'l1_error_mean': 1.8741, 'l2_errors_mean': 7.1355, 'r2_scores_mean': 0.2083, 'pearson_std': 0.1168, 'l2_error_q1': 4.8814, 'l2_error_q2': 6.7414, 'l2_error_q3': 9.1943, 'r2_score_q1': 0.1274, 'r2_score_q2': 0.1819, 'r2_score_q3': 0.2548, 'mape_mean': 58.1902, 'mape_std': 18.3808, 'rmse_mean': 2.626, 'rmse_std': 0.4898}
2025-11-14 18:21:24,676 - INFO - Fold 2 Val Epoch 21/200, Batch 0, Loss: 10.4411, Pearson: 0.4777, Spearman: 0.4842
2025-11-14 18:21:26,279 - INFO - Fold 2 Val Epoch 21/200, Batch 10, Loss: 9.5329, Pearson: 0.5533, Spearman: 0.5542
2025-11-14 18:21:27,504 - INFO - Fold 2 Val Epoch 21/200, Batch 20, Loss: 6.4920, Pearson: 0.5277, Spearman: 0.5266
2025-11-14 18:21:29,901 - INFO - Fold 2 Val Epoch 21/200, Val Loss: 8.2557, Pearson Mean: 0.5486, Spearman Mean: 0.5392
2025-11-14 18:21:29,901 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3163, 'spearman_mean_genewise': 0.2948, 'l1_error_mean': 2.0447, 'l2_errors_mean': 8.2777, 'r2_scores_mean': 0.1001, 'pearson_std': 0.1269, 'l2_error_q1': 5.2033, 'l2_error_q2': 7.9854, 'l2_error_q3': 11.2762, 'r2_score_q1': 0.0344, 'r2_score_q2': 0.074, 'r2_score_q3': 0.1353, 'mape_mean': 63.8985, 'mape_std': 19.2565, 'rmse_mean': 2.811, 'rmse_std': 0.613}
2025-11-14 18:21:29,901 - INFO - Learning rate for epoch 21: 0.0001
2025-11-14 18:21:29,901 - INFO - No improvement in spearman genewise. Patience: 3/30
2025-11-14 18:21:30,821 - INFO - Fold 2 Train Epoch 22/200, Batch 0, Loss: 7.3424, Pearson: 0.6456, Spearman: 0.6170
2025-11-14 18:21:41,020 - INFO - Fold 2 Train Epoch 22/200, Batch 10, Loss: 7.0769, Pearson: 0.6537, Spearman: 0.6116
2025-11-14 18:21:51,186 - INFO - Fold 2 Train Epoch 22/200, Batch 20, Loss: 7.1859, Pearson: 0.6507, Spearman: 0.6140
2025-11-14 18:22:01,368 - INFO - Fold 2 Train Epoch 22/200, Batch 30, Loss: 6.9254, Pearson: 0.6487, Spearman: 0.6080
2025-11-14 18:22:11,551 - INFO - Fold 2 Train Epoch 22/200, Batch 40, Loss: 7.1759, Pearson: 0.6446, Spearman: 0.6149
2025-11-14 18:22:21,716 - INFO - Fold 2 Train Epoch 22/200, Batch 50, Loss: 6.9488, Pearson: 0.6387, Spearman: 0.5991
========================= 7.1355414390563965
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 22 =====================
Sample y_true values (first sample, first 10 genes):
[0.        7.6074176 7.6074176 0.        0.        8.300316  0.
 0.        0.        7.6074176]
Sample y_pred values (first sample, first 10 genes):
[1.314513  1.9859507 3.4521987 1.0968528 3.6881757 4.462135  0.3539848
 1.965201  3.1112275 5.0820155]
y_true  -> mean=2.2485, std=3.5417, min=0.0000, max=12.3891
y_pred  -> mean=2.0991, std=2.2197, min=0.0000, max=13.8144
Batch 0 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 18:22:31,899 - INFO - Fold 2 Train Epoch 22/200, Batch 60, Loss: 7.1057, Pearson: 0.6505, Spearman: 0.6152
2025-11-14 18:22:42,070 - INFO - Fold 2 Train Epoch 22/200, Batch 70, Loss: 7.0281, Pearson: 0.6562, Spearman: 0.6144
2025-11-14 18:22:52,238 - INFO - Fold 2 Train Epoch 22/200, Batch 80, Loss: 6.9034, Pearson: 0.6541, Spearman: 0.6189
2025-11-14 18:22:56,692 - INFO - Fold 2 Train Epoch 22/200, Train Loss: 7.1019, Pearson Mean: 0.6466, Spearman Mean: 0.6105
2025-11-14 18:22:56,692 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4455, 'spearman_mean_genewise': 0.4007, 'l1_error_mean': 1.8694, 'l2_errors_mean': 7.102, 'r2_scores_mean': 0.2117, 'pearson_std': 0.1162, 'l2_error_q1': 4.8549, 'l2_error_q2': 6.7372, 'l2_error_q3': 9.1671, 'r2_score_q1': 0.13, 'r2_score_q2': 0.1856, 'r2_score_q3': 0.2585, 'mape_mean': 58.0262, 'mape_std': 18.3149, 'rmse_mean': 2.6199, 'rmse_std': 0.4879}
2025-11-14 18:22:57,019 - INFO - Fold 2 Val Epoch 22/200, Batch 0, Loss: 10.2849, Pearson: 0.4730, Spearman: 0.4798
2025-11-14 18:22:58,406 - INFO - Fold 2 Val Epoch 22/200, Batch 10, Loss: 9.5716, Pearson: 0.5485, Spearman: 0.5517
2025-11-14 18:22:59,618 - INFO - Fold 2 Val Epoch 22/200, Batch 20, Loss: 6.4192, Pearson: 0.5383, Spearman: 0.5355
2025-11-14 18:23:02,018 - INFO - Fold 2 Val Epoch 22/200, Val Loss: 8.2468, Pearson Mean: 0.5488, Spearman Mean: 0.5393
2025-11-14 18:23:02,018 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3158, 'spearman_mean_genewise': 0.2945, 'l1_error_mean': 2.0477, 'l2_errors_mean': 8.2703, 'r2_scores_mean': 0.1007, 'pearson_std': 0.1274, 'l2_error_q1': 5.1909, 'l2_error_q2': 7.9682, 'l2_error_q3': 11.2904, 'r2_score_q1': 0.0352, 'r2_score_q2': 0.074, 'r2_score_q3': 0.1327, 'mape_mean': 64.1391, 'mape_std': 19.1952, 'rmse_mean': 2.8097, 'rmse_std': 0.6133}
2025-11-14 18:23:02,018 - INFO - Learning rate for epoch 22: 0.0001
2025-11-14 18:23:02,018 - INFO - No improvement in spearman genewise. Patience: 4/30
2025-11-14 18:23:03,067 - INFO - Fold 2 Train Epoch 23/200, Batch 0, Loss: 6.7312, Pearson: 0.6621, Spearman: 0.6208
2025-11-14 18:23:13,243 - INFO - Fold 2 Train Epoch 23/200, Batch 10, Loss: 7.0337, Pearson: 0.6384, Spearman: 0.6075
2025-11-14 18:23:23,417 - INFO - Fold 2 Train Epoch 23/200, Batch 20, Loss: 7.2024, Pearson: 0.6484, Spearman: 0.6119
Batch 54 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6374
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6398
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.101955413818359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 23 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       7.875719 7.875719 0.       0.       7.875719
 0.       9.261728]
Sample y_pred values (first sample, first 10 genes):
[0.08387995 3.8807578  2.4242265  1.018777   4.0673757  2.2947912
 0.44629747 1.2723023  2.890852   9.502936  ]
y_true  -> mean=2.0879, std=3.4618, min=0.0000, max=12.5696
y_pred  -> mean=2.1100, std=2.2847, min=0.0000, max=13.2407
Batch 0 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 18:23:33,595 - INFO - Fold 2 Train Epoch 23/200, Batch 30, Loss: 7.4809, Pearson: 0.6420, Spearman: 0.6157
2025-11-14 18:23:43,787 - INFO - Fold 2 Train Epoch 23/200, Batch 40, Loss: 7.0099, Pearson: 0.6545, Spearman: 0.6142
2025-11-14 18:23:53,994 - INFO - Fold 2 Train Epoch 23/200, Batch 50, Loss: 7.1723, Pearson: 0.6513, Spearman: 0.6145
2025-11-14 18:24:04,163 - INFO - Fold 2 Train Epoch 23/200, Batch 60, Loss: 7.1509, Pearson: 0.6437, Spearman: 0.6053
2025-11-14 18:24:14,334 - INFO - Fold 2 Train Epoch 23/200, Batch 70, Loss: 7.2150, Pearson: 0.6454, Spearman: 0.6114
2025-11-14 18:24:24,516 - INFO - Fold 2 Train Epoch 23/200, Batch 80, Loss: 7.1266, Pearson: 0.6530, Spearman: 0.6145
Batch 23 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6432
2025-11-14 18:24:29,002 - INFO - Fold 2 Train Epoch 23/200, Train Loss: 7.0679, Pearson Mean: 0.6488, Spearman Mean: 0.6129
2025-11-14 18:24:29,002 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4495, 'spearman_mean_genewise': 0.4043, 'l1_error_mean': 1.8632, 'l2_errors_mean': 7.0678, 'r2_scores_mean': 0.2152, 'pearson_std': 0.1158, 'l2_error_q1': 4.8447, 'l2_error_q2': 6.7076, 'l2_error_q3': 9.0936, 'r2_score_q1': 0.134, 'r2_score_q2': 0.1889, 'r2_score_q3': 0.2618, 'mape_mean': 57.8788, 'mape_std': 18.3353, 'rmse_mean': 2.6137, 'rmse_std': 0.4862}
2025-11-14 18:24:29,363 - INFO - Fold 2 Val Epoch 23/200, Batch 0, Loss: 10.5902, Pearson: 0.4663, Spearman: 0.4727
2025-11-14 18:24:30,886 - INFO - Fold 2 Val Epoch 23/200, Batch 10, Loss: 9.5591, Pearson: 0.5518, Spearman: 0.5524
2025-11-14 18:24:32,105 - INFO - Fold 2 Val Epoch 23/200, Batch 20, Loss: 6.4695, Pearson: 0.5315, Spearman: 0.5289
2025-11-14 18:24:34,511 - INFO - Fold 2 Val Epoch 23/200, Val Loss: 8.3010, Pearson Mean: 0.5456, Spearman Mean: 0.5357
2025-11-14 18:24:34,512 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3117, 'spearman_mean_genewise': 0.2908, 'l1_error_mean': 2.0486, 'l2_errors_mean': 8.3239, 'r2_scores_mean': 0.0943, 'pearson_std': 0.1277, 'l2_error_q1': 5.2333, 'l2_error_q2': 8.0654, 'l2_error_q3': 11.3203, 'r2_score_q1': 0.0292, 'r2_score_q2': 0.0686, 'r2_score_q3': 0.1294, 'mape_mean': 64.2654, 'mape_std': 19.0922, 'rmse_mean': 2.8192, 'rmse_std': 0.613}
2025-11-14 18:24:34,512 - INFO - Learning rate for epoch 23: 0.0001
2025-11-14 18:24:34,512 - INFO - No improvement in spearman genewise. Patience: 5/30
2025-11-14 18:24:35,411 - INFO - Fold 2 Train Epoch 24/200, Batch 0, Loss: 7.0647, Pearson: 0.6454, Spearman: 0.6105
2025-11-14 18:24:45,630 - INFO - Fold 2 Train Epoch 24/200, Batch 10, Loss: 6.9152, Pearson: 0.6521, Spearman: 0.6164
2025-11-14 18:24:55,844 - INFO - Fold 2 Train Epoch 24/200, Batch 20, Loss: 6.9949, Pearson: 0.6475, Spearman: 0.6126
2025-11-14 18:25:05,985 - INFO - Fold 2 Train Epoch 24/200, Batch 30, Loss: 7.0438, Pearson: 0.6557, Spearman: 0.6163
2025-11-14 18:25:16,171 - INFO - Fold 2 Train Epoch 24/200, Batch 40, Loss: 6.8943, Pearson: 0.6503, Spearman: 0.6120
2025-11-14 18:25:26,344 - INFO - Fold 2 Train Epoch 24/200, Batch 50, Loss: 7.1163, Pearson: 0.6480, Spearman: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6407
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6528
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.067816257476807
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 24 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 0.        7.5660534 9.1750765]
Sample y_pred values (first sample, first 10 genes):
[0.54013354 2.219121   1.2507046  0.6705364  3.475802   2.4263616
 0.2807355  1.5278753  2.7284162  8.191455  ]
y_true  -> mean=2.0669, std=3.4792, min=0.0000, max=12.6253
y_pred  -> mean=2.1133, std=2.2489, min=0.0000, max=14.1551
Batch 0 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6480
2025-11-14 18:25:36,496 - INFO - Fold 2 Train Epoch 24/200, Batch 60, Loss: 6.6765, Pearson: 0.6582, Spearman: 0.6170
2025-11-14 18:25:46,695 - INFO - Fold 2 Train Epoch 24/200, Batch 70, Loss: 7.1929, Pearson: 0.6444, Spearman: 0.6090
2025-11-14 18:25:56,859 - INFO - Fold 2 Train Epoch 24/200, Batch 80, Loss: 7.1093, Pearson: 0.6530, Spearman: 0.6195
2025-11-14 18:26:01,401 - INFO - Fold 2 Train Epoch 24/200, Train Loss: 7.0325, Pearson Mean: 0.6510, Spearman Mean: 0.6154
2025-11-14 18:26:01,401 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4538, 'spearman_mean_genewise': 0.408, 'l1_error_mean': 1.8557, 'l2_errors_mean': 7.0323, 'r2_scores_mean': 0.2188, 'pearson_std': 0.115, 'l2_error_q1': 4.828, 'l2_error_q2': 6.6736, 'l2_error_q3': 9.0545, 'r2_score_q1': 0.1376, 'r2_score_q2': 0.1918, 'r2_score_q3': 0.2671, 'mape_mean': 57.7934, 'mape_std': 18.3047, 'rmse_mean': 2.6072, 'rmse_std': 0.4843}
2025-11-14 18:26:01,735 - INFO - Fold 2 Val Epoch 24/200, Batch 0, Loss: 10.3388, Pearson: 0.4722, Spearman: 0.4789
2025-11-14 18:26:03,123 - INFO - Fold 2 Val Epoch 24/200, Batch 10, Loss: 9.7056, Pearson: 0.5501, Spearman: 0.5524
2025-11-14 18:26:04,346 - INFO - Fold 2 Val Epoch 24/200, Batch 20, Loss: 6.4618, Pearson: 0.5317, Spearman: 0.5292
2025-11-14 18:26:06,750 - INFO - Fold 2 Val Epoch 24/200, Val Loss: 8.2718, Pearson Mean: 0.5472, Spearman Mean: 0.5372
2025-11-14 18:26:06,751 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3139, 'spearman_mean_genewise': 0.2929, 'l1_error_mean': 2.0374, 'l2_errors_mean': 8.2946, 'r2_scores_mean': 0.098, 'pearson_std': 0.1279, 'l2_error_q1': 5.2116, 'l2_error_q2': 7.9851, 'l2_error_q3': 11.2946, 'r2_score_q1': 0.0334, 'r2_score_q2': 0.0718, 'r2_score_q3': 0.1299, 'mape_mean': 64.4873, 'mape_std': 18.9937, 'rmse_mean': 2.8139, 'rmse_std': 0.6138}
2025-11-14 18:26:06,751 - INFO - Learning rate for epoch 24: 0.0001
2025-11-14 18:26:06,751 - INFO - No improvement in spearman genewise. Patience: 6/30
2025-11-14 18:26:07,738 - INFO - Fold 2 Train Epoch 25/200, Batch 0, Loss: 6.8689, Pearson: 0.6419, Spearman: 0.6026
2025-11-14 18:26:17,933 - INFO - Fold 2 Train Epoch 25/200, Batch 10, Loss: 6.9062, Pearson: 0.6522, Spearman: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6522
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6546
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.032285213470459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 25 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       7.932547 7.932547 0.       0.
 0.       7.932547]
Sample y_pred values (first sample, first 10 genes):
[0.02915609 0.21149251 1.1163195  0.735925   2.041819   1.2903445
 0.5964664  0.88360703 1.91274    7.5474715 ]
y_true  -> mean=1.9102, std=3.4065, min=0.0000, max=12.7954
y_pred  -> mean=2.1129, std=2.2565, min=0.0000, max=13.2956
Batch 0 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6550
2025-11-14 18:26:28,104 - INFO - Fold 2 Train Epoch 25/200, Batch 20, Loss: 6.9224, Pearson: 0.6435, Spearman: 0.6066
2025-11-14 18:26:38,309 - INFO - Fold 2 Train Epoch 25/200, Batch 30, Loss: 6.9714, Pearson: 0.6566, Spearman: 0.6191
2025-11-14 18:26:48,453 - INFO - Fold 2 Train Epoch 25/200, Batch 40, Loss: 7.1573, Pearson: 0.6509, Spearman: 0.6186
2025-11-14 18:26:58,631 - INFO - Fold 2 Train Epoch 25/200, Batch 50, Loss: 7.2734, Pearson: 0.6486, Spearman: 0.6128
2025-11-14 18:27:08,814 - INFO - Fold 2 Train Epoch 25/200, Batch 60, Loss: 7.0131, Pearson: 0.6604, Spearman: 0.6210
2025-11-14 18:27:19,014 - INFO - Fold 2 Train Epoch 25/200, Batch 70, Loss: 7.0541, Pearson: 0.6555, Spearman: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6694
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6413
2025-11-14 18:27:29,190 - INFO - Fold 2 Train Epoch 25/200, Batch 80, Loss: 6.8909, Pearson: 0.6566, Spearman: 0.6209
2025-11-14 18:27:33,723 - INFO - Fold 2 Train Epoch 25/200, Train Loss: 7.0043, Pearson Mean: 0.6528, Spearman Mean: 0.6176
2025-11-14 18:27:33,723 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4571, 'spearman_mean_genewise': 0.4108, 'l1_error_mean': 1.8549, 'l2_errors_mean': 7.0044, 'r2_scores_mean': 0.2216, 'pearson_std': 0.1145, 'l2_error_q1': 4.8035, 'l2_error_q2': 6.6291, 'l2_error_q3': 9.0045, 'r2_score_q1': 0.141, 'r2_score_q2': 0.1946, 'r2_score_q3': 0.2691, 'mape_mean': 57.5532, 'mape_std': 18.3059, 'rmse_mean': 2.6022, 'rmse_std': 0.4825}
2025-11-14 18:27:34,040 - INFO - Fold 2 Val Epoch 25/200, Batch 0, Loss: 10.2676, Pearson: 0.4761, Spearman: 0.4831
2025-11-14 18:27:35,407 - INFO - Fold 2 Val Epoch 25/200, Batch 10, Loss: 9.6320, Pearson: 0.5504, Spearman: 0.5507
2025-11-14 18:27:36,630 - INFO - Fold 2 Val Epoch 25/200, Batch 20, Loss: 6.4562, Pearson: 0.5352, Spearman: 0.5311
2025-11-14 18:27:39,048 - INFO - Fold 2 Val Epoch 25/200, Val Loss: 8.2648, Pearson Mean: 0.5477, Spearman Mean: 0.5380
2025-11-14 18:27:39,049 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3113, 'spearman_mean_genewise': 0.2899, 'l1_error_mean': 2.032, 'l2_errors_mean': 8.2867, 'r2_scores_mean': 0.0994, 'pearson_std': 0.1284, 'l2_error_q1': 5.2125, 'l2_error_q2': 7.9846, 'l2_error_q3': 11.347, 'r2_score_q1': 0.0347, 'r2_score_q2': 0.0723, 'r2_score_q3': 0.1297, 'mape_mean': 65.0242, 'mape_std': 19.5, 'rmse_mean': 2.8123, 'rmse_std': 0.6146}
2025-11-14 18:27:39,049 - INFO - Learning rate for epoch 25: 0.0001
2025-11-14 18:27:39,049 - INFO - No improvement in spearman genewise. Patience: 7/30
2025-11-14 18:27:40,076 - INFO - Fold 2 Train Epoch 26/200, Batch 0, Loss: 6.7441, Pearson: 0.6639, Spearman: 0.6315
2025-11-14 18:27:50,251 - INFO - Fold 2 Train Epoch 26/200, Batch 10, Loss: 6.7730, Pearson: 0.6567, Spearman: 0.6145
2025-11-14 18:28:00,426 - INFO - Fold 2 Train Epoch 26/200, Batch 20, Loss: 7.0493, Pearson: 0.6569, Spearman: 0.6188
2025-11-14 18:28:10,594 - INFO - Fold 2 Train Epoch 26/200, Batch 30, Loss: 7.0501, Pearson: 0.6612, Spearman: 0.6273
2025-11-14 18:28:20,774 - INFO - Fold 2 Train Epoch 26/200, Batch 40, Loss: 6.9328, Pearson: 0.6623, Spearman: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6526
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6696
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.004390716552734
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 26 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       7.854893
 0.       9.240896]
Sample y_pred values (first sample, first 10 genes):
[0.29689938 3.5410604  2.2348638  0.41822344 3.42705    2.0995674
 0.2605804  1.9805574  2.8838737  8.341366  ]
y_true  -> mean=2.1084, std=3.4715, min=0.0000, max=12.5574
y_pred  -> mean=2.1051, std=2.2338, min=0.0000, max=12.9145
Batch 0 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6462
2025-11-14 18:28:30,927 - INFO - Fold 2 Train Epoch 26/200, Batch 50, Loss: 7.0585, Pearson: 0.6565, Spearman: 0.6151
2025-11-14 18:28:41,098 - INFO - Fold 2 Train Epoch 26/200, Batch 60, Loss: 6.9281, Pearson: 0.6624, Spearman: 0.6283
2025-11-14 18:28:51,267 - INFO - Fold 2 Train Epoch 26/200, Batch 70, Loss: 6.7680, Pearson: 0.6644, Spearman: 0.6268
2025-11-14 18:29:01,457 - INFO - Fold 2 Train Epoch 26/200, Batch 80, Loss: 6.9136, Pearson: 0.6507, Spearman: 0.6185
2025-11-14 18:29:05,888 - INFO - Fold 2 Train Epoch 26/200, Train Loss: 6.9617, Pearson Mean: 0.6554, Spearman Mean: 0.6204
2025-11-14 18:29:05,888 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4621, 'spearman_mean_genewise': 0.4151, 'l1_error_mean': 1.8484, 'l2_errors_mean': 6.9618, 'r2_scores_mean': 0.226, 'pearson_std': 0.1138, 'l2_error_q1': 4.7817, 'l2_error_q2': 6.5997, 'l2_error_q3': 8.9491, 'r2_score_q1': 0.1445, 'r2_score_q2': 0.1997, 'r2_score_q3': 0.2746, 'mape_mean': 57.3125, 'mape_std': 18.2206, 'rmse_mean': 2.5944, 'rmse_std': 0.4804}
2025-11-14 18:29:06,245 - INFO - Fold 2 Val Epoch 26/200, Batch 0, Loss: 10.3606, Pearson: 0.4731, Spearman: 0.4796
2025-11-14 18:29:07,687 - INFO - Fold 2 Val Epoch 26/200, Batch 10, Loss: 9.7883, Pearson: 0.5456, Spearman: 0.5461
2025-11-14 18:29:08,911 - INFO - Fold 2 Val Epoch 26/200, Batch 20, Loss: 6.5247, Pearson: 0.5261, Spearman: 0.5224
2025-11-14 18:29:11,344 - INFO - Fold 2 Val Epoch 26/200, Val Loss: 8.3136, Pearson Mean: 0.5442, Spearman Mean: 0.5344
2025-11-14 18:29:11,344 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3074, 'spearman_mean_genewise': 0.2878, 'l1_error_mean': 2.0589, 'l2_errors_mean': 8.3356, 'r2_scores_mean': 0.0941, 'pearson_std': 0.1283, 'l2_error_q1': 5.2452, 'l2_error_q2': 8.0391, 'l2_error_q3': 11.3311, 'r2_score_q1': 0.031, 'r2_score_q2': 0.0678, 'r2_score_q3': 0.1268, 'mape_mean': 64.6928, 'mape_std': 19.2219, 'rmse_mean': 2.8206, 'rmse_std': 0.6162}
2025-11-14 18:29:11,344 - INFO - Learning rate for epoch 26: 1e-05
2025-11-14 18:29:11,344 - INFO - No improvement in spearman genewise. Patience: 8/30
2025-11-14 18:29:12,315 - INFO - Fold 2 Train Epoch 27/200, Batch 0, Loss: 6.8160, Pearson: 0.6546, Spearman: 0.6251
2025-11-14 18:29:22,301 - INFO - Fold 2 Train Epoch 27/200, Batch 10, Loss: 6.7798, Pearson: 0.6630, Spearman: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6679
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6507
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6637
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.961839199066162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 27 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       8.753074 7.654777 0.       8.347688
 8.347688 8.347688]
Sample y_pred values (first sample, first 10 genes):
[0.91383606 0.23502803 1.5446136  1.8851881  2.7069893  1.9297507
 0.8371237  3.1312642  3.1520028  6.5617514 ]
y_true  -> mean=2.0149, std=3.4518, min=0.0000, max=13.1224
y_pred  -> mean=2.0961, std=2.2353, min=0.0000, max=13.3934
Batch 0 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6705
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6474
2025-11-14 18:29:32,460 - INFO - Fold 2 Train Epoch 27/200, Batch 20, Loss: 6.9388, Pearson: 0.6581, Spearman: 0.6199
2025-11-14 18:29:42,654 - INFO - Fold 2 Train Epoch 27/200, Batch 30, Loss: 6.9295, Pearson: 0.6574, Spearman: 0.6245
2025-11-14 18:29:52,869 - INFO - Fold 2 Train Epoch 27/200, Batch 40, Loss: 6.9338, Pearson: 0.6639, Spearman: 0.6244
2025-11-14 18:30:03,048 - INFO - Fold 2 Train Epoch 27/200, Batch 50, Loss: 6.6985, Pearson: 0.6547, Spearman: 0.6214
2025-11-14 18:30:13,190 - INFO - Fold 2 Train Epoch 27/200, Batch 60, Loss: 6.9582, Pearson: 0.6709, Spearman: 0.6339
2025-11-14 18:30:23,393 - INFO - Fold 2 Train Epoch 27/200, Batch 70, Loss: 7.0559, Pearson: 0.6607, Spearman: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6704
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6692
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6705
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6709
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6720
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6710
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6555
2025-11-14 18:30:33,553 - INFO - Fold 2 Train Epoch 27/200, Batch 80, Loss: 6.9329, Pearson: 0.6564, Spearman: 0.6251
2025-11-14 18:30:38,022 - INFO - Fold 2 Train Epoch 27/200, Train Loss: 6.9068, Pearson Mean: 0.6589, Spearman Mean: 0.6244
2025-11-14 18:30:38,023 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4691, 'spearman_mean_genewise': 0.421, 'l1_error_mean': 1.838, 'l2_errors_mean': 6.9067, 'r2_scores_mean': 0.2317, 'pearson_std': 0.1123, 'l2_error_q1': 4.7511, 'l2_error_q2': 6.5467, 'l2_error_q3': 8.8816, 'r2_score_q1': 0.1506, 'r2_score_q2': 0.2054, 'r2_score_q3': 0.2807, 'mape_mean': 57.198, 'mape_std': 18.2185, 'rmse_mean': 2.5843, 'rmse_std': 0.4777}
2025-11-14 18:30:38,362 - INFO - Fold 2 Val Epoch 27/200, Batch 0, Loss: 10.2300, Pearson: 0.4755, Spearman: 0.4819
2025-11-14 18:30:39,868 - INFO - Fold 2 Val Epoch 27/200, Batch 10, Loss: 9.5670, Pearson: 0.5524, Spearman: 0.5537
2025-11-14 18:30:41,095 - INFO - Fold 2 Val Epoch 27/200, Batch 20, Loss: 6.4933, Pearson: 0.5311, Spearman: 0.5276
2025-11-14 18:30:43,509 - INFO - Fold 2 Val Epoch 27/200, Val Loss: 8.2744, Pearson Mean: 0.5464, Spearman Mean: 0.5366
2025-11-14 18:30:43,510 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3108, 'spearman_mean_genewise': 0.29, 'l1_error_mean': 2.0555, 'l2_errors_mean': 8.2962, 'r2_scores_mean': 0.0984, 'pearson_std': 0.128, 'l2_error_q1': 5.2169, 'l2_error_q2': 7.9583, 'l2_error_q3': 11.326, 'r2_score_q1': 0.0341, 'r2_score_q2': 0.0719, 'r2_score_q3': 0.1301, 'mape_mean': 64.572, 'mape_std': 19.1911, 'rmse_mean': 2.8138, 'rmse_std': 0.6152}
2025-11-14 18:30:43,510 - INFO - Learning rate for epoch 27: 1e-05
2025-11-14 18:30:43,510 - INFO - No improvement in spearman genewise. Patience: 9/30
2025-11-14 18:30:46,394 - INFO - Fold 2 Train Epoch 28/200, Batch 0, Loss: 6.8950, Pearson: 0.6633, Spearman: 0.6239
2025-11-14 18:30:56,446 - INFO - Fold 2 Train Epoch 28/200, Batch 10, Loss: 6.8764, Pearson: 0.6572, Spearman: 0.6222
2025-11-14 18:31:06,628 - INFO - Fold 2 Train Epoch 28/200, Batch 20, Loss: 7.0584, Pearson: 0.6617, Spearman: 0.6269
2025-11-14 18:31:16,820 - INFO - Fold 2 Train Epoch 28/200, Batch 30, Loss: 7.0082, Pearson: 0.6578, Spearman: 0.6271
2025-11-14 18:31:27,028 - INFO - Fold 2 Train Epoch 28/200, Batch 40, Loss: 6.8868, Pearson: 0.6646, Spearman: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6498
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6580
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.906715393066406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 28 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.6527727  0.1163302  0.79999644 0.2982462
 0.         0.24766031 1.4646113  6.023094  ]
y_true  -> mean=2.1732, std=3.5066, min=0.0000, max=12.6201
y_pred  -> mean=2.1067, std=2.2579, min=0.0000, max=12.9006
Batch 0 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6661
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6731
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6670
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6670
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6561
2025-11-14 18:31:37,196 - INFO - Fold 2 Train Epoch 28/200, Batch 50, Loss: 6.9026, Pearson: 0.6622, Spearman: 0.6273
2025-11-14 18:31:47,362 - INFO - Fold 2 Train Epoch 28/200, Batch 60, Loss: 6.7586, Pearson: 0.6614, Spearman: 0.6231
2025-11-14 18:31:57,557 - INFO - Fold 2 Train Epoch 28/200, Batch 70, Loss: 6.5537, Pearson: 0.6707, Spearman: 0.6238
2025-11-14 18:32:07,050 - INFO - Fold 2 Train Epoch 28/200, Batch 80, Loss: 6.8016, Pearson: 0.6625, Spearman: 0.6307
2025-11-14 18:32:11,399 - INFO - Fold 2 Train Epoch 28/200, Train Loss: 6.8910, Pearson Mean: 0.6599, Spearman Mean: 0.6252
2025-11-14 18:32:11,399 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4708, 'spearman_mean_genewise': 0.4225, 'l1_error_mean': 1.8361, 'l2_errors_mean': 6.8912, 'r2_scores_mean': 0.2333, 'pearson_std': 0.1121, 'l2_error_q1': 4.7266, 'l2_error_q2': 6.5416, 'l2_error_q3': 8.8552, 'r2_score_q1': 0.1524, 'r2_score_q2': 0.2073, 'r2_score_q3': 0.282, 'mape_mean': 57.112, 'mape_std': 18.2076, 'rmse_mean': 2.5814, 'rmse_std': 0.4769}
2025-11-14 18:32:11,666 - INFO - Fold 2 Val Epoch 28/200, Batch 0, Loss: 10.2877, Pearson: 0.4735, Spearman: 0.4799
2025-11-14 18:32:12,876 - INFO - Fold 2 Val Epoch 28/200, Batch 10, Loss: 9.7027, Pearson: 0.5485, Spearman: 0.5491
2025-11-14 18:32:14,484 - INFO - Fold 2 Val Epoch 28/200, Batch 20, Loss: 6.4925, Pearson: 0.5302, Spearman: 0.5265
2025-11-14 18:32:16,935 - INFO - Fold 2 Val Epoch 28/200, Val Loss: 8.2887, Pearson Mean: 0.5457, Spearman Mean: 0.5357
2025-11-14 18:32:16,935 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3097, 'spearman_mean_genewise': 0.2891, 'l1_error_mean': 2.0508, 'l2_errors_mean': 8.3109, 'r2_scores_mean': 0.0966, 'pearson_std': 0.1284, 'l2_error_q1': 5.2223, 'l2_error_q2': 8.0045, 'l2_error_q3': 11.3372, 'r2_score_q1': 0.0329, 'r2_score_q2': 0.071, 'r2_score_q3': 0.1294, 'mape_mean': 64.7742, 'mape_std': 19.2295, 'rmse_mean': 2.8165, 'rmse_std': 0.6151}
2025-11-14 18:32:16,936 - INFO - Learning rate for epoch 28: 1e-05
2025-11-14 18:32:16,936 - INFO - No improvement in spearman genewise. Patience: 10/30
2025-11-14 18:32:17,919 - INFO - Fold 2 Train Epoch 29/200, Batch 0, Loss: 6.7002, Pearson: 0.6590, Spearman: 0.6250
2025-11-14 18:32:28,139 - INFO - Fold 2 Train Epoch 29/200, Batch 10, Loss: 6.5650, Pearson: 0.6696, Spearman: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6742
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6649
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6707
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6740
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6682
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6654
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.891181945800781
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 29 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.266691]
Sample y_pred values (first sample, first 10 genes):
[0.13332495 0.75724995 0.54669166 0.32682762 0.97775996 0.40433416
 0.29892042 0.5741413  1.7676842  7.4982605 ]
y_true  -> mean=1.9919, std=3.4388, min=0.0000, max=12.5082
y_pred  -> mean=2.0943, std=2.2525, min=0.0000, max=13.2175
Batch 0 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6680
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6696
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6570
2025-11-14 18:32:38,308 - INFO - Fold 2 Train Epoch 29/200, Batch 20, Loss: 7.0228, Pearson: 0.6595, Spearman: 0.6304
2025-11-14 18:32:48,492 - INFO - Fold 2 Train Epoch 29/200, Batch 30, Loss: 6.9152, Pearson: 0.6555, Spearman: 0.6273
2025-11-14 18:32:58,658 - INFO - Fold 2 Train Epoch 29/200, Batch 40, Loss: 6.8515, Pearson: 0.6536, Spearman: 0.6271
2025-11-14 18:33:08,826 - INFO - Fold 2 Train Epoch 29/200, Batch 50, Loss: 6.9898, Pearson: 0.6566, Spearman: 0.6236
2025-11-14 18:33:19,011 - INFO - Fold 2 Train Epoch 29/200, Batch 60, Loss: 7.0355, Pearson: 0.6581, Spearman: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6707
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6725
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6681
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6717
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6670
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6665
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6589
2025-11-14 18:33:29,204 - INFO - Fold 2 Train Epoch 29/200, Batch 70, Loss: 6.6993, Pearson: 0.6554, Spearman: 0.6233
2025-11-14 18:33:38,703 - INFO - Fold 2 Train Epoch 29/200, Batch 80, Loss: 6.8144, Pearson: 0.6576, Spearman: 0.6280
2025-11-14 18:33:43,072 - INFO - Fold 2 Train Epoch 29/200, Train Loss: 6.8856, Pearson Mean: 0.6603, Spearman Mean: 0.6257
2025-11-14 18:33:43,072 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4714, 'spearman_mean_genewise': 0.423, 'l1_error_mean': 1.8357, 'l2_errors_mean': 6.8858, 'r2_scores_mean': 0.2338, 'pearson_std': 0.1121, 'l2_error_q1': 4.7288, 'l2_error_q2': 6.5347, 'l2_error_q3': 8.861, 'r2_score_q1': 0.1526, 'r2_score_q2': 0.208, 'r2_score_q3': 0.2817, 'mape_mean': 57.084, 'mape_std': 18.1946, 'rmse_mean': 2.5804, 'rmse_std': 0.4766}
2025-11-14 18:33:43,346 - INFO - Fold 2 Val Epoch 29/200, Batch 0, Loss: 10.3352, Pearson: 0.4749, Spearman: 0.4813
2025-11-14 18:33:44,605 - INFO - Fold 2 Val Epoch 29/200, Batch 10, Loss: 9.5727, Pearson: 0.5505, Spearman: 0.5510
2025-11-14 18:33:46,282 - INFO - Fold 2 Val Epoch 29/200, Batch 20, Loss: 6.5363, Pearson: 0.5272, Spearman: 0.5237
2025-11-14 18:33:48,700 - INFO - Fold 2 Val Epoch 29/200, Val Loss: 8.2919, Pearson Mean: 0.5455, Spearman Mean: 0.5356
2025-11-14 18:33:48,700 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3103, 'spearman_mean_genewise': 0.2896, 'l1_error_mean': 2.0648, 'l2_errors_mean': 8.3132, 'r2_scores_mean': 0.0965, 'pearson_std': 0.1277, 'l2_error_q1': 5.2247, 'l2_error_q2': 7.9856, 'l2_error_q3': 11.3499, 'r2_score_q1': 0.033, 'r2_score_q2': 0.0713, 'r2_score_q3': 0.1281, 'mape_mean': 64.2352, 'mape_std': 19.2056, 'rmse_mean': 2.8168, 'rmse_std': 0.6156}
2025-11-14 18:33:48,700 - INFO - Learning rate for epoch 29: 1e-05
2025-11-14 18:33:48,700 - INFO - No improvement in spearman genewise. Patience: 11/30
2025-11-14 18:33:49,650 - INFO - Fold 2 Train Epoch 30/200, Batch 0, Loss: 6.9695, Pearson: 0.6568, Spearman: 0.6263
2025-11-14 18:33:59,857 - INFO - Fold 2 Train Epoch 30/200, Batch 10, Loss: 6.7702, Pearson: 0.6593, Spearman: 0.6250
2025-11-14 18:34:10,030 - INFO - Fold 2 Train Epoch 30/200, Batch 20, Loss: 6.9293, Pearson: 0.6532, Spearman: 0.6214
2025-11-14 18:34:20,231 - INFO - Fold 2 Train Epoch 30/200, Batch 30, Loss: 6.9021, Pearson: 0.6518, Spearman: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6710
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6674
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6570
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6637
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.885770320892334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 30 =====================
Sample y_true values (first sample, first 10 genes):
[0.       8.454432 0.       0.       0.       0.       0.       0.
 0.       8.454432]
Sample y_pred values (first sample, first 10 genes):
[0.03221062 0.         1.01632    0.         1.7648977  0.54069877
 0.         0.43945915 1.6002383  6.43443   ]
y_true  -> mean=2.1218, std=3.5004, min=0.0000, max=13.1224
y_pred  -> mean=2.1054, std=2.2555, min=0.0000, max=12.8525
Batch 0 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6696
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6640
2025-11-14 18:34:30,426 - INFO - Fold 2 Train Epoch 30/200, Batch 40, Loss: 6.7070, Pearson: 0.6664, Spearman: 0.6328
2025-11-14 18:34:40,585 - INFO - Fold 2 Train Epoch 30/200, Batch 50, Loss: 7.0765, Pearson: 0.6549, Spearman: 0.6243
2025-11-14 18:34:50,764 - INFO - Fold 2 Train Epoch 30/200, Batch 60, Loss: 6.6277, Pearson: 0.6706, Spearman: 0.6283
2025-11-14 18:35:00,947 - INFO - Fold 2 Train Epoch 30/200, Batch 70, Loss: 6.7797, Pearson: 0.6656, Spearman: 0.6302
2025-11-14 18:35:10,487 - INFO - Fold 2 Train Epoch 30/200, Batch 80, Loss: 6.9966, Pearson: 0.6564, Spearman: 0.6183
2025-11-14 18:35:14,846 - INFO - Fold 2 Train Epoch 30/200, Train Loss: 6.8795, Pearson Mean: 0.6606, Spearman Mean: 0.6260
2025-11-14 18:35:14,846 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4721, 'spearman_mean_genewise': 0.4236, 'l1_error_mean': 1.8349, 'l2_errors_mean': 6.8796, 'r2_scores_mean': 0.2345, 'pearson_std': 0.112, 'l2_error_q1': 4.7206, 'l2_error_q2': 6.5234, 'l2_error_q3': 8.8473, 'r2_score_q1': 0.153, 'r2_score_q2': 0.2091, 'r2_score_q3': 0.2831, 'mape_mean': 57.0588, 'mape_std': 18.1945, 'rmse_mean': 2.5793, 'rmse_std': 0.4761}
2025-11-14 18:35:15,129 - INFO - Fold 2 Val Epoch 30/200, Batch 0, Loss: 10.2523, Pearson: 0.4739, Spearman: 0.4804
2025-11-14 18:35:16,383 - INFO - Fold 2 Val Epoch 30/200, Batch 10, Loss: 9.6226, Pearson: 0.5499, Spearman: 0.5504
2025-11-14 18:35:18,025 - INFO - Fold 2 Val Epoch 30/200, Batch 20, Loss: 6.4893, Pearson: 0.5319, Spearman: 0.5281
2025-11-14 18:35:20,475 - INFO - Fold 2 Val Epoch 30/200, Val Loss: 8.2811, Pearson Mean: 0.5462, Spearman Mean: 0.5362
2025-11-14 18:35:20,476 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3102, 'spearman_mean_genewise': 0.2898, 'l1_error_mean': 2.0561, 'l2_errors_mean': 8.3038, 'r2_scores_mean': 0.0976, 'pearson_std': 0.128, 'l2_error_q1': 5.2195, 'l2_error_q2': 7.9867, 'l2_error_q3': 11.3276, 'r2_score_q1': 0.0338, 'r2_score_q2': 0.0712, 'r2_score_q3': 0.1293, 'mape_mean': 64.5626, 'mape_std': 19.2498, 'rmse_mean': 2.8151, 'rmse_std': 0.6155}
2025-11-14 18:35:20,476 - INFO - Learning rate for epoch 30: 1e-05
2025-11-14 18:35:20,476 - INFO - No improvement in spearman genewise. Patience: 12/30
2025-11-14 18:35:21,379 - INFO - Fold 2 Train Epoch 31/200, Batch 0, Loss: 6.9649, Pearson: 0.6594, Spearman: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6697
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6661
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6706
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6665
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6576
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6602
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.879611492156982
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 31 =====================
Sample y_true values (first sample, first 10 genes):
[0.       8.14592  0.       0.       0.       0.       0.       0.
 8.14592  9.755126]
Sample y_pred values (first sample, first 10 genes):
[0.75926125 2.0925117  0.78246313 0.42327625 2.0259352  0.93505466
 0.1453151  0.90142936 2.3343315  7.900546  ]
y_true  -> mean=2.1530, std=3.5077, min=0.0000, max=12.5404
y_pred  -> mean=2.0997, std=2.2223, min=0.0000, max=13.6065
Batch 0 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6645
2025-11-14 18:35:31,620 - INFO - Fold 2 Train Epoch 31/200, Batch 10, Loss: 6.7533, Pearson: 0.6627, Spearman: 0.6241
2025-11-14 18:35:41,772 - INFO - Fold 2 Train Epoch 31/200, Batch 20, Loss: 6.9501, Pearson: 0.6567, Spearman: 0.6255
2025-11-14 18:35:51,954 - INFO - Fold 2 Train Epoch 31/200, Batch 30, Loss: 6.9399, Pearson: 0.6555, Spearman: 0.6251
2025-11-14 18:36:02,162 - INFO - Fold 2 Train Epoch 31/200, Batch 40, Loss: 7.0974, Pearson: 0.6581, Spearman: 0.6248
2025-11-14 18:36:12,339 - INFO - Fold 2 Train Epoch 31/200, Batch 50, Loss: 7.1384, Pearson: 0.6640, Spearman: 0.6276
2025-11-14 18:36:22,540 - INFO - Fold 2 Train Epoch 31/200, Batch 60, Loss: 6.8508, Pearson: 0.6639, Spearman: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6713
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6693
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6695
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6678
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6643
2025-11-14 18:36:32,697 - INFO - Fold 2 Train Epoch 31/200, Batch 70, Loss: 6.7440, Pearson: 0.6700, Spearman: 0.6312
2025-11-14 18:36:42,194 - INFO - Fold 2 Train Epoch 31/200, Batch 80, Loss: 6.7341, Pearson: 0.6604, Spearman: 0.6273
2025-11-14 18:36:46,581 - INFO - Fold 2 Train Epoch 31/200, Train Loss: 6.8827, Pearson Mean: 0.6605, Spearman Mean: 0.6260
2025-11-14 18:36:46,581 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4719, 'spearman_mean_genewise': 0.4234, 'l1_error_mean': 1.837, 'l2_errors_mean': 6.8825, 'r2_scores_mean': 0.2342, 'pearson_std': 0.1118, 'l2_error_q1': 4.7291, 'l2_error_q2': 6.5321, 'l2_error_q3': 8.847, 'r2_score_q1': 0.1532, 'r2_score_q2': 0.2086, 'r2_score_q3': 0.2832, 'mape_mean': 57.0766, 'mape_std': 18.1932, 'rmse_mean': 2.5798, 'rmse_std': 0.4763}
2025-11-14 18:36:46,854 - INFO - Fold 2 Val Epoch 31/200, Batch 0, Loss: 10.3167, Pearson: 0.4733, Spearman: 0.4798
2025-11-14 18:36:48,100 - INFO - Fold 2 Val Epoch 31/200, Batch 10, Loss: 9.6294, Pearson: 0.5491, Spearman: 0.5502
2025-11-14 18:36:49,773 - INFO - Fold 2 Val Epoch 31/200, Batch 20, Loss: 6.5012, Pearson: 0.5300, Spearman: 0.5262
2025-11-14 18:36:52,200 - INFO - Fold 2 Val Epoch 31/200, Val Loss: 8.2922, Pearson Mean: 0.5455, Spearman Mean: 0.5355
2025-11-14 18:36:52,200 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3094, 'spearman_mean_genewise': 0.2888, 'l1_error_mean': 2.059, 'l2_errors_mean': 8.3146, 'r2_scores_mean': 0.0963, 'pearson_std': 0.1279, 'l2_error_q1': 5.2276, 'l2_error_q2': 7.9882, 'l2_error_q3': 11.3444, 'r2_score_q1': 0.0327, 'r2_score_q2': 0.0707, 'r2_score_q3': 0.1289, 'mape_mean': 64.5264, 'mape_std': 19.1848, 'rmse_mean': 2.817, 'rmse_std': 0.6155}
2025-11-14 18:36:52,200 - INFO - Learning rate for epoch 31: 1e-05
2025-11-14 18:36:52,200 - INFO - No improvement in spearman genewise. Patience: 13/30
2025-11-14 18:36:53,142 - INFO - Fold 2 Train Epoch 32/200, Batch 0, Loss: 6.8031, Pearson: 0.6648, Spearman: 0.6272
2025-11-14 18:37:03,353 - INFO - Fold 2 Train Epoch 32/200, Batch 10, Loss: 7.0249, Pearson: 0.6589, Spearman: 0.6289
2025-11-14 18:37:13,529 - INFO - Fold 2 Train Epoch 32/200, Batch 20, Loss: 6.8567, Pearson: 0.6648, Spearman: 0.6315
2025-11-14 18:37:23,724 - INFO - Fold 2 Train Epoch 32/200, Batch 30, Loss: 7.0302, Pearson: 0.6591, Spearman: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6715
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6621
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6577
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.8824968338012695
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 32 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        8.805025  0.
 0.        8.399635  7.7067127]
Sample y_pred values (first sample, first 10 genes):
[0.16462743 0.9288497  1.4595952  1.4035956  2.6611013  2.2094867
 0.10553968 2.7990844  2.4798398  5.133266  ]
y_true  -> mean=2.1331, std=3.4904, min=0.0000, max=12.5705
y_pred  -> mean=2.1027, std=2.2633, min=0.0000, max=12.9375
Batch 0 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6710
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6684
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6703
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6644
2025-11-14 18:37:33,910 - INFO - Fold 2 Train Epoch 32/200, Batch 40, Loss: 7.0569, Pearson: 0.6682, Spearman: 0.6299
2025-11-14 18:37:44,121 - INFO - Fold 2 Train Epoch 32/200, Batch 50, Loss: 6.8626, Pearson: 0.6583, Spearman: 0.6250
2025-11-14 18:37:54,277 - INFO - Fold 2 Train Epoch 32/200, Batch 60, Loss: 6.8130, Pearson: 0.6655, Spearman: 0.6269
2025-11-14 18:38:04,468 - INFO - Fold 2 Train Epoch 32/200, Batch 70, Loss: 6.7301, Pearson: 0.6798, Spearman: 0.6370
2025-11-14 18:38:13,974 - INFO - Fold 2 Train Epoch 32/200, Batch 80, Loss: 6.8219, Pearson: 0.6584, Spearman: 0.6298
2025-11-14 18:38:18,364 - INFO - Fold 2 Train Epoch 32/200, Train Loss: 6.8734, Pearson Mean: 0.6611, Spearman Mean: 0.6267
2025-11-14 18:38:18,364 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4729, 'spearman_mean_genewise': 0.4242, 'l1_error_mean': 1.8342, 'l2_errors_mean': 6.8732, 'r2_scores_mean': 0.2351, 'pearson_std': 0.1118, 'l2_error_q1': 4.7134, 'l2_error_q2': 6.5194, 'l2_error_q3': 8.8144, 'r2_score_q1': 0.1534, 'r2_score_q2': 0.2091, 'r2_score_q3': 0.2832, 'mape_mean': 57.0106, 'mape_std': 18.1764, 'rmse_mean': 2.5781, 'rmse_std': 0.4759}
2025-11-14 18:38:18,632 - INFO - Fold 2 Val Epoch 32/200, Batch 0, Loss: 10.2506, Pearson: 0.4752, Spearman: 0.4817
2025-11-14 18:38:19,843 - INFO - Fold 2 Val Epoch 32/200, Batch 10, Loss: 9.6507, Pearson: 0.5491, Spearman: 0.5498
2025-11-14 18:38:21,501 - INFO - Fold 2 Val Epoch 32/200, Batch 20, Loss: 6.5151, Pearson: 0.5289, Spearman: 0.5251
2025-11-14 18:38:23,938 - INFO - Fold 2 Val Epoch 32/200, Val Loss: 8.2842, Pearson Mean: 0.5458, Spearman Mean: 0.5358
2025-11-14 18:38:23,939 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3096, 'spearman_mean_genewise': 0.289, 'l1_error_mean': 2.0561, 'l2_errors_mean': 8.3062, 'r2_scores_mean': 0.0976, 'pearson_std': 0.1278, 'l2_error_q1': 5.2151, 'l2_error_q2': 7.9775, 'l2_error_q3': 11.3426, 'r2_score_q1': 0.0336, 'r2_score_q2': 0.0715, 'r2_score_q3': 0.1295, 'mape_mean': 64.7168, 'mape_std': 19.1921, 'rmse_mean': 2.8154, 'rmse_std': 0.6161}
2025-11-14 18:38:23,939 - INFO - Learning rate for epoch 32: 1.0000000000000002e-06
2025-11-14 18:38:23,939 - INFO - No improvement in spearman genewise. Patience: 14/30
2025-11-14 18:38:24,871 - INFO - Fold 2 Train Epoch 33/200, Batch 0, Loss: 6.5278, Pearson: 0.6704, Spearman: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6724
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6727
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6661
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6695
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6715
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6798
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6588
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6661
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.8732380867004395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 33 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.5718584 0.        7.5718584 0.        0.
 0.        0.        8.264749 ]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.36154962 0.39535564 3.020979   1.4613302
 0.47648507 0.5438221  2.9645758  6.6387014 ]
y_true  -> mean=2.0369, std=3.4417, min=0.0000, max=12.5750
y_pred  -> mean=2.1059, std=2.2706, min=0.0000, max=12.8193
Batch 0 Pearson correlation: 0.6704
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6613
2025-11-14 18:38:35,123 - INFO - Fold 2 Train Epoch 33/200, Batch 10, Loss: 7.1458, Pearson: 0.6573, Spearman: 0.6244
2025-11-14 18:38:45,308 - INFO - Fold 2 Train Epoch 33/200, Batch 20, Loss: 6.8659, Pearson: 0.6617, Spearman: 0.6201
2025-11-14 18:38:55,482 - INFO - Fold 2 Train Epoch 33/200, Batch 30, Loss: 6.8386, Pearson: 0.6591, Spearman: 0.6276
2025-11-14 18:39:05,680 - INFO - Fold 2 Train Epoch 33/200, Batch 40, Loss: 6.7116, Pearson: 0.6689, Spearman: 0.6316
2025-11-14 18:39:15,890 - INFO - Fold 2 Train Epoch 33/200, Batch 50, Loss: 6.8424, Pearson: 0.6560, Spearman: 0.6173
2025-11-14 18:39:26,059 - INFO - Fold 2 Train Epoch 33/200, Batch 60, Loss: 7.0785, Pearson: 0.6528, Spearman: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6702
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6671
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6663
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6690
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6684
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6730
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6691
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6706
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6663
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6665
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6689
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6715
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6674
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6733
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6703
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6661
2025-11-14 18:39:36,275 - INFO - Fold 2 Train Epoch 33/200, Batch 70, Loss: 6.8050, Pearson: 0.6607, Spearman: 0.6268
2025-11-14 18:39:45,798 - INFO - Fold 2 Train Epoch 33/200, Batch 80, Loss: 6.8881, Pearson: 0.6489, Spearman: 0.6216
2025-11-14 18:39:50,196 - INFO - Fold 2 Train Epoch 33/200, Train Loss: 6.8599, Pearson Mean: 0.6619, Spearman Mean: 0.6273
2025-11-14 18:39:50,196 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4745, 'spearman_mean_genewise': 0.4256, 'l1_error_mean': 1.8332, 'l2_errors_mean': 6.8601, 'r2_scores_mean': 0.2364, 'pearson_std': 0.1116, 'l2_error_q1': 4.714, 'l2_error_q2': 6.5094, 'l2_error_q3': 8.8164, 'r2_score_q1': 0.1549, 'r2_score_q2': 0.211, 'r2_score_q3': 0.2847, 'mape_mean': 56.9905, 'mape_std': 18.1983, 'rmse_mean': 2.5757, 'rmse_std': 0.4751}
2025-11-14 18:39:50,464 - INFO - Fold 2 Val Epoch 33/200, Batch 0, Loss: 10.4151, Pearson: 0.4752, Spearman: 0.4817
2025-11-14 18:39:51,694 - INFO - Fold 2 Val Epoch 33/200, Batch 10, Loss: 9.5515, Pearson: 0.5502, Spearman: 0.5508
2025-11-14 18:39:53,345 - INFO - Fold 2 Val Epoch 33/200, Batch 20, Loss: 6.5370, Pearson: 0.5281, Spearman: 0.5245
2025-11-14 18:39:55,762 - INFO - Fold 2 Val Epoch 33/200, Val Loss: 8.2999, Pearson Mean: 0.5458, Spearman Mean: 0.5358
2025-11-14 18:39:55,763 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3098, 'spearman_mean_genewise': 0.2892, 'l1_error_mean': 2.0707, 'l2_errors_mean': 8.3219, 'r2_scores_mean': 0.0956, 'pearson_std': 0.1277, 'l2_error_q1': 5.2275, 'l2_error_q2': 8.0052, 'l2_error_q3': 11.3519, 'r2_score_q1': 0.0318, 'r2_score_q2': 0.0698, 'r2_score_q3': 0.1282, 'mape_mean': 63.9164, 'mape_std': 19.3487, 'rmse_mean': 2.8183, 'rmse_std': 0.6157}
2025-11-14 18:39:55,763 - INFO - Learning rate for epoch 33: 1.0000000000000002e-06
2025-11-14 18:39:55,763 - INFO - No improvement in spearman genewise. Patience: 15/30
2025-11-14 18:39:56,691 - INFO - Fold 2 Train Epoch 34/200, Batch 0, Loss: 6.9931, Pearson: 0.6654, Spearman: 0.6307
2025-11-14 18:40:06,907 - INFO - Fold 2 Train Epoch 34/200, Batch 10, Loss: 6.8743, Pearson: 0.6630, Spearman: 0.6273
2025-11-14 18:40:17,108 - INFO - Fold 2 Train Epoch 34/200, Batch 20, Loss: 6.9827, Pearson: 0.6618, Spearman: 0.6282
2025-11-14 18:40:27,299 - INFO - Fold 2 Train Epoch 34/200, Batch 30, Loss: 6.8568, Pearson: 0.6540, Spearman: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6695
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6556
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6564
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.860069274902344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 34 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        6.5314617 0.
 0.        7.2238803 8.139733 ]
Sample y_pred values (first sample, first 10 genes):
[0.         0.37940297 1.2681332  1.0405848  3.92388    1.6913484
 0.7472092  1.4377618  4.9133563  8.104936  ]
y_true  -> mean=2.2463, std=3.5358, min=0.0000, max=12.6263
y_pred  -> mean=2.1078, std=2.2619, min=0.0000, max=13.5386
Batch 0 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6751
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6677
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6706
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6742
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6540
2025-11-14 18:40:37,507 - INFO - Fold 2 Train Epoch 34/200, Batch 40, Loss: 6.9301, Pearson: 0.6514, Spearman: 0.6243
2025-11-14 18:40:47,687 - INFO - Fold 2 Train Epoch 34/200, Batch 50, Loss: 7.0718, Pearson: 0.6534, Spearman: 0.6240
2025-11-14 18:40:57,866 - INFO - Fold 2 Train Epoch 34/200, Batch 60, Loss: 6.5860, Pearson: 0.6697, Spearman: 0.6339
2025-11-14 18:41:08,051 - INFO - Fold 2 Train Epoch 34/200, Batch 70, Loss: 6.6885, Pearson: 0.6700, Spearman: 0.6285
2025-11-14 18:41:17,592 - INFO - Fold 2 Train Epoch 34/200, Batch 80, Loss: 6.8415, Pearson: 0.6622, Spearman: 0.6226
2025-11-14 18:41:22,004 - INFO - Fold 2 Train Epoch 34/200, Train Loss: 6.8626, Pearson Mean: 0.6617, Spearman Mean: 0.6272
2025-11-14 18:41:22,004 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4742, 'spearman_mean_genewise': 0.4253, 'l1_error_mean': 1.8336, 'l2_errors_mean': 6.8628, 'r2_scores_mean': 0.2362, 'pearson_std': 0.1115, 'l2_error_q1': 4.7257, 'l2_error_q2': 6.5177, 'l2_error_q3': 8.8299, 'r2_score_q1': 0.155, 'r2_score_q2': 0.2101, 'r2_score_q3': 0.2843, 'mape_mean': 56.9834, 'mape_std': 18.1798, 'rmse_mean': 2.5762, 'rmse_std': 0.4754}
2025-11-14 18:41:22,272 - INFO - Fold 2 Val Epoch 34/200, Batch 0, Loss: 10.3261, Pearson: 0.4740, Spearman: 0.4803
2025-11-14 18:41:23,488 - INFO - Fold 2 Val Epoch 34/200, Batch 10, Loss: 9.6159, Pearson: 0.5500, Spearman: 0.5506
2025-11-14 18:41:25,187 - INFO - Fold 2 Val Epoch 34/200, Batch 20, Loss: 6.5277, Pearson: 0.5272, Spearman: 0.5236
2025-11-14 18:41:27,619 - INFO - Fold 2 Val Epoch 34/200, Val Loss: 8.2952, Pearson Mean: 0.5451, Spearman Mean: 0.5352
2025-11-14 18:41:27,620 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3096, 'spearman_mean_genewise': 0.289, 'l1_error_mean': 2.0584, 'l2_errors_mean': 8.3168, 'r2_scores_mean': 0.0963, 'pearson_std': 0.1279, 'l2_error_q1': 5.2275, 'l2_error_q2': 7.9973, 'l2_error_q3': 11.3447, 'r2_score_q1': 0.033, 'r2_score_q2': 0.0711, 'r2_score_q3': 0.1294, 'mape_mean': 64.5219, 'mape_std': 19.2125, 'rmse_mean': 2.8174, 'rmse_std': 0.6158}
2025-11-14 18:41:27,620 - INFO - Learning rate for epoch 34: 1.0000000000000002e-06
2025-11-14 18:41:27,620 - INFO - No improvement in spearman genewise. Patience: 16/30
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6665
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6677
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6753
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6683
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6697
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6697
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6686
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6683
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6670
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6649
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6736
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6699
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6594
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6705
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.862830638885498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 35 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 7.8471937 0.        8.540146 ]
Sample y_pred values (first sample, first 10 genes):
[0.50912637 0.58259755 1.5495962  1.0326102  2.1875415  1.7715062
 0.56441635 2.4355524  2.1425316  6.83338   ]
2025-11-14 18:41:28,550 - INFO - Fold 2 Train Epoch 35/200, Batch 0, Loss: 7.0784, Pearson: 0.6543, Spearman: 0.6246
2025-11-14 18:41:38,744 - INFO - Fold 2 Train Epoch 35/200, Batch 10, Loss: 6.9559, Pearson: 0.6698, Spearman: 0.6302
2025-11-14 18:41:48,946 - INFO - Fold 2 Train Epoch 35/200, Batch 20, Loss: 6.7507, Pearson: 0.6627, Spearman: 0.6316
2025-11-14 18:41:59,157 - INFO - Fold 2 Train Epoch 35/200, Batch 30, Loss: 6.8232, Pearson: 0.6528, Spearman: 0.6254
2025-11-14 18:42:09,340 - INFO - Fold 2 Train Epoch 35/200, Batch 40, Loss: 6.7134, Pearson: 0.6606, Spearman: 0.6255
2025-11-14 18:42:19,534 - INFO - Fold 2 Train Epoch 35/200, Batch 50, Loss: 6.7778, Pearson: 0.6598, Spearman: 0.6281
y_true  -> mean=2.1302, std=3.5169, min=0.0000, max=12.4785
y_pred  -> mean=2.1023, std=2.2391, min=0.0000, max=13.5344
Batch 0 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6677
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6720
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6684
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6729
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6706
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6688
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6703
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6681
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6665
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6708
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6661
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6648
2025-11-14 18:42:29,732 - INFO - Fold 2 Train Epoch 35/200, Batch 60, Loss: 7.1949, Pearson: 0.6634, Spearman: 0.6286
2025-11-14 18:42:39,917 - INFO - Fold 2 Train Epoch 35/200, Batch 70, Loss: 6.8031, Pearson: 0.6596, Spearman: 0.6251
2025-11-14 18:42:49,412 - INFO - Fold 2 Train Epoch 35/200, Batch 80, Loss: 6.9781, Pearson: 0.6598, Spearman: 0.6314
2025-11-14 18:42:53,815 - INFO - Fold 2 Train Epoch 35/200, Train Loss: 6.8678, Pearson Mean: 0.6615, Spearman Mean: 0.6271
2025-11-14 18:42:53,815 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4736, 'spearman_mean_genewise': 0.4248, 'l1_error_mean': 1.8343, 'l2_errors_mean': 6.8677, 'r2_scores_mean': 0.2357, 'pearson_std': 0.1115, 'l2_error_q1': 4.7235, 'l2_error_q2': 6.5183, 'l2_error_q3': 8.815, 'r2_score_q1': 0.1543, 'r2_score_q2': 0.2103, 'r2_score_q3': 0.2846, 'mape_mean': 57.0136, 'mape_std': 18.1857, 'rmse_mean': 2.5771, 'rmse_std': 0.4755}
2025-11-14 18:42:54,098 - INFO - Fold 2 Val Epoch 35/200, Batch 0, Loss: 10.3552, Pearson: 0.4736, Spearman: 0.4801
2025-11-14 18:42:55,354 - INFO - Fold 2 Val Epoch 35/200, Batch 10, Loss: 9.5797, Pearson: 0.5506, Spearman: 0.5512
2025-11-14 18:42:57,032 - INFO - Fold 2 Val Epoch 35/200, Batch 20, Loss: 6.5194, Pearson: 0.5287, Spearman: 0.5252
2025-11-14 18:42:59,479 - INFO - Fold 2 Val Epoch 35/200, Val Loss: 8.2930, Pearson Mean: 0.5456, Spearman Mean: 0.5357
2025-11-14 18:42:59,480 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3099, 'spearman_mean_genewise': 0.2892, 'l1_error_mean': 2.0595, 'l2_errors_mean': 8.3149, 'r2_scores_mean': 0.0963, 'pearson_std': 0.1279, 'l2_error_q1': 5.2291, 'l2_error_q2': 8.0076, 'l2_error_q3': 11.3465, 'r2_score_q1': 0.0321, 'r2_score_q2': 0.0714, 'r2_score_q3': 0.1293, 'mape_mean': 64.3756, 'mape_std': 19.195, 'rmse_mean': 2.8171, 'rmse_std': 0.6156}
2025-11-14 18:42:59,480 - INFO - Learning rate for epoch 35: 1.0000000000000002e-06
2025-11-14 18:42:59,480 - INFO - No improvement in spearman genewise. Patience: 17/30
2025-11-14 18:43:00,390 - INFO - Fold 2 Train Epoch 36/200, Batch 0, Loss: 6.8071, Pearson: 0.6622, Spearman: 0.6226
2025-11-14 18:43:10,643 - INFO - Fold 2 Train Epoch 36/200, Batch 10, Loss: 6.8072, Pearson: 0.6554, Spearman: 0.6253
2025-11-14 18:43:20,811 - INFO - Fold 2 Train Epoch 36/200, Batch 20, Loss: 6.9047, Pearson: 0.6670, Spearman: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6708
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6690
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6693
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6690
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6636
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6553
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.867694854736328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 36 =====================
Sample y_true values (first sample, first 10 genes):
[0.       8.388588 7.695668 0.       7.695668 0.       0.       0.
 0.       8.388588]
Sample y_pred values (first sample, first 10 genes):
[0.8327573  3.58942    4.0888953  0.6470858  5.7235775  3.8270211
 0.25242072 1.3889066  3.807301   6.6998887 ]
y_true  -> mean=2.0722, std=3.4815, min=0.0000, max=13.8155
y_pred  -> mean=2.1126, std=2.3002, min=0.0000, max=13.0952
Batch 0 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6758
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6684
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6670
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6688
2025-11-14 18:43:30,987 - INFO - Fold 2 Train Epoch 36/200, Batch 30, Loss: 6.9682, Pearson: 0.6547, Spearman: 0.6202
2025-11-14 18:43:41,173 - INFO - Fold 2 Train Epoch 36/200, Batch 40, Loss: 6.9996, Pearson: 0.6552, Spearman: 0.6199
2025-11-14 18:43:51,396 - INFO - Fold 2 Train Epoch 36/200, Batch 50, Loss: 6.8187, Pearson: 0.6652, Spearman: 0.6261
2025-11-14 18:44:01,527 - INFO - Fold 2 Train Epoch 36/200, Batch 60, Loss: 6.6689, Pearson: 0.6655, Spearman: 0.6305
2025-11-14 18:44:11,713 - INFO - Fold 2 Train Epoch 36/200, Batch 70, Loss: 6.8642, Pearson: 0.6668, Spearman: 0.6302
2025-11-14 18:44:21,234 - INFO - Fold 2 Train Epoch 36/200, Batch 80, Loss: 7.1510, Pearson: 0.6616, Spearman: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6720
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6690
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6695
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6711
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6680
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6696
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6691
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6678
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6652
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6633
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
2025-11-14 18:44:25,620 - INFO - Fold 2 Train Epoch 36/200, Train Loss: 6.8614, Pearson Mean: 0.6618, Spearman Mean: 0.6273
2025-11-14 18:44:25,620 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4744, 'spearman_mean_genewise': 0.4254, 'l1_error_mean': 1.8332, 'l2_errors_mean': 6.8614, 'r2_scores_mean': 0.2363, 'pearson_std': 0.1115, 'l2_error_q1': 4.7021, 'l2_error_q2': 6.5166, 'l2_error_q3': 8.8057, 'r2_score_q1': 0.1551, 'r2_score_q2': 0.211, 'r2_score_q3': 0.2849, 'mape_mean': 56.9846, 'mape_std': 18.1799, 'rmse_mean': 2.5759, 'rmse_std': 0.4753}
2025-11-14 18:44:25,889 - INFO - Fold 2 Val Epoch 36/200, Batch 0, Loss: 10.2594, Pearson: 0.4747, Spearman: 0.4812
2025-11-14 18:44:27,116 - INFO - Fold 2 Val Epoch 36/200, Batch 10, Loss: 9.6519, Pearson: 0.5493, Spearman: 0.5498
2025-11-14 18:44:28,794 - INFO - Fold 2 Val Epoch 36/200, Batch 20, Loss: 6.5135, Pearson: 0.5286, Spearman: 0.5250
2025-11-14 18:44:31,233 - INFO - Fold 2 Val Epoch 36/200, Val Loss: 8.2877, Pearson Mean: 0.5454, Spearman Mean: 0.5356
2025-11-14 18:44:31,233 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3093, 'spearman_mean_genewise': 0.2888, 'l1_error_mean': 2.0553, 'l2_errors_mean': 8.3094, 'r2_scores_mean': 0.0971, 'pearson_std': 0.1279, 'l2_error_q1': 5.2239, 'l2_error_q2': 7.9749, 'l2_error_q3': 11.3509, 'r2_score_q1': 0.0333, 'r2_score_q2': 0.0715, 'r2_score_q3': 0.1287, 'mape_mean': 64.741, 'mape_std': 19.1828, 'rmse_mean': 2.816, 'rmse_std': 0.6158}
2025-11-14 18:44:31,233 - INFO - Learning rate for epoch 36: 1.0000000000000002e-06
2025-11-14 18:44:31,234 - INFO - No improvement in spearman genewise. Patience: 18/30
2025-11-14 18:44:32,156 - INFO - Fold 2 Train Epoch 37/200, Batch 0, Loss: 6.9495, Pearson: 0.6622, Spearman: 0.6281
2025-11-14 18:44:42,413 - INFO - Fold 2 Train Epoch 37/200, Batch 10, Loss: 6.8829, Pearson: 0.6580, Spearman: 0.6211
2025-11-14 18:44:52,564 - INFO - Fold 2 Train Epoch 37/200, Batch 20, Loss: 6.8079, Pearson: 0.6620, Spearman: 0.6266
2025-11-14 18:45:02,713 - INFO - Fold 2 Train Epoch 37/200, Batch 30, Loss: 6.8255, Pearson: 0.6654, Spearman: 0.6321
2025-11-14 18:45:12,893 - INFO - Fold 2 Train Epoch 37/200, Batch 40, Loss: 7.0037, Pearson: 0.6571, Spearman: 0.6269
2025-11-14 18:45:23,087 - INFO - Fold 2 Train Epoch 37/200, Batch 50, Loss: 6.9716, Pearson: 0.6599, Spearman: 0.6260
========================= 6.861386299133301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 37 =====================
Sample y_true values (first sample, first 10 genes):
[0.        7.02574   0.        0.        7.430909  6.3334813 0.
 6.3334813 7.02574   8.816759 ]
Sample y_pred values (first sample, first 10 genes):
[1.255271  4.763032  3.3203247 2.6961064 7.6753664 5.471862  1.544507
 4.331382  5.360302  9.449201 ]
y_true  -> mean=2.1628, std=3.5160, min=0.0000, max=12.4773
y_pred  -> mean=2.1075, std=2.2617, min=0.0000, max=13.3588
Batch 0 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6683
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6782
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6671
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6739
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6661
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6710
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 18:45:33,254 - INFO - Fold 2 Train Epoch 37/200, Batch 60, Loss: 6.8804, Pearson: 0.6639, Spearman: 0.6288
2025-11-14 18:45:43,457 - INFO - Fold 2 Train Epoch 37/200, Batch 70, Loss: 6.9779, Pearson: 0.6540, Spearman: 0.6189
2025-11-14 18:45:52,954 - INFO - Fold 2 Train Epoch 37/200, Batch 80, Loss: 6.5386, Pearson: 0.6731, Spearman: 0.6311
2025-11-14 18:45:57,377 - INFO - Fold 2 Train Epoch 37/200, Train Loss: 6.8602, Pearson Mean: 0.6618, Spearman Mean: 0.6273
2025-11-14 18:45:57,377 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4744, 'spearman_mean_genewise': 0.4255, 'l1_error_mean': 1.8328, 'l2_errors_mean': 6.8602, 'r2_scores_mean': 0.2364, 'pearson_std': 0.1115, 'l2_error_q1': 4.7147, 'l2_error_q2': 6.512, 'l2_error_q3': 8.7951, 'r2_score_q1': 0.1549, 'r2_score_q2': 0.2103, 'r2_score_q3': 0.2843, 'mape_mean': 56.9584, 'mape_std': 18.18, 'rmse_mean': 2.5758, 'rmse_std': 0.475}
2025-11-14 18:45:57,651 - INFO - Fold 2 Val Epoch 37/200, Batch 0, Loss: 10.3393, Pearson: 0.4747, Spearman: 0.4812
2025-11-14 18:45:58,943 - INFO - Fold 2 Val Epoch 37/200, Batch 10, Loss: 9.6742, Pearson: 0.5480, Spearman: 0.5489
2025-11-14 18:46:00,673 - INFO - Fold 2 Val Epoch 37/200, Batch 20, Loss: 6.4852, Pearson: 0.5312, Spearman: 0.5276
2025-11-14 18:46:03,134 - INFO - Fold 2 Val Epoch 37/200, Val Loss: 8.2891, Pearson Mean: 0.5461, Spearman Mean: 0.5362
2025-11-14 18:46:03,134 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3097, 'spearman_mean_genewise': 0.2893, 'l1_error_mean': 2.0565, 'l2_errors_mean': 8.3117, 'r2_scores_mean': 0.0966, 'pearson_std': 0.128, 'l2_error_q1': 5.219, 'l2_error_q2': 7.9875, 'l2_error_q3': 11.3379, 'r2_score_q1': 0.0328, 'r2_score_q2': 0.0709, 'r2_score_q3': 0.1297, 'mape_mean': 64.4446, 'mape_std': 19.2379, 'rmse_mean': 2.8166, 'rmse_std': 0.6152}
2025-11-14 18:46:03,135 - INFO - Learning rate for epoch 37: 1.0000000000000002e-06
2025-11-14 18:46:03,135 - INFO - No improvement in spearman genewise. Patience: 19/30
2025-11-14 18:46:04,107 - INFO - Fold 2 Train Epoch 38/200, Batch 0, Loss: 6.7670, Pearson: 0.6580, Spearman: 0.6231
2025-11-14 18:46:14,168 - INFO - Fold 2 Train Epoch 38/200, Batch 10, Loss: 6.7826, Pearson: 0.6644, Spearman: 0.6264
2025-11-14 18:46:24,360 - INFO - Fold 2 Train Epoch 38/200, Batch 20, Loss: 7.0386, Pearson: 0.6560, Spearman: 0.6293
Batch 54 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6732
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6680
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6678
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6649
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6713
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6683
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6710
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6731
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6593
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6653
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.860213279724121
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 38 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.512406]
Sample y_pred values (first sample, first 10 genes):
[0.18765092 0.         0.55565447 0.09145367 0.52831686 0.44575912
 0.39227313 0.49326998 1.4957485  6.117244  ]
y_true  -> mean=2.0263, std=3.4527, min=0.0000, max=12.5162
y_pred  -> mean=2.1071, std=2.2780, min=0.0000, max=13.1369
Batch 0 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6697
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6760
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6677
2025-11-14 18:46:34,548 - INFO - Fold 2 Train Epoch 38/200, Batch 30, Loss: 6.7769, Pearson: 0.6637, Spearman: 0.6301
2025-11-14 18:46:44,713 - INFO - Fold 2 Train Epoch 38/200, Batch 40, Loss: 6.8459, Pearson: 0.6518, Spearman: 0.6188
2025-11-14 18:46:54,906 - INFO - Fold 2 Train Epoch 38/200, Batch 50, Loss: 7.0030, Pearson: 0.6630, Spearman: 0.6280
2025-11-14 18:47:05,085 - INFO - Fold 2 Train Epoch 38/200, Batch 60, Loss: 6.9390, Pearson: 0.6576, Spearman: 0.6256
2025-11-14 18:47:15,281 - INFO - Fold 2 Train Epoch 38/200, Batch 70, Loss: 6.8102, Pearson: 0.6660, Spearman: 0.6314
2025-11-14 18:47:24,814 - INFO - Fold 2 Train Epoch 38/200, Batch 80, Loss: 6.7341, Pearson: 0.6629, Spearman: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6701
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6732
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6681
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6677
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6693
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6702
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6701
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6619
2025-11-14 18:47:29,203 - INFO - Fold 2 Train Epoch 38/200, Train Loss: 6.8644, Pearson Mean: 0.6617, Spearman Mean: 0.6272
2025-11-14 18:47:29,204 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4741, 'spearman_mean_genewise': 0.4252, 'l1_error_mean': 1.8339, 'l2_errors_mean': 6.8646, 'r2_scores_mean': 0.2361, 'pearson_std': 0.1114, 'l2_error_q1': 4.7174, 'l2_error_q2': 6.5151, 'l2_error_q3': 8.8178, 'r2_score_q1': 0.1548, 'r2_score_q2': 0.2105, 'r2_score_q3': 0.2842, 'mape_mean': 56.995, 'mape_std': 18.1711, 'rmse_mean': 2.5766, 'rmse_std': 0.4753}
2025-11-14 18:47:29,475 - INFO - Fold 2 Val Epoch 38/200, Batch 0, Loss: 10.3610, Pearson: 0.4733, Spearman: 0.4796
2025-11-14 18:47:30,725 - INFO - Fold 2 Val Epoch 38/200, Batch 10, Loss: 9.5937, Pearson: 0.5512, Spearman: 0.5521
2025-11-14 18:47:32,340 - INFO - Fold 2 Val Epoch 38/200, Batch 20, Loss: 6.5241, Pearson: 0.5280, Spearman: 0.5244
2025-11-14 18:47:34,773 - INFO - Fold 2 Val Epoch 38/200, Val Loss: 8.2971, Pearson Mean: 0.5453, Spearman Mean: 0.5354
2025-11-14 18:47:34,773 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3093, 'spearman_mean_genewise': 0.2889, 'l1_error_mean': 2.0602, 'l2_errors_mean': 8.3189, 'r2_scores_mean': 0.0958, 'pearson_std': 0.1281, 'l2_error_q1': 5.2314, 'l2_error_q2': 8.0125, 'l2_error_q3': 11.3439, 'r2_score_q1': 0.0327, 'r2_score_q2': 0.0704, 'r2_score_q3': 0.1288, 'mape_mean': 64.4346, 'mape_std': 19.2361, 'rmse_mean': 2.8178, 'rmse_std': 0.6155}
2025-11-14 18:47:34,773 - INFO - Learning rate for epoch 38: 1.0000000000000002e-07
2025-11-14 18:47:34,773 - INFO - No improvement in spearman genewise. Patience: 20/30
2025-11-14 18:47:35,677 - INFO - Fold 2 Train Epoch 39/200, Batch 0, Loss: 6.8612, Pearson: 0.6547, Spearman: 0.6227
2025-11-14 18:47:45,903 - INFO - Fold 2 Train Epoch 39/200, Batch 10, Loss: 6.9908, Pearson: 0.6533, Spearman: 0.6266
2025-11-14 18:47:56,088 - INFO - Fold 2 Train Epoch 39/200, Batch 20, Loss: 6.7355, Pearson: 0.6626, Spearman: 0.6247
2025-11-14 18:48:06,264 - INFO - Fold 2 Train Epoch 39/200, Batch 30, Loss: 6.9560, Pearson: 0.6616, Spearman: 0.6258
2025-11-14 18:48:16,440 - INFO - Fold 2 Train Epoch 39/200, Batch 40, Loss: 6.7801, Pearson: 0.6651, Spearman: 0.6300
2025-11-14 18:48:26,654 - INFO - Fold 2 Train Epoch 39/200, Batch 50, Loss: 6.7850, Pearson: 0.6606, Spearman: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6663
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6548
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.864581108093262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 39 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 8.282374 8.282374]
Sample y_pred values (first sample, first 10 genes):
[0.41684648 0.27546877 0.66836345 0.68375635 2.7280822  1.1466157
 0.61321807 0.9612004  3.244573   7.4413137 ]
y_true  -> mean=2.0199, std=3.4634, min=0.0000, max=12.5750
y_pred  -> mean=2.1025, std=2.2407, min=0.0000, max=13.2706
Batch 0 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6663
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6688
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6671
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6674
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6720
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6761
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6606
2025-11-14 18:48:36,829 - INFO - Fold 2 Train Epoch 39/200, Batch 60, Loss: 7.0641, Pearson: 0.6666, Spearman: 0.6313
2025-11-14 18:48:47,020 - INFO - Fold 2 Train Epoch 39/200, Batch 70, Loss: 6.8278, Pearson: 0.6613, Spearman: 0.6254
2025-11-14 18:48:56,482 - INFO - Fold 2 Train Epoch 39/200, Batch 80, Loss: 7.0900, Pearson: 0.6637, Spearman: 0.6324
2025-11-14 18:49:00,910 - INFO - Fold 2 Train Epoch 39/200, Train Loss: 6.8573, Pearson Mean: 0.6620, Spearman Mean: 0.6275
2025-11-14 18:49:00,910 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4748, 'spearman_mean_genewise': 0.4258, 'l1_error_mean': 1.8319, 'l2_errors_mean': 6.8572, 'r2_scores_mean': 0.2368, 'pearson_std': 0.1114, 'l2_error_q1': 4.7179, 'l2_error_q2': 6.5137, 'l2_error_q3': 8.813, 'r2_score_q1': 0.155, 'r2_score_q2': 0.2111, 'r2_score_q3': 0.2858, 'mape_mean': 56.9279, 'mape_std': 18.1748, 'rmse_mean': 2.5752, 'rmse_std': 0.4751}
2025-11-14 18:49:01,185 - INFO - Fold 2 Val Epoch 39/200, Batch 0, Loss: 10.2946, Pearson: 0.4738, Spearman: 0.4803
2025-11-14 18:49:02,453 - INFO - Fold 2 Val Epoch 39/200, Batch 10, Loss: 9.6612, Pearson: 0.5499, Spearman: 0.5507
2025-11-14 18:49:04,154 - INFO - Fold 2 Val Epoch 39/200, Batch 20, Loss: 6.4780, Pearson: 0.5311, Spearman: 0.5275
2025-11-14 18:49:06,593 - INFO - Fold 2 Val Epoch 39/200, Val Loss: 8.2844, Pearson Mean: 0.5459, Spearman Mean: 0.5359
2025-11-14 18:49:06,594 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3095, 'spearman_mean_genewise': 0.2891, 'l1_error_mean': 2.0507, 'l2_errors_mean': 8.3067, 'r2_scores_mean': 0.0971, 'pearson_std': 0.1281, 'l2_error_q1': 5.22, 'l2_error_q2': 7.98, 'l2_error_q3': 11.3358, 'r2_score_q1': 0.0332, 'r2_score_q2': 0.0715, 'r2_score_q3': 0.1293, 'mape_mean': 64.8267, 'mape_std': 19.1449, 'rmse_mean': 2.8157, 'rmse_std': 0.6151}
2025-11-14 18:49:06,594 - INFO - Learning rate for epoch 39: 1.0000000000000002e-07
2025-11-14 18:49:06,594 - INFO - No improvement in spearman genewise. Patience: 21/30
2025-11-14 18:49:07,516 - INFO - Fold 2 Train Epoch 40/200, Batch 0, Loss: 6.7114, Pearson: 0.6634, Spearman: 0.6262
2025-11-14 18:49:17,750 - INFO - Fold 2 Train Epoch 40/200, Batch 10, Loss: 7.0439, Pearson: 0.6648, Spearman: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6690
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6711
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6726
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6809
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6693
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6737
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6686
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6711
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6621
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6569
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.857191562652588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 40 =====================
Sample y_true values (first sample, first 10 genes):
[0.       7.332293 0.       0.       0.       0.       0.       0.
 6.6398   8.835862]
Sample y_pred values (first sample, first 10 genes):
[0.82224715 1.3191857  2.282764   1.5313915  4.065998   3.2231328
 0.93282306 1.7251737  4.9997635  8.751171  ]
y_true  -> mean=2.0627, std=3.4614, min=0.0000, max=12.6644
y_pred  -> mean=2.1011, std=2.2637, min=0.0000, max=13.8102
Batch 0 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6725
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6699
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6725
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6710
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6787
2025-11-14 18:49:27,925 - INFO - Fold 2 Train Epoch 40/200, Batch 20, Loss: 6.8030, Pearson: 0.6686, Spearman: 0.6298
2025-11-14 18:49:38,110 - INFO - Fold 2 Train Epoch 40/200, Batch 30, Loss: 7.0871, Pearson: 0.6579, Spearman: 0.6266
2025-11-14 18:49:48,309 - INFO - Fold 2 Train Epoch 40/200, Batch 40, Loss: 7.0206, Pearson: 0.6653, Spearman: 0.6274
2025-11-14 18:49:58,472 - INFO - Fold 2 Train Epoch 40/200, Batch 50, Loss: 6.8699, Pearson: 0.6651, Spearman: 0.6312
2025-11-14 18:50:08,671 - INFO - Fold 2 Train Epoch 40/200, Batch 60, Loss: 6.8809, Pearson: 0.6628, Spearman: 0.6255
2025-11-14 18:50:18,853 - INFO - Fold 2 Train Epoch 40/200, Batch 70, Loss: 6.9627, Pearson: 0.6522, Spearman: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6686
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6729
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6679
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6709
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6717
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6613
2025-11-14 18:50:28,390 - INFO - Fold 2 Train Epoch 40/200, Batch 80, Loss: 7.0178, Pearson: 0.6649, Spearman: 0.6302
2025-11-14 18:50:32,776 - INFO - Fold 2 Train Epoch 40/200, Train Loss: 6.8611, Pearson Mean: 0.6618, Spearman Mean: 0.6274
2025-11-14 18:50:32,776 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4744, 'spearman_mean_genewise': 0.4254, 'l1_error_mean': 1.8332, 'l2_errors_mean': 6.8612, 'r2_scores_mean': 0.2363, 'pearson_std': 0.1115, 'l2_error_q1': 4.7146, 'l2_error_q2': 6.5164, 'l2_error_q3': 8.8148, 'r2_score_q1': 0.1556, 'r2_score_q2': 0.2113, 'r2_score_q3': 0.285, 'mape_mean': 56.9686, 'mape_std': 18.1816, 'rmse_mean': 2.5759, 'rmse_std': 0.4752}
2025-11-14 18:50:33,045 - INFO - Fold 2 Val Epoch 40/200, Batch 0, Loss: 10.3583, Pearson: 0.4738, Spearman: 0.4802
2025-11-14 18:50:34,290 - INFO - Fold 2 Val Epoch 40/200, Batch 10, Loss: 9.5973, Pearson: 0.5504, Spearman: 0.5514
2025-11-14 18:50:35,990 - INFO - Fold 2 Val Epoch 40/200, Batch 20, Loss: 6.5273, Pearson: 0.5277, Spearman: 0.5241
2025-11-14 18:50:38,448 - INFO - Fold 2 Val Epoch 40/200, Val Loss: 8.2935, Pearson Mean: 0.5455, Spearman Mean: 0.5356
2025-11-14 18:50:38,449 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3101, 'spearman_mean_genewise': 0.2894, 'l1_error_mean': 2.0588, 'l2_errors_mean': 8.3152, 'r2_scores_mean': 0.0962, 'pearson_std': 0.1279, 'l2_error_q1': 5.2349, 'l2_error_q2': 7.9968, 'l2_error_q3': 11.3521, 'r2_score_q1': 0.0325, 'r2_score_q2': 0.071, 'r2_score_q3': 0.1286, 'mape_mean': 64.3599, 'mape_std': 19.1384, 'rmse_mean': 2.8171, 'rmse_std': 0.6156}
2025-11-14 18:50:38,449 - INFO - Learning rate for epoch 40: 1.0000000000000002e-07
2025-11-14 18:50:38,449 - INFO - No improvement in spearman genewise. Patience: 22/30
2025-11-14 18:50:39,413 - INFO - Fold 2 Train Epoch 41/200, Batch 0, Loss: 6.8359, Pearson: 0.6480, Spearman: 0.6161
2025-11-14 18:50:49,456 - INFO - Fold 2 Train Epoch 41/200, Batch 10, Loss: 6.6512, Pearson: 0.6656, Spearman: 0.6273
2025-11-14 18:50:59,650 - INFO - Fold 2 Train Epoch 41/200, Batch 20, Loss: 6.6823, Pearson: 0.6633, Spearman: 0.6270
2025-11-14 18:51:09,868 - INFO - Fold 2 Train Epoch 41/200, Batch 30, Loss: 6.8514, Pearson: 0.6714, Spearman: 0.6321
2025-11-14 18:51:20,062 - INFO - Fold 2 Train Epoch 41/200, Batch 40, Loss: 6.8354, Pearson: 0.6725, Spearman: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6649
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6683
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6645
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6588
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.8612446784973145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 41 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 8.125447 0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.3144204  0.         0.97600216 1.2665999  1.4573703  0.6286466
 0.41346067 1.62088    2.2054935  5.0458174 ]
y_true  -> mean=1.9285, std=3.4257, min=0.0000, max=12.5810
y_pred  -> mean=2.0968, std=2.2364, min=0.0000, max=13.1146
Batch 0 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6692
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6681
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6695
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6670
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6680
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6725
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6700
2025-11-14 18:51:30,248 - INFO - Fold 2 Train Epoch 41/200, Batch 50, Loss: 6.9088, Pearson: 0.6653, Spearman: 0.6308
2025-11-14 18:51:40,447 - INFO - Fold 2 Train Epoch 41/200, Batch 60, Loss: 6.8374, Pearson: 0.6714, Spearman: 0.6357
2025-11-14 18:51:50,663 - INFO - Fold 2 Train Epoch 41/200, Batch 70, Loss: 6.8007, Pearson: 0.6658, Spearman: 0.6339
2025-11-14 18:52:00,194 - INFO - Fold 2 Train Epoch 41/200, Batch 80, Loss: 6.6899, Pearson: 0.6648, Spearman: 0.6267
2025-11-14 18:52:04,566 - INFO - Fold 2 Train Epoch 41/200, Train Loss: 6.8660, Pearson Mean: 0.6616, Spearman Mean: 0.6272
2025-11-14 18:52:04,566 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4739, 'spearman_mean_genewise': 0.4251, 'l1_error_mean': 1.8343, 'l2_errors_mean': 6.866, 'r2_scores_mean': 0.2359, 'pearson_std': 0.1114, 'l2_error_q1': 4.7195, 'l2_error_q2': 6.5241, 'l2_error_q3': 8.8227, 'r2_score_q1': 0.1549, 'r2_score_q2': 0.2098, 'r2_score_q3': 0.2837, 'mape_mean': 57.0084, 'mape_std': 18.1791, 'rmse_mean': 2.5768, 'rmse_std': 0.4755}
2025-11-14 18:52:04,836 - INFO - Fold 2 Val Epoch 41/200, Batch 0, Loss: 10.2881, Pearson: 0.4743, Spearman: 0.4809
2025-11-14 18:52:06,050 - INFO - Fold 2 Val Epoch 41/200, Batch 10, Loss: 9.7046, Pearson: 0.5480, Spearman: 0.5488
2025-11-14 18:52:07,751 - INFO - Fold 2 Val Epoch 41/200, Batch 20, Loss: 6.4812, Pearson: 0.5307, Spearman: 0.5270
2025-11-14 18:52:10,231 - INFO - Fold 2 Val Epoch 41/200, Val Loss: 8.2876, Pearson Mean: 0.5458, Spearman Mean: 0.5358
2025-11-14 18:52:10,232 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3094, 'spearman_mean_genewise': 0.2889, 'l1_error_mean': 2.0505, 'l2_errors_mean': 8.3099, 'r2_scores_mean': 0.0969, 'pearson_std': 0.128, 'l2_error_q1': 5.2194, 'l2_error_q2': 7.9926, 'l2_error_q3': 11.334, 'r2_score_q1': 0.0327, 'r2_score_q2': 0.071, 'r2_score_q3': 0.1293, 'mape_mean': 64.8026, 'mape_std': 19.1813, 'rmse_mean': 2.8162, 'rmse_std': 0.6156}
2025-11-14 18:52:10,232 - INFO - Learning rate for epoch 41: 1.0000000000000002e-07
2025-11-14 18:52:10,232 - INFO - No improvement in spearman genewise. Patience: 23/30
2025-11-14 18:52:11,242 - INFO - Fold 2 Train Epoch 42/200, Batch 0, Loss: 7.0294, Pearson: 0.6587, Spearman: 0.6231
2025-11-14 18:52:21,287 - INFO - Fold 2 Train Epoch 42/200, Batch 10, Loss: 6.8719, Pearson: 0.6654, Spearman: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6680
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6671
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6671
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6703
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6605
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6588
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.8659820556640625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 42 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.6978674 7.6978674 0.        0.        0.
 0.        0.        8.390787 ]
Sample y_pred values (first sample, first 10 genes):
[0.5185842 0.        1.1625603 0.8875919 2.3093147 1.6788626 0.4942312
 1.7793965 2.9010832 6.4493246]
y_true  -> mean=2.1811, std=3.5219, min=0.0000, max=12.4276
y_pred  -> mean=2.1095, std=2.2699, min=0.0000, max=13.9500
Batch 0 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6677
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6701
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6589
2025-11-14 18:52:31,499 - INFO - Fold 2 Train Epoch 42/200, Batch 20, Loss: 6.8798, Pearson: 0.6549, Spearman: 0.6278
2025-11-14 18:52:41,692 - INFO - Fold 2 Train Epoch 42/200, Batch 30, Loss: 6.7434, Pearson: 0.6641, Spearman: 0.6276
2025-11-14 18:52:51,891 - INFO - Fold 2 Train Epoch 42/200, Batch 40, Loss: 6.7192, Pearson: 0.6630, Spearman: 0.6276
2025-11-14 18:53:02,050 - INFO - Fold 2 Train Epoch 42/200, Batch 50, Loss: 6.8089, Pearson: 0.6636, Spearman: 0.6255
2025-11-14 18:53:12,257 - INFO - Fold 2 Train Epoch 42/200, Batch 60, Loss: 6.5745, Pearson: 0.6615, Spearman: 0.6281
2025-11-14 18:53:22,438 - INFO - Fold 2 Train Epoch 42/200, Batch 70, Loss: 6.9323, Pearson: 0.6654, Spearman: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6759
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6719
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6730
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6697
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6711
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6724
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6593
2025-11-14 18:53:31,958 - INFO - Fold 2 Train Epoch 42/200, Batch 80, Loss: 6.9584, Pearson: 0.6587, Spearman: 0.6238
2025-11-14 18:53:36,354 - INFO - Fold 2 Train Epoch 42/200, Train Loss: 6.8594, Pearson Mean: 0.6620, Spearman Mean: 0.6275
2025-11-14 18:53:36,354 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4746, 'spearman_mean_genewise': 0.4255, 'l1_error_mean': 1.8323, 'l2_errors_mean': 6.8592, 'r2_scores_mean': 0.2366, 'pearson_std': 0.1115, 'l2_error_q1': 4.7116, 'l2_error_q2': 6.5033, 'l2_error_q3': 8.8215, 'r2_score_q1': 0.1558, 'r2_score_q2': 0.2106, 'r2_score_q3': 0.2859, 'mape_mean': 56.9386, 'mape_std': 18.1676, 'rmse_mean': 2.5755, 'rmse_std': 0.4752}
2025-11-14 18:53:36,626 - INFO - Fold 2 Val Epoch 42/200, Batch 0, Loss: 10.3116, Pearson: 0.4746, Spearman: 0.4811
2025-11-14 18:53:37,900 - INFO - Fold 2 Val Epoch 42/200, Batch 10, Loss: 9.6441, Pearson: 0.5488, Spearman: 0.5495
2025-11-14 18:53:39,594 - INFO - Fold 2 Val Epoch 42/200, Batch 20, Loss: 6.4970, Pearson: 0.5306, Spearman: 0.5268
2025-11-14 18:53:42,003 - INFO - Fold 2 Val Epoch 42/200, Val Loss: 8.2896, Pearson Mean: 0.5458, Spearman Mean: 0.5359
2025-11-14 18:53:42,004 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3094, 'spearman_mean_genewise': 0.289, 'l1_error_mean': 2.0575, 'l2_errors_mean': 8.3118, 'r2_scores_mean': 0.0967, 'pearson_std': 0.1279, 'l2_error_q1': 5.2219, 'l2_error_q2': 7.9856, 'l2_error_q3': 11.3431, 'r2_score_q1': 0.033, 'r2_score_q2': 0.0711, 'r2_score_q3': 0.1294, 'mape_mean': 64.5579, 'mape_std': 19.2449, 'rmse_mean': 2.8165, 'rmse_std': 0.6155}
2025-11-14 18:53:42,004 - INFO - Learning rate for epoch 42: 1.0000000000000002e-07
2025-11-14 18:53:42,004 - INFO - No improvement in spearman genewise. Patience: 24/30
2025-11-14 18:53:43,056 - INFO - Fold 2 Train Epoch 43/200, Batch 0, Loss: 6.8218, Pearson: 0.6661, Spearman: 0.6346
2025-11-14 18:53:53,202 - INFO - Fold 2 Train Epoch 43/200, Batch 10, Loss: 6.7649, Pearson: 0.6658, Spearman: 0.6270
2025-11-14 18:54:03,386 - INFO - Fold 2 Train Epoch 43/200, Batch 20, Loss: 7.1104, Pearson: 0.6643, Spearman: 0.6331
2025-11-14 18:54:13,596 - INFO - Fold 2 Train Epoch 43/200, Batch 30, Loss: 6.8825, Pearson: 0.6615, Spearman: 0.6263
2025-11-14 18:54:23,774 - INFO - Fold 2 Train Epoch 43/200, Batch 40, Loss: 6.8427, Pearson: 0.6669, Spearman: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6712
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6618
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6551
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.859220027923584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 43 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        7.7291756 7.7291756
 7.7291756 0.        9.11514  ]
Sample y_pred values (first sample, first 10 genes):
[0.45237774 0.         1.9035635  1.3794681  2.2433074  2.2073412
 0.63278157 2.6469555  1.8008857  5.6071033 ]
y_true  -> mean=2.1492, std=3.4990, min=0.0000, max=12.5140
y_pred  -> mean=2.1034, std=2.2340, min=0.0000, max=12.8545
Batch 0 Pearson correlation: 0.6661
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6713
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6677
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6716
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6688
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6689
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6723
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6622
2025-11-14 18:54:33,958 - INFO - Fold 2 Train Epoch 43/200, Batch 50, Loss: 6.7275, Pearson: 0.6636, Spearman: 0.6243
2025-11-14 18:54:44,138 - INFO - Fold 2 Train Epoch 43/200, Batch 60, Loss: 6.9814, Pearson: 0.6573, Spearman: 0.6273
2025-11-14 18:54:54,351 - INFO - Fold 2 Train Epoch 43/200, Batch 70, Loss: 6.7481, Pearson: 0.6620, Spearman: 0.6264
2025-11-14 18:55:03,878 - INFO - Fold 2 Train Epoch 43/200, Batch 80, Loss: 6.9388, Pearson: 0.6562, Spearman: 0.6220
2025-11-14 18:55:08,268 - INFO - Fold 2 Train Epoch 43/200, Train Loss: 6.8596, Pearson Mean: 0.6620, Spearman Mean: 0.6275
2025-11-14 18:55:08,268 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4746, 'spearman_mean_genewise': 0.4257, 'l1_error_mean': 1.8331, 'l2_errors_mean': 6.8597, 'r2_scores_mean': 0.2365, 'pearson_std': 0.1113, 'l2_error_q1': 4.709, 'l2_error_q2': 6.5057, 'l2_error_q3': 8.8313, 'r2_score_q1': 0.1552, 'r2_score_q2': 0.2104, 'r2_score_q3': 0.2849, 'mape_mean': 56.9665, 'mape_std': 18.175, 'rmse_mean': 2.5757, 'rmse_std': 0.4751}
2025-11-14 18:55:08,536 - INFO - Fold 2 Val Epoch 43/200, Batch 0, Loss: 10.3235, Pearson: 0.4739, Spearman: 0.4803
2025-11-14 18:55:09,760 - INFO - Fold 2 Val Epoch 43/200, Batch 10, Loss: 9.6019, Pearson: 0.5511, Spearman: 0.5519
2025-11-14 18:55:11,389 - INFO - Fold 2 Val Epoch 43/200, Batch 20, Loss: 6.5072, Pearson: 0.5292, Spearman: 0.5255
2025-11-14 18:55:13,814 - INFO - Fold 2 Val Epoch 43/200, Val Loss: 8.2899, Pearson Mean: 0.5457, Spearman Mean: 0.5358
2025-11-14 18:55:13,814 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3095, 'spearman_mean_genewise': 0.289, 'l1_error_mean': 2.057, 'l2_errors_mean': 8.3119, 'r2_scores_mean': 0.0967, 'pearson_std': 0.128, 'l2_error_q1': 5.217, 'l2_error_q2': 7.9905, 'l2_error_q3': 11.3308, 'r2_score_q1': 0.0329, 'r2_score_q2': 0.0709, 'r2_score_q3': 0.1301, 'mape_mean': 64.5765, 'mape_std': 19.2593, 'rmse_mean': 2.8166, 'rmse_std': 0.6155}
2025-11-14 18:55:13,814 - INFO - Learning rate for epoch 43: 1.0000000000000002e-07
2025-11-14 18:55:13,814 - INFO - No improvement in spearman genewise. Patience: 25/30
2025-11-14 18:55:14,804 - INFO - Fold 2 Train Epoch 44/200, Batch 0, Loss: 6.7559, Pearson: 0.6558, Spearman: 0.6242
2025-11-14 18:55:25,018 - INFO - Fold 2 Train Epoch 44/200, Batch 10, Loss: 6.9369, Pearson: 0.6605, Spearman: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6707
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6702
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6760
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6688
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6694
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6727
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6671
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.859735488891602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 44 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       9.047772]
Sample y_pred values (first sample, first 10 genes):
[0.38356918 0.5935054  0.475714   0.68742144 2.4416623  1.0956094
 0.29753387 1.556325   2.3940964  7.0204816 ]
y_true  -> mean=1.9816, std=3.4398, min=0.0000, max=12.6872
y_pred  -> mean=2.0927, std=2.2302, min=0.0000, max=13.8035
Batch 0 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6691
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6568
2025-11-14 18:55:35,236 - INFO - Fold 2 Train Epoch 44/200, Batch 20, Loss: 7.0307, Pearson: 0.6532, Spearman: 0.6255
2025-11-14 18:55:45,416 - INFO - Fold 2 Train Epoch 44/200, Batch 30, Loss: 6.8619, Pearson: 0.6579, Spearman: 0.6242
2025-11-14 18:55:55,608 - INFO - Fold 2 Train Epoch 44/200, Batch 40, Loss: 6.7120, Pearson: 0.6672, Spearman: 0.6280
2025-11-14 18:56:05,817 - INFO - Fold 2 Train Epoch 44/200, Batch 50, Loss: 7.0563, Pearson: 0.6557, Spearman: 0.6203
2025-11-14 18:56:16,006 - INFO - Fold 2 Train Epoch 44/200, Batch 60, Loss: 6.8580, Pearson: 0.6574, Spearman: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6721
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6707
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6723
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6734
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6686
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6684
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6632
2025-11-14 18:56:26,190 - INFO - Fold 2 Train Epoch 44/200, Batch 70, Loss: 6.7033, Pearson: 0.6607, Spearman: 0.6325
2025-11-14 18:56:35,699 - INFO - Fold 2 Train Epoch 44/200, Batch 80, Loss: 6.7158, Pearson: 0.6599, Spearman: 0.6275
2025-11-14 18:56:40,101 - INFO - Fold 2 Train Epoch 44/200, Train Loss: 6.8651, Pearson Mean: 0.6617, Spearman Mean: 0.6272
2025-11-14 18:56:40,101 - INFO - Training Metrics: {'pearson_mean_genewise': 0.474, 'spearman_mean_genewise': 0.4252, 'l1_error_mean': 1.8343, 'l2_errors_mean': 6.8651, 'r2_scores_mean': 0.236, 'pearson_std': 0.1114, 'l2_error_q1': 4.711, 'l2_error_q2': 6.5198, 'l2_error_q3': 8.8297, 'r2_score_q1': 0.1543, 'r2_score_q2': 0.2107, 'r2_score_q3': 0.2843, 'mape_mean': 56.9987, 'mape_std': 18.1768, 'rmse_mean': 2.5767, 'rmse_std': 0.4753}
2025-11-14 18:56:40,392 - INFO - Fold 2 Val Epoch 44/200, Batch 0, Loss: 10.2954, Pearson: 0.4743, Spearman: 0.4807
2025-11-14 18:56:41,617 - INFO - Fold 2 Val Epoch 44/200, Batch 10, Loss: 9.6035, Pearson: 0.5509, Spearman: 0.5514
2025-11-14 18:56:43,254 - INFO - Fold 2 Val Epoch 44/200, Batch 20, Loss: 6.5230, Pearson: 0.5282, Spearman: 0.5244
2025-11-14 18:56:45,716 - INFO - Fold 2 Val Epoch 44/200, Val Loss: 8.2928, Pearson Mean: 0.5452, Spearman Mean: 0.5353
2025-11-14 18:56:45,717 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3094, 'spearman_mean_genewise': 0.2889, 'l1_error_mean': 2.0595, 'l2_errors_mean': 8.3145, 'r2_scores_mean': 0.0965, 'pearson_std': 0.1279, 'l2_error_q1': 5.2282, 'l2_error_q2': 7.9924, 'l2_error_q3': 11.3498, 'r2_score_q1': 0.0328, 'r2_score_q2': 0.0716, 'r2_score_q3': 0.1293, 'mape_mean': 64.5664, 'mape_std': 19.2207, 'rmse_mean': 2.8169, 'rmse_std': 0.6159}
2025-11-14 18:56:45,717 - INFO - Learning rate for epoch 44: 1.0000000000000004e-08
2025-11-14 18:56:45,717 - INFO - No improvement in spearman genewise. Patience: 26/30
2025-11-14 18:56:46,609 - INFO - Fold 2 Train Epoch 45/200, Batch 0, Loss: 6.7621, Pearson: 0.6667, Spearman: 0.6343
2025-11-14 18:56:56,873 - INFO - Fold 2 Train Epoch 45/200, Batch 10, Loss: 7.1153, Pearson: 0.6507, Spearman: 0.6182
2025-11-14 18:57:07,045 - INFO - Fold 2 Train Epoch 45/200, Batch 20, Loss: 6.8273, Pearson: 0.6635, Spearman: 0.6270
2025-11-14 18:57:17,210 - INFO - Fold 2 Train Epoch 45/200, Batch 30, Loss: 6.9476, Pearson: 0.6581, Spearman: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6696
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6674
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6649
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6750
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6609
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6651
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.86511754989624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 45 =====================
Sample y_true values (first sample, first 10 genes):
[ 0.         0.         0.         0.         8.92906    0.
  0.         7.5431633  7.5431633 10.027584 ]
Sample y_pred values (first sample, first 10 genes):
[0.62359667 1.225867   1.9878329  0.6443721  4.4650326  1.9549437
 0.17652652 1.7900295  3.19986    8.195869  ]
y_true  -> mean=2.1497, std=3.4869, min=0.0000, max=12.7151
y_pred  -> mean=2.1016, std=2.2462, min=0.0000, max=13.1119
Batch 0 Pearson correlation: 0.6667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6680
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6679
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6704
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6681
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6674
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6540
2025-11-14 18:57:27,392 - INFO - Fold 2 Train Epoch 45/200, Batch 40, Loss: 6.9219, Pearson: 0.6604, Spearman: 0.6260
2025-11-14 18:57:37,584 - INFO - Fold 2 Train Epoch 45/200, Batch 50, Loss: 7.1865, Pearson: 0.6565, Spearman: 0.6205
2025-11-14 18:57:47,763 - INFO - Fold 2 Train Epoch 45/200, Batch 60, Loss: 6.7307, Pearson: 0.6673, Spearman: 0.6292
2025-11-14 18:57:57,941 - INFO - Fold 2 Train Epoch 45/200, Batch 70, Loss: 6.6515, Pearson: 0.6700, Spearman: 0.6270
2025-11-14 18:58:07,471 - INFO - Fold 2 Train Epoch 45/200, Batch 80, Loss: 7.0484, Pearson: 0.6631, Spearman: 0.6323
2025-11-14 18:58:11,886 - INFO - Fold 2 Train Epoch 45/200, Train Loss: 6.8539, Pearson Mean: 0.6622, Spearman Mean: 0.6277
2025-11-14 18:58:11,886 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4751, 'spearman_mean_genewise': 0.4261, 'l1_error_mean': 1.8314, 'l2_errors_mean': 6.854, 'r2_scores_mean': 0.2371, 'pearson_std': 0.1115, 'l2_error_q1': 4.7092, 'l2_error_q2': 6.4997, 'l2_error_q3': 8.7899, 'r2_score_q1': 0.1556, 'r2_score_q2': 0.2106, 'r2_score_q3': 0.2852, 'mape_mean': 56.9146, 'mape_std': 18.1719, 'rmse_mean': 2.5746, 'rmse_std': 0.4748}
2025-11-14 18:58:12,170 - INFO - Fold 2 Val Epoch 45/200, Batch 0, Loss: 10.4055, Pearson: 0.4727, Spearman: 0.4790
2025-11-14 18:58:13,402 - INFO - Fold 2 Val Epoch 45/200, Batch 10, Loss: 9.4753, Pearson: 0.5539, Spearman: 0.5544
2025-11-14 18:58:15,076 - INFO - Fold 2 Val Epoch 45/200, Batch 20, Loss: 6.5559, Pearson: 0.5266, Spearman: 0.5230
2025-11-14 18:58:17,532 - INFO - Fold 2 Val Epoch 45/200, Val Loss: 8.3074, Pearson Mean: 0.5448, Spearman Mean: 0.5350
2025-11-14 18:58:17,532 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3095, 'spearman_mean_genewise': 0.2889, 'l1_error_mean': 2.0729, 'l2_errors_mean': 8.3289, 'r2_scores_mean': 0.0949, 'pearson_std': 0.1279, 'l2_error_q1': 5.2341, 'l2_error_q2': 8.0277, 'l2_error_q3': 11.3465, 'r2_score_q1': 0.0317, 'r2_score_q2': 0.0699, 'r2_score_q3': 0.1271, 'mape_mean': 63.9805, 'mape_std': 19.276, 'rmse_mean': 2.8194, 'rmse_std': 0.6162}
2025-11-14 18:58:17,533 - INFO - Learning rate for epoch 45: 1.0000000000000004e-08
2025-11-14 18:58:17,533 - INFO - No improvement in spearman genewise. Patience: 27/30
2025-11-14 18:58:18,440 - INFO - Fold 2 Train Epoch 46/200, Batch 0, Loss: 6.8951, Pearson: 0.6588, Spearman: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6649
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6679
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6670
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6718
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6674
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6663
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6665
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6651
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6628
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.853988170623779
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 46 =====================
Sample y_true values (first sample, first 10 genes):
[ 0.        0.        0.        0.        0.        0.        0.
  0.        0.       10.008893]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.2989182  0.         0.75203204 0.26267904
 0.01070756 0.         1.5435495  6.7702827 ]
y_true  -> mean=2.0951, std=3.4893, min=0.0000, max=12.4292
y_pred  -> mean=2.0992, std=2.2390, min=0.0000, max=14.1065
Batch 0 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6547
2025-11-14 18:58:28,661 - INFO - Fold 2 Train Epoch 46/200, Batch 10, Loss: 6.8024, Pearson: 0.6627, Spearman: 0.6323
2025-11-14 18:58:38,863 - INFO - Fold 2 Train Epoch 46/200, Batch 20, Loss: 6.9763, Pearson: 0.6609, Spearman: 0.6266
2025-11-14 18:58:49,058 - INFO - Fold 2 Train Epoch 46/200, Batch 30, Loss: 6.8770, Pearson: 0.6604, Spearman: 0.6327
2025-11-14 18:58:59,253 - INFO - Fold 2 Train Epoch 46/200, Batch 40, Loss: 6.9060, Pearson: 0.6507, Spearman: 0.6199
2025-11-14 18:59:09,414 - INFO - Fold 2 Train Epoch 46/200, Batch 50, Loss: 6.9606, Pearson: 0.6634, Spearman: 0.6276
2025-11-14 18:59:19,618 - INFO - Fold 2 Train Epoch 46/200, Batch 60, Loss: 6.9050, Pearson: 0.6585, Spearman: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6671
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6720
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6680
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6763
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6694
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6661
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6677
2025-11-14 18:59:29,805 - INFO - Fold 2 Train Epoch 46/200, Batch 70, Loss: 6.9406, Pearson: 0.6642, Spearman: 0.6256
2025-11-14 18:59:39,291 - INFO - Fold 2 Train Epoch 46/200, Batch 80, Loss: 7.0946, Pearson: 0.6584, Spearman: 0.6268
2025-11-14 18:59:43,774 - INFO - Fold 2 Train Epoch 46/200, Train Loss: 6.8604, Pearson Mean: 0.6619, Spearman Mean: 0.6274
2025-11-14 18:59:43,775 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4744, 'spearman_mean_genewise': 0.4255, 'l1_error_mean': 1.8332, 'l2_errors_mean': 6.8602, 'r2_scores_mean': 0.2364, 'pearson_std': 0.1115, 'l2_error_q1': 4.7054, 'l2_error_q2': 6.5121, 'l2_error_q3': 8.8054, 'r2_score_q1': 0.1549, 'r2_score_q2': 0.2113, 'r2_score_q3': 0.2848, 'mape_mean': 56.965, 'mape_std': 18.175, 'rmse_mean': 2.5758, 'rmse_std': 0.475}
2025-11-14 18:59:44,049 - INFO - Fold 2 Val Epoch 46/200, Batch 0, Loss: 10.3364, Pearson: 0.4735, Spearman: 0.4800
2025-11-14 18:59:45,329 - INFO - Fold 2 Val Epoch 46/200, Batch 10, Loss: 9.6583, Pearson: 0.5482, Spearman: 0.5491
2025-11-14 18:59:47,036 - INFO - Fold 2 Val Epoch 46/200, Batch 20, Loss: 6.4922, Pearson: 0.5305, Spearman: 0.5269
2025-11-14 18:59:49,490 - INFO - Fold 2 Val Epoch 46/200, Val Loss: 8.2916, Pearson Mean: 0.5458, Spearman Mean: 0.5358
2025-11-14 18:59:49,490 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3095, 'spearman_mean_genewise': 0.2889, 'l1_error_mean': 2.0558, 'l2_errors_mean': 8.314, 'r2_scores_mean': 0.0964, 'pearson_std': 0.1281, 'l2_error_q1': 5.2304, 'l2_error_q2': 7.9932, 'l2_error_q3': 11.3383, 'r2_score_q1': 0.0326, 'r2_score_q2': 0.0708, 'r2_score_q3': 0.1293, 'mape_mean': 64.5658, 'mape_std': 19.1842, 'rmse_mean': 2.8169, 'rmse_std': 0.6155}
2025-11-14 18:59:49,491 - INFO - Learning rate for epoch 46: 1.0000000000000004e-08
2025-11-14 18:59:49,491 - INFO - No improvement in spearman genewise. Patience: 28/30
2025-11-14 18:59:50,548 - INFO - Fold 2 Train Epoch 47/200, Batch 0, Loss: 6.8636, Pearson: 0.6676, Spearman: 0.6310
2025-11-14 19:00:00,645 - INFO - Fold 2 Train Epoch 47/200, Batch 10, Loss: 6.7922, Pearson: 0.6603, Spearman: 0.6259
2025-11-14 19:00:10,820 - INFO - Fold 2 Train Epoch 47/200, Batch 20, Loss: 6.7255, Pearson: 0.6657, Spearman: 0.6255
2025-11-14 19:00:21,020 - INFO - Fold 2 Train Epoch 47/200, Batch 30, Loss: 6.8626, Pearson: 0.6587, Spearman: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6678
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6717
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6734
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6709
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6677
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6709
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6611
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6587
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.8601837158203125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 47 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        6.1603417 6.1603417 7.545051  6.8524323 0.
 6.1603417 6.1603417 9.102778 ]
Sample y_pred values (first sample, first 10 genes):
[1.1428381 3.44636   3.2260876 1.9712291 5.5159802 4.618387  1.4887357
 3.3458412 3.9011    9.172424 ]
y_true  -> mean=2.1895, std=3.5159, min=0.0000, max=12.5404
y_pred  -> mean=2.1060, std=2.2810, min=0.0000, max=13.2060
Batch 0 Pearson correlation: 0.6676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6704
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6683
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6688
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6567
2025-11-14 19:00:31,206 - INFO - Fold 2 Train Epoch 47/200, Batch 40, Loss: 6.9146, Pearson: 0.6610, Spearman: 0.6261
2025-11-14 19:00:41,382 - INFO - Fold 2 Train Epoch 47/200, Batch 50, Loss: 6.6825, Pearson: 0.6623, Spearman: 0.6261
2025-11-14 19:00:51,574 - INFO - Fold 2 Train Epoch 47/200, Batch 60, Loss: 6.9420, Pearson: 0.6504, Spearman: 0.6220
2025-11-14 19:01:01,791 - INFO - Fold 2 Train Epoch 47/200, Batch 70, Loss: 6.6991, Pearson: 0.6673, Spearman: 0.6259
2025-11-14 19:01:11,298 - INFO - Fold 2 Train Epoch 47/200, Batch 80, Loss: 7.0447, Pearson: 0.6605, Spearman: 0.6243
2025-11-14 19:01:15,706 - INFO - Fold 2 Train Epoch 47/200, Train Loss: 6.8574, Pearson Mean: 0.6620, Spearman Mean: 0.6275
2025-11-14 19:01:15,706 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4747, 'spearman_mean_genewise': 0.4257, 'l1_error_mean': 1.8321, 'l2_errors_mean': 6.8575, 'r2_scores_mean': 0.2367, 'pearson_std': 0.1116, 'l2_error_q1': 4.7095, 'l2_error_q2': 6.5099, 'l2_error_q3': 8.819, 'r2_score_q1': 0.1557, 'r2_score_q2': 0.2104, 'r2_score_q3': 0.2854, 'mape_mean': 56.9436, 'mape_std': 18.1824, 'rmse_mean': 2.5752, 'rmse_std': 0.475}
2025-11-14 19:01:15,975 - INFO - Fold 2 Val Epoch 47/200, Batch 0, Loss: 10.2905, Pearson: 0.4745, Spearman: 0.4810
2025-11-14 19:01:17,200 - INFO - Fold 2 Val Epoch 47/200, Batch 10, Loss: 9.7398, Pearson: 0.5472, Spearman: 0.5480
2025-11-14 19:01:18,838 - INFO - Fold 2 Val Epoch 47/200, Batch 20, Loss: 6.5001, Pearson: 0.5291, Spearman: 0.5253
2025-11-14 19:01:21,264 - INFO - Fold 2 Val Epoch 47/200, Val Loss: 8.2906, Pearson Mean: 0.5456, Spearman Mean: 0.5356
2025-11-14 19:01:21,264 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.309, 'spearman_mean_genewise': 0.2886, 'l1_error_mean': 2.0497, 'l2_errors_mean': 8.3127, 'r2_scores_mean': 0.0967, 'pearson_std': 0.1281, 'l2_error_q1': 5.2221, 'l2_error_q2': 7.9779, 'l2_error_q3': 11.3447, 'r2_score_q1': 0.0325, 'r2_score_q2': 0.0711, 'r2_score_q3': 0.129, 'mape_mean': 64.8977, 'mape_std': 19.2081, 'rmse_mean': 2.8167, 'rmse_std': 0.6158}
2025-11-14 19:01:21,264 - INFO - Learning rate for epoch 47: 1.0000000000000004e-08
2025-11-14 19:01:21,264 - INFO - No improvement in spearman genewise. Patience: 29/30
2025-11-14 19:01:22,198 - INFO - Fold 2 Train Epoch 48/200, Batch 0, Loss: 6.9492, Pearson: 0.6615, Spearman: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6686
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6681
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6680
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6712
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6677
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6686
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6710
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6654
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6637
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.8574628829956055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 48 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.08289109 0.         1.1567338  0.         1.6631863  0.
 0.         0.7769489  1.7747359  7.298314  ]
y_true  -> mean=2.1678, std=3.5129, min=0.0000, max=12.5441
y_pred  -> mean=2.1051, std=2.2529, min=0.0000, max=13.1058
Batch 0 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6699
2025-11-14 19:01:32,448 - INFO - Fold 2 Train Epoch 48/200, Batch 10, Loss: 7.0012, Pearson: 0.6639, Spearman: 0.6277
2025-11-14 19:01:42,625 - INFO - Fold 2 Train Epoch 48/200, Batch 20, Loss: 6.9188, Pearson: 0.6608, Spearman: 0.6302
2025-11-14 19:01:52,839 - INFO - Fold 2 Train Epoch 48/200, Batch 30, Loss: 6.9510, Pearson: 0.6602, Spearman: 0.6257
2025-11-14 19:02:03,027 - INFO - Fold 2 Train Epoch 48/200, Batch 40, Loss: 6.7900, Pearson: 0.6724, Spearman: 0.6354
2025-11-14 19:02:13,212 - INFO - Fold 2 Train Epoch 48/200, Batch 50, Loss: 6.7649, Pearson: 0.6646, Spearman: 0.6277
2025-11-14 19:02:23,400 - INFO - Fold 2 Train Epoch 48/200, Batch 60, Loss: 6.8435, Pearson: 0.6620, Spearman: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6679
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6663
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6694
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6678
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6724
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6678
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6658
2025-11-14 19:02:33,625 - INFO - Fold 2 Train Epoch 48/200, Batch 70, Loss: 6.8406, Pearson: 0.6670, Spearman: 0.6351
2025-11-14 19:02:43,165 - INFO - Fold 2 Train Epoch 48/200, Batch 80, Loss: 6.7802, Pearson: 0.6617, Spearman: 0.6299
2025-11-14 19:02:47,570 - INFO - Fold 2 Train Epoch 48/200, Train Loss: 6.8572, Pearson Mean: 0.6621, Spearman Mean: 0.6276
2025-11-14 19:02:47,570 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4749, 'spearman_mean_genewise': 0.4258, 'l1_error_mean': 1.832, 'l2_errors_mean': 6.8571, 'r2_scores_mean': 0.2368, 'pearson_std': 0.1113, 'l2_error_q1': 4.7062, 'l2_error_q2': 6.5114, 'l2_error_q3': 8.8055, 'r2_score_q1': 0.1559, 'r2_score_q2': 0.2106, 'r2_score_q3': 0.2855, 'mape_mean': 56.9329, 'mape_std': 18.166, 'rmse_mean': 2.5752, 'rmse_std': 0.4751}
2025-11-14 19:02:47,839 - INFO - Fold 2 Val Epoch 48/200, Batch 0, Loss: 10.2723, Pearson: 0.4752, Spearman: 0.4818
2025-11-14 19:02:49,051 - INFO - Fold 2 Val Epoch 48/200, Batch 10, Loss: 9.7051, Pearson: 0.5477, Spearman: 0.5482
2025-11-14 19:02:50,767 - INFO - Fold 2 Val Epoch 48/200, Batch 20, Loss: 6.4788, Pearson: 0.5315, Spearman: 0.5277
2025-11-14 19:02:53,213 - INFO - Fold 2 Val Epoch 48/200, Val Loss: 8.2819, Pearson Mean: 0.5462, Spearman Mean: 0.5362
2025-11-14 19:02:53,213 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3094, 'spearman_mean_genewise': 0.2891, 'l1_error_mean': 2.0519, 'l2_errors_mean': 8.3044, 'r2_scores_mean': 0.0975, 'pearson_std': 0.1281, 'l2_error_q1': 5.2209, 'l2_error_q2': 7.98, 'l2_error_q3': 11.3243, 'r2_score_q1': 0.0334, 'r2_score_q2': 0.0712, 'r2_score_q3': 0.1303, 'mape_mean': 64.7709, 'mape_std': 19.2549, 'rmse_mean': 2.8153, 'rmse_std': 0.6151}
2025-11-14 19:02:53,213 - INFO - Learning rate for epoch 48: 1.0000000000000004e-08
2025-11-14 19:02:53,214 - INFO - No improvement in spearman genewise. Patience: 30/30
2025-11-14 19:02:53,214 - INFO - Early stopping triggered. Breaking training loop.
2025-11-14 19:02:53,215 - INFO - ===== Completed Fold 2/5 =====
2025-11-14 19:02:53,215 - INFO - 
===== Starting Fold 3/5 =====
2025-11-14 19:02:53,215 - INFO - Fold 3: Train=29, Val=7
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6689
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6670
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6625
image : torch.Size([116, 3, 224, 224]), y_true: torch.Size([116, 785]), y_pred: torch.Size([116, 785])
Batch 84 Pearson correlation: 0.6712
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.857140064239502
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A1.h5ad
 Loaded images: (346, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (346, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A4.h5ad
 Loaded images: (343, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (343, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A5.h5ad
 Loaded images: (332, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (332, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B1.h5ad
 Loaded images: (295, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (295, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B2.h5ad
 Loaded images: (270, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (270, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B3.h5ad
 Loaded images: (298, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (298, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B4.h5ad
 Loaded images: (283, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (283, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B5.h5ad
 Loaded images: (289, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (289, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C1.h5ad
 Loaded images: (176, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (176, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C2.h5ad
 Loaded images: (187, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (187, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C3.h5ad
 Loaded images: (180, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (180, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C5.h5ad
 Loaded images: (181, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (181, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C6.h5ad
 Loaded images: (178, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (178, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D1.h5ad
 Loaded images: (306, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (306, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D3.h5ad
 Loaded images: (301, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (301, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D4.h5ad
 Loaded images: (302, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (302, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D5.h5ad
 Loaded images: (306, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (306, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D6.h5ad
 Loaded images: (315, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (315, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E2.h5ad
 Loaded images: (572, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (572, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E3.h5ad
 Loaded images: (570, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (570, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F1.h5ad
 Loaded images: (691, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (691, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F2.h5ad
 Loaded images: (695, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (695, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F3.h5ad
 Loaded images: (712, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (712, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G1.h5ad
 Loaded images: (441, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (441, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G2.h5ad
 Loaded images: (467, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (467, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G3.h5ad
 Loaded images: (463, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (463, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H1.h5ad
 Loaded images: (613, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (613, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H2.h5ad
 Loaded images: (603, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (603, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H3.h5ad
 Loaded images: (510, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (510, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A2.h5ad
 Loaded images: (325, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (325, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A3.h5ad
 Loaded images: (359, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (359, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A6.h5ad
 Loaded images: (360, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (360, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B6.h5ad
 Loaded images: (277, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (277, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C4.h5ad
 Loaded images: (184, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (184, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D2.h5ad
2025-11-14 19:03:10,898 - INFO - Fold 3: Train=11225, Val=2395
2025-11-14 19:03:10,898 - INFO - train_datasets length:  11225
2025-11-14 19:03:10,898 - INFO - Number of train batches: 88
2025-11-14 19:03:10,898 - INFO - Initializing model...
2025-11-14 19:03:10,995 - INFO - Model
STNet(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (denseblock1): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition1): _Transition(
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock2): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition2): _Transition(
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock3): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer17): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer18): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer19): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer20): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer21): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer22): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer23): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer24): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition3): _Transition(
      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock4): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=1024, out_features=785, bias=True)
)
2025-11-14 19:03:10,997 - INFO - Using device: cuda
2025-11-14 19:03:12,030 - INFO - Fold 3 Train Epoch 1/200, Batch 0, Loss: 15.8191, Pearson: -0.0160, Spearman: -0.0110
2025-11-14 19:03:21,678 - INFO - Fold 3 Train Epoch 1/200, Batch 10, Loss: 14.9464, Pearson: 0.1243, Spearman: 0.0611
2025-11-14 19:03:31,209 - INFO - Fold 3 Train Epoch 1/200, Batch 20, Loss: 13.8581, Pearson: 0.2325, Spearman: 0.1388
2025-11-14 19:03:40,751 - INFO - Fold 3 Train Epoch 1/200, Batch 30, Loss: 11.8274, Pearson: 0.3129, Spearman: 0.2268
2025-11-14 19:03:50,291 - INFO - Fold 3 Train Epoch 1/200, Batch 40, Loss: 10.7182, Pearson: 0.3740, Spearman: 0.3283
 Loaded images: (303, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (303, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E1.h5ad
 Loaded images: (587, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (587, 785)
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 1 =====================
Sample y_true values (first sample, first 10 genes):
[7.4774823 0.        0.        0.        0.        7.4774823 0.
 8.170346  7.4774823 8.170346 ]
Sample y_pred values (first sample, first 10 genes):
[0.1783336  0.4669034  0.48975313 0.5202826  0.23753043 0.46183476
 0.         0.         0.03967899 0.54786265]
y_true  -> mean=2.0809, std=3.4763, min=0.0000, max=12.6824
y_pred  -> mean=0.1717, std=0.2494, min=0.0000, max=1.7290
Batch 0 Pearson correlation: -0.0160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: -0.0009
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.0173
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.0370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.0410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.0508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.0786
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.0832
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.1004
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.0959
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.1243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.1361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.1617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.1539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.1664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.1875
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.1962
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.1990
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.2391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.1946
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.2325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.2304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.2514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.2339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.2412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.2434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.2814
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.2696
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.2704
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.2551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.3129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.3036
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.2877
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.3066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.3510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.3338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.3482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.3048
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.3716
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.3751
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.3740
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.3231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.3672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.4067
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.3895
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.3651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.3980
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.3821
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.4192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.4264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 19:03:59,662 - INFO - Fold 3 Train Epoch 1/200, Batch 50, Loss: 10.6949, Pearson: 0.3859, Spearman: 0.3976
2025-11-14 19:04:06,463 - INFO - Fold 3 Train Epoch 1/200, Batch 60, Loss: 9.5911, Pearson: 0.4593, Spearman: 0.4358
2025-11-14 19:04:15,849 - INFO - Fold 3 Train Epoch 1/200, Batch 70, Loss: 9.2309, Pearson: 0.4903, Spearman: 0.4642
2025-11-14 19:04:25,955 - INFO - Fold 3 Train Epoch 1/200, Batch 80, Loss: 9.3528, Pearson: 0.5041, Spearman: 0.4834
2025-11-14 19:04:34,116 - INFO - Fold 3 Train Epoch 1/200, Train Loss: 11.5973, Pearson Mean: 0.3354, Spearman Mean: 0.3018
2025-11-14 19:04:34,116 - INFO - Training Metrics: {'pearson_mean_genewise': 0.1756, 'spearman_mean_genewise': 0.1623, 'l1_error_mean': 2.2244, 'l2_errors_mean': 11.6061, 'r2_scores_mean': -0.1841, 'pearson_std': 0.0643, 'l2_error_q1': 5.4287, 'l2_error_q2': 8.524, 'l2_error_q3': 14.3698, 'r2_score_q1': -0.1002, 'r2_score_q2': -0.0145, 'r2_score_q3': 0.0139, 'mape_mean': 79.822, 'mape_std': 7.4556, 'rmse_mean': 3.2147, 'rmse_std': 1.1278}
2025-11-14 19:04:34,450 - INFO - Fold 3 Val Epoch 1/200, Batch 0, Loss: 11.4732, Pearson: 0.4181, Spearman: 0.4253
2025-11-14 19:04:36,139 - INFO - Fold 3 Val Epoch 1/200, Batch 10, Loss: 10.3632, Pearson: 0.5293, Spearman: 0.5032
2025-11-14 19:04:39,680 - INFO - Fold 3 Val Epoch 1/200, Val Loss: 9.7920, Pearson Mean: 0.4787, Spearman Mean: 0.4777
2025-11-14 19:04:39,680 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2356, 'spearman_mean_genewise': 0.2278, 'l1_error_mean': 2.436, 'l2_errors_mean': 9.8233, 'r2_scores_mean': 0.0078, 'pearson_std': 0.1021, 'l2_error_q1': 6.2899, 'l2_error_q2': 9.3312, 'l2_error_q3': 12.9082, 'r2_score_q1': 0.011, 'r2_score_q2': 0.0335, 'r2_score_q3': 0.0609, 'mape_mean': 70.0383, 'mape_std': 14.9872, 'rmse_mean': 3.0624, 'rmse_std': 0.6672}
2025-11-14 19:04:39,680 - INFO - Learning rate for epoch 1: 0.0001
2025-11-14 19:04:39,728 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_3/best_model.pth
2025-11-14 19:04:40,704 - INFO - Fold 3 Train Epoch 2/200, Batch 0, Loss: 8.6230, Pearson: 0.5114, Spearman: 0.4931
2025-11-14 19:04:50,844 - INFO - Fold 3 Train Epoch 2/200, Batch 10, Loss: 8.9059, Pearson: 0.5199, Spearman: 0.5005
Batch 50 Pearson correlation: 0.3859
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.4369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.4222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.4181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.4289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.4258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.4458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.4438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.4308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.4389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.4593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.4463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.4685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.4355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.4491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.4741
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.4506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.4774
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.4828
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.4487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.4903
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.4806
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.4613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.4874
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.4706
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.4853
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.4976
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.4819
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.4973
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.4801
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5041
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.4835
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.4949
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.4898
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.4751
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.4830
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.4838
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.4941
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 11.606060028076172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 2 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       8.019782 0.       0.       0.
 0.       8.712765]
Sample y_pred values (first sample, first 10 genes):
[0.34592915 0.90841264 1.1546307  0.44831648 2.2415056  1.3345801
 0.4971518  1.0416055  2.255031   3.606464  ]
y_true  -> mean=1.9049, std=3.4011, min=0.0000, max=12.8039
y_pred  -> mean=1.7285, std=1.5164, min=0.0000, max=10.2999
Batch 0 Pearson correlation: 0.5114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.4974
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5035
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5103
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.4921
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.4992
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5005
2025-11-14 19:05:00,974 - INFO - Fold 3 Train Epoch 2/200, Batch 20, Loss: 8.6015, Pearson: 0.5058, Spearman: 0.4960
2025-11-14 19:05:11,134 - INFO - Fold 3 Train Epoch 2/200, Batch 30, Loss: 8.5866, Pearson: 0.5182, Spearman: 0.5097
2025-11-14 19:05:21,254 - INFO - Fold 3 Train Epoch 2/200, Batch 40, Loss: 8.5371, Pearson: 0.5276, Spearman: 0.5091
2025-11-14 19:05:30,685 - INFO - Fold 3 Train Epoch 2/200, Batch 50, Loss: 8.6368, Pearson: 0.5334, Spearman: 0.5098
2025-11-14 19:05:37,944 - INFO - Fold 3 Train Epoch 2/200, Batch 60, Loss: 8.7792, Pearson: 0.5329, Spearman: 0.5224
2025-11-14 19:05:48,003 - INFO - Fold 3 Train Epoch 2/200, Batch 70, Loss: 8.5560, Pearson: 0.5390, Spearman: 0.5205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5107
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5058
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.4983
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5602
2025-11-14 19:05:58,147 - INFO - Fold 3 Train Epoch 2/200, Batch 80, Loss: 8.8287, Pearson: 0.5460, Spearman: 0.5210
2025-11-14 19:06:06,306 - INFO - Fold 3 Train Epoch 2/200, Train Loss: 8.6992, Pearson Mean: 0.5328, Spearman Mean: 0.5124
2025-11-14 19:06:06,306 - INFO - Training Metrics: {'pearson_mean_genewise': 0.2886, 'spearman_mean_genewise': 0.2644, 'l1_error_mean': 2.1463, 'l2_errors_mean': 8.6995, 'r2_scores_mean': 0.0562, 'pearson_std': 0.0884, 'l2_error_q1': 5.1184, 'l2_error_q2': 7.7759, 'l2_error_q3': 11.8176, 'r2_score_q1': 0.0417, 'r2_score_q2': 0.071, 'r2_score_q3': 0.1064, 'mape_mean': 68.1503, 'mape_std': 15.6626, 'rmse_mean': 2.868, 'rmse_std': 0.6884}
2025-11-14 19:06:06,669 - INFO - Fold 3 Val Epoch 2/200, Batch 0, Loss: 10.8176, Pearson: 0.4599, Spearman: 0.4683
2025-11-14 19:06:08,444 - INFO - Fold 3 Val Epoch 2/200, Batch 10, Loss: 9.4011, Pearson: 0.5527, Spearman: 0.5252
2025-11-14 19:06:12,160 - INFO - Fold 3 Val Epoch 2/200, Val Loss: 9.0683, Pearson Mean: 0.5233, Spearman Mean: 0.5177
2025-11-14 19:06:12,160 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2843, 'spearman_mean_genewise': 0.2695, 'l1_error_mean': 2.3527, 'l2_errors_mean': 9.0994, 'r2_scores_mean': 0.0699, 'pearson_std': 0.1142, 'l2_error_q1': 6.1608, 'l2_error_q2': 8.9521, 'l2_error_q3': 11.79, 'r2_score_q1': 0.0327, 'r2_score_q2': 0.0611, 'r2_score_q3': 0.1049, 'mape_mean': 64.5348, 'mape_std': 18.0975, 'rmse_mean': 2.9625, 'rmse_std': 0.5683}
2025-11-14 19:06:12,161 - INFO - Learning rate for epoch 2: 0.0001
2025-11-14 19:06:12,232 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_3/best_model.pth
2025-11-14 19:06:13,126 - INFO - Fold 3 Train Epoch 3/200, Batch 0, Loss: 8.1730, Pearson: 0.5456, Spearman: 0.5199
2025-11-14 19:06:22,878 - INFO - Fold 3 Train Epoch 3/200, Batch 10, Loss: 8.4524, Pearson: 0.5676, Spearman: 0.5381
2025-11-14 19:06:33,055 - INFO - Fold 3 Train Epoch 3/200, Batch 20, Loss: 8.9890, Pearson: 0.5681, Spearman: 0.5482
2025-11-14 19:06:43,197 - INFO - Fold 3 Train Epoch 3/200, Batch 30, Loss: 8.3062, Pearson: 0.5454, Spearman: 0.5292
2025-11-14 19:06:53,348 - INFO - Fold 3 Train Epoch 3/200, Batch 40, Loss: 8.1748, Pearson: 0.5652, Spearman: 0.5420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.5679
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.5523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.5498
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.5301
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.699501037597656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 3 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       6.659058 0.       7.351564 0.       0.       0.
 6.659058 8.449749]
Sample y_pred values (first sample, first 10 genes):
[0.8343843  2.5923705  2.6545477  1.5383139  5.072568   3.3999205
 0.97709113 2.535388   4.4217644  9.1507435 ]
y_true  -> mean=1.9104, std=3.4070, min=0.0000, max=12.6482
y_pred  -> mean=1.9607, std=1.9947, min=0.0000, max=11.8741
Batch 0 Pearson correlation: 0.5456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5674
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5737
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5733
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5705
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5732
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5681
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5743
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5691
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5657
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5729
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5694
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5652
2025-11-14 19:07:02,273 - INFO - Fold 3 Train Epoch 3/200, Batch 50, Loss: 7.9859, Pearson: 0.5751, Spearman: 0.5358
2025-11-14 19:07:09,994 - INFO - Fold 3 Train Epoch 3/200, Batch 60, Loss: 7.9510, Pearson: 0.5802, Spearman: 0.5472
2025-11-14 19:07:20,141 - INFO - Fold 3 Train Epoch 3/200, Batch 70, Loss: 8.2993, Pearson: 0.5672, Spearman: 0.5434
2025-11-14 19:07:30,287 - INFO - Fold 3 Train Epoch 3/200, Batch 80, Loss: 8.2863, Pearson: 0.5643, Spearman: 0.5410
2025-11-14 19:07:38,379 - INFO - Fold 3 Train Epoch 3/200, Train Loss: 8.2476, Pearson Mean: 0.5652, Spearman Mean: 0.5395
2025-11-14 19:07:38,379 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3218, 'spearman_mean_genewise': 0.291, 'l1_error_mean': 2.0845, 'l2_errors_mean': 8.2484, 'r2_scores_mean': 0.0971, 'pearson_std': 0.0931, 'l2_error_q1': 5.0433, 'l2_error_q2': 7.5855, 'l2_error_q3': 11.2477, 'r2_score_q1': 0.0589, 'r2_score_q2': 0.0941, 'r2_score_q3': 0.1352, 'mape_mean': 65.7508, 'mape_std': 16.6081, 'rmse_mean': 2.8039, 'rmse_std': 0.6218}
2025-11-14 19:07:38,679 - INFO - Fold 3 Val Epoch 3/200, Batch 0, Loss: 10.5478, Pearson: 0.4855, Spearman: 0.4935
2025-11-14 19:07:40,370 - INFO - Fold 3 Val Epoch 3/200, Batch 10, Loss: 8.8897, Pearson: 0.5707, Spearman: 0.5458
2025-11-14 19:07:44,012 - INFO - Fold 3 Val Epoch 3/200, Val Loss: 8.8283, Pearson Mean: 0.5384, Spearman Mean: 0.5364
2025-11-14 19:07:44,013 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3026, 'spearman_mean_genewise': 0.2851, 'l1_error_mean': 2.2481, 'l2_errors_mean': 8.8586, 'r2_scores_mean': 0.0886, 'pearson_std': 0.1248, 'l2_error_q1': 6.1326, 'l2_error_q2': 8.752, 'l2_error_q3': 11.3409, 'r2_score_q1': 0.0366, 'r2_score_q2': 0.0669, 'r2_score_q3': 0.1214, 'mape_mean': 62.8892, 'mape_std': 19.6486, 'rmse_mean': 2.9263, 'rmse_std': 0.5434}
2025-11-14 19:07:44,013 - INFO - Learning rate for epoch 3: 0.0001
2025-11-14 19:07:44,071 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_3/best_model.pth
2025-11-14 19:07:44,986 - INFO - Fold 3 Train Epoch 4/200, Batch 0, Loss: 7.8635, Pearson: 0.5660, Spearman: 0.5320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5671
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5740
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5847
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5743
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5715
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5774
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5665
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5751
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5782
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5786
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5719
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5727
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5762
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5802
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5769
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5704
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5762
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5767
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5693
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5844
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5734
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5760
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.5714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.5715
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.5687
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.5738
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.248435974121094
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 4 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.316388 0.       7.316388 0.       0.       0.
 7.316388 7.316388]
Sample y_pred values (first sample, first 10 genes):
[0.3380071  0.11240011 0.61121327 0.57150924 2.70544    1.5442872
 0.5057452  1.3795168  2.8245363  5.903633  ]
y_true  -> mean=1.9222, std=3.4015, min=0.0000, max=12.5078
y_pred  -> mean=1.9206, std=1.9401, min=0.0000, max=12.0271
Batch 0 Pearson correlation: 0.5660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5735
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5856
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5898
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5894
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5679
2025-11-14 19:07:55,097 - INFO - Fold 3 Train Epoch 4/200, Batch 10, Loss: 7.9912, Pearson: 0.5724, Spearman: 0.5443
2025-11-14 19:08:05,246 - INFO - Fold 3 Train Epoch 4/200, Batch 20, Loss: 7.8914, Pearson: 0.5935, Spearman: 0.5490
2025-11-14 19:08:15,376 - INFO - Fold 3 Train Epoch 4/200, Batch 30, Loss: 8.2268, Pearson: 0.5725, Spearman: 0.5527
2025-11-14 19:08:25,488 - INFO - Fold 3 Train Epoch 4/200, Batch 40, Loss: 8.1158, Pearson: 0.5864, Spearman: 0.5629
2025-11-14 19:08:33,994 - INFO - Fold 3 Train Epoch 4/200, Batch 50, Loss: 7.9815, Pearson: 0.5956, Spearman: 0.5637
2025-11-14 19:08:42,035 - INFO - Fold 3 Train Epoch 4/200, Batch 60, Loss: 8.0133, Pearson: 0.5797, Spearman: 0.5476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5747
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5787
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5889
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5724
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5830
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5770
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5831
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5715
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5812
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5734
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5894
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5940
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5935
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5925
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5804
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5726
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5853
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5782
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5794
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5723
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5814
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5725
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5854
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5827
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5863
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5869
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5825
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5851
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5844
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5848
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5875
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5864
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5731
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5724
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5835
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5809
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6020
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5836
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5701
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5792
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5916
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5956
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5719
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5745
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5786
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5910
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5785
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5777
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5850
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5705
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5792
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5797
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5872
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5967
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5850
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5833
2025-11-14 19:08:52,193 - INFO - Fold 3 Train Epoch 4/200, Batch 70, Loss: 8.0459, Pearson: 0.5921, Spearman: 0.5589
2025-11-14 19:09:02,335 - INFO - Fold 3 Train Epoch 4/200, Batch 80, Loss: 8.1312, Pearson: 0.5801, Spearman: 0.5559
2025-11-14 19:09:10,461 - INFO - Fold 3 Train Epoch 4/200, Train Loss: 7.9890, Pearson Mean: 0.5824, Spearman Mean: 0.5533
2025-11-14 19:09:10,461 - INFO - Training Metrics: {'pearson_mean_genewise': 0.344, 'spearman_mean_genewise': 0.3095, 'l1_error_mean': 2.0323, 'l2_errors_mean': 7.9886, 'r2_scores_mean': 0.1205, 'pearson_std': 0.0984, 'l2_error_q1': 5.0006, 'l2_error_q2': 7.4518, 'l2_error_q3': 10.816, 'r2_score_q1': 0.071, 'r2_score_q2': 0.1107, 'r2_score_q3': 0.1527, 'mape_mean': 64.2725, 'mape_std': 17.3009, 'rmse_mean': 2.764, 'rmse_std': 0.5905}
2025-11-14 19:09:10,771 - INFO - Fold 3 Val Epoch 4/200, Batch 0, Loss: 10.7987, Pearson: 0.4846, Spearman: 0.4936
2025-11-14 19:09:12,571 - INFO - Fold 3 Val Epoch 4/200, Batch 10, Loss: 8.6750, Pearson: 0.5815, Spearman: 0.5595
2025-11-14 19:09:16,151 - INFO - Fold 3 Val Epoch 4/200, Val Loss: 8.6703, Pearson Mean: 0.5517, Spearman Mean: 0.5465
2025-11-14 19:09:16,152 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3132, 'spearman_mean_genewise': 0.2887, 'l1_error_mean': 2.2273, 'l2_errors_mean': 8.6983, 'r2_scores_mean': 0.1009, 'pearson_std': 0.1288, 'l2_error_q1': 6.0923, 'l2_error_q2': 8.6435, 'l2_error_q3': 11.0634, 'r2_score_q1': 0.0419, 'r2_score_q2': 0.0765, 'r2_score_q3': 0.1297, 'mape_mean': 64.0871, 'mape_std': 19.3143, 'rmse_mean': 2.9028, 'rmse_std': 0.5218}
2025-11-14 19:09:16,152 - INFO - Learning rate for epoch 4: 0.0001
2025-11-14 19:09:16,202 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_3/best_model.pth
2025-11-14 19:09:17,120 - INFO - Fold 3 Train Epoch 5/200, Batch 0, Loss: 7.7561, Pearson: 0.5920, Spearman: 0.5597
2025-11-14 19:09:27,265 - INFO - Fold 3 Train Epoch 5/200, Batch 10, Loss: 7.6224, Pearson: 0.5848, Spearman: 0.5533
2025-11-14 19:09:37,412 - INFO - Fold 3 Train Epoch 5/200, Batch 20, Loss: 8.1454, Pearson: 0.5831, Spearman: 0.5632
2025-11-14 19:09:47,544 - INFO - Fold 3 Train Epoch 5/200, Batch 30, Loss: 7.5648, Pearson: 0.6053, Spearman: 0.5667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5970
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5990
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5847
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5850
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5713
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5921
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5909
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5852
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5902
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5944
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5856
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5887
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5821
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5896
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5882
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5801
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5990
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5902
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5794
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.5995
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.5762
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.5807
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.5667
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.988615036010742
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 5 =====================
Sample y_true values (first sample, first 10 genes):
[ 0.        0.        0.        0.        0.        0.        0.
  0.        0.       10.101979]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.73705196 0.         0.9680591  0.27838796
 0.         0.32907048 1.2320735  4.314225  ]
y_true  -> mean=2.0570, std=3.4523, min=0.0000, max=13.8155
y_pred  -> mean=1.9391, std=2.0081, min=0.0000, max=12.6585
Batch 0 Pearson correlation: 0.5920
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5916
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5918
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5781
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5940
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5864
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5867
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5878
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6028
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5817
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5848
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5860
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5880
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5894
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5872
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6040
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5941
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5881
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5984
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5916
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5831
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6047
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6047
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5970
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6010
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5907
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5809
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5875
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6053
2025-11-14 19:09:57,669 - INFO - Fold 3 Train Epoch 5/200, Batch 40, Loss: 7.9087, Pearson: 0.5978, Spearman: 0.5688
2025-11-14 19:10:05,530 - INFO - Fold 3 Train Epoch 5/200, Batch 50, Loss: 8.1513, Pearson: 0.6051, Spearman: 0.5609
2025-11-14 19:10:14,258 - INFO - Fold 3 Train Epoch 5/200, Batch 60, Loss: 7.6749, Pearson: 0.5969, Spearman: 0.5684
2025-11-14 19:10:24,416 - INFO - Fold 3 Train Epoch 5/200, Batch 70, Loss: 7.5767, Pearson: 0.6047, Spearman: 0.5675
2025-11-14 19:10:34,534 - INFO - Fold 3 Train Epoch 5/200, Batch 80, Loss: 7.6395, Pearson: 0.5966, Spearman: 0.5606
2025-11-14 19:10:42,691 - INFO - Fold 3 Train Epoch 5/200, Train Loss: 7.8575, Pearson Mean: 0.5923, Spearman Mean: 0.5613
2025-11-14 19:10:42,692 - INFO - Training Metrics: {'pearson_mean_genewise': 0.358, 'spearman_mean_genewise': 0.3215, 'l1_error_mean': 1.9967, 'l2_errors_mean': 7.8571, 'r2_scores_mean': 0.1326, 'pearson_std': 0.1014, 'l2_error_q1': 4.971, 'l2_error_q2': 7.3254, 'l2_error_q3': 10.6326, 'r2_score_q1': 0.0783, 'r2_score_q2': 0.1188, 'r2_score_q3': 0.1659, 'mape_mean': 63.7951, 'mape_std': 17.2342, 'rmse_mean': 2.7427, 'rmse_std': 0.5784}
2025-11-14 19:10:43,080 - INFO - Fold 3 Val Epoch 5/200, Batch 0, Loss: 10.5719, Pearson: 0.4821, Spearman: 0.4928
2025-11-14 19:10:44,821 - INFO - Fold 3 Val Epoch 5/200, Batch 10, Loss: 8.9235, Pearson: 0.5696, Spearman: 0.5555
2025-11-14 19:10:48,441 - INFO - Fold 3 Val Epoch 5/200, Val Loss: 8.8929, Pearson Mean: 0.5382, Spearman Mean: 0.5398
2025-11-14 19:10:48,442 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2995, 'spearman_mean_genewise': 0.2812, 'l1_error_mean': 2.2441, 'l2_errors_mean': 8.9264, 'r2_scores_mean': 0.0824, 'pearson_std': 0.123, 'l2_error_q1': 6.1284, 'l2_error_q2': 8.7843, 'l2_error_q3': 11.5225, 'r2_score_q1': 0.0323, 'r2_score_q2': 0.066, 'r2_score_q3': 0.1172, 'mape_mean': 64.0543, 'mape_std': 18.3284, 'rmse_mean': 2.9364, 'rmse_std': 0.5512}
2025-11-14 19:10:48,442 - INFO - Learning rate for epoch 5: 0.0001
2025-11-14 19:10:48,442 - INFO - No improvement in spearman genewise. Patience: 1/30
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5970
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6010
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5852
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5976
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5980
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5820
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5928
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5913
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5978
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5863
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5903
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5956
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5803
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5832
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5749
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5893
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5917
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5926
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6051
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5942
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5904
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5882
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5964
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5763
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5771
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5926
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5884
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5969
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5895
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5990
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5924
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5786
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5876
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6002
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6073
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5973
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5934
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6047
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5934
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5827
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5880
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5853
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6069
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5939
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5864
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5966
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5863
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5957
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5944
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.5825
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.5947
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.5916
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.5826
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.857120513916016
2025-11-14 19:10:49,394 - INFO - Fold 3 Train Epoch 6/200, Batch 0, Loss: 7.4596, Pearson: 0.5949, Spearman: 0.5623
2025-11-14 19:10:59,558 - INFO - Fold 3 Train Epoch 6/200, Batch 10, Loss: 7.6576, Pearson: 0.5946, Spearman: 0.5633
2025-11-14 19:11:09,651 - INFO - Fold 3 Train Epoch 6/200, Batch 20, Loss: 7.7223, Pearson: 0.6050, Spearman: 0.5732
2025-11-14 19:11:19,760 - INFO - Fold 3 Train Epoch 6/200, Batch 30, Loss: 7.6232, Pearson: 0.6001, Spearman: 0.5647
2025-11-14 19:11:29,873 - INFO - Fold 3 Train Epoch 6/200, Batch 40, Loss: 7.8666, Pearson: 0.6029, Spearman: 0.5720
2025-11-14 19:11:36,906 - INFO - Fold 3 Train Epoch 6/200, Batch 50, Loss: 7.6563, Pearson: 0.5973, Spearman: 0.5701
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 6 =====================
Sample y_true values (first sample, first 10 genes):
[7.4444838 0.        0.        0.        0.        0.        0.
 7.4444838 0.        0.       ]
Sample y_pred values (first sample, first 10 genes):
[0.93419343 1.5134201  3.6971073  0.988189   4.5532207  4.1543856
 0.98408586 2.016065   3.8106585  4.978194  ]
y_true  -> mean=1.8970, std=3.3960, min=0.0000, max=12.6482
y_pred  -> mean=1.9838, std=2.0392, min=0.0000, max=13.2752
Batch 0 Pearson correlation: 0.5949
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5971
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5778
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5972
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5823
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5925
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5926
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6064
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5964
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5946
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5946
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5962
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5942
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6021
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5940
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5979
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5891
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5985
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5906
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5992
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6023
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5995
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6017
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5907
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5995
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6001
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5918
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5891
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6076
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6118
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6073
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6029
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6078
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5972
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6046
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5892
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6111
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5991
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5973
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6009
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5812
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5984
2025-11-14 19:11:46,446 - INFO - Fold 3 Train Epoch 6/200, Batch 60, Loss: 7.5187, Pearson: 0.6063, Spearman: 0.5640
2025-11-14 19:11:56,572 - INFO - Fold 3 Train Epoch 6/200, Batch 70, Loss: 7.9648, Pearson: 0.6012, Spearman: 0.5663
2025-11-14 19:12:06,682 - INFO - Fold 3 Train Epoch 6/200, Batch 80, Loss: 7.4768, Pearson: 0.6011, Spearman: 0.5741
2025-11-14 19:12:14,811 - INFO - Fold 3 Train Epoch 6/200, Train Loss: 7.7322, Pearson Mean: 0.6001, Spearman Mean: 0.5680
2025-11-14 19:12:14,811 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3677, 'spearman_mean_genewise': 0.3294, 'l1_error_mean': 1.9855, 'l2_errors_mean': 7.7331, 'r2_scores_mean': 0.1433, 'pearson_std': 0.1049, 'l2_error_q1': 4.954, 'l2_error_q2': 7.2073, 'l2_error_q3': 10.4361, 'r2_score_q1': 0.0825, 'r2_score_q2': 0.1262, 'r2_score_q3': 0.1786, 'mape_mean': 62.8918, 'mape_std': 17.6355, 'rmse_mean': 2.7228, 'rmse_std': 0.5654}
2025-11-14 19:12:15,138 - INFO - Fold 3 Val Epoch 6/200, Batch 0, Loss: 10.5796, Pearson: 0.4825, Spearman: 0.4956
2025-11-14 19:12:16,839 - INFO - Fold 3 Val Epoch 6/200, Batch 10, Loss: 8.4187, Pearson: 0.5943, Spearman: 0.5705
2025-11-14 19:12:20,530 - INFO - Fold 3 Val Epoch 6/200, Val Loss: 8.5983, Pearson Mean: 0.5577, Spearman Mean: 0.5549
2025-11-14 19:12:20,531 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3213, 'spearman_mean_genewise': 0.2967, 'l1_error_mean': 2.2561, 'l2_errors_mean': 8.6264, 'r2_scores_mean': 0.1072, 'pearson_std': 0.1306, 'l2_error_q1': 6.0949, 'l2_error_q2': 8.5404, 'l2_error_q3': 10.9029, 'r2_score_q1': 0.0422, 'r2_score_q2': 0.0756, 'r2_score_q3': 0.1394, 'mape_mean': 59.9249, 'mape_std': 20.0012, 'rmse_mean': 2.8902, 'rmse_std': 0.5225}
2025-11-14 19:12:20,531 - INFO - Learning rate for epoch 6: 0.0001
2025-11-14 19:12:20,592 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_3/best_model.pth
2025-11-14 19:12:21,632 - INFO - Fold 3 Train Epoch 7/200, Batch 0, Loss: 7.1716, Pearson: 0.6077, Spearman: 0.5677
2025-11-14 19:12:31,679 - INFO - Fold 3 Train Epoch 7/200, Batch 10, Loss: 7.5857, Pearson: 0.6091, Spearman: 0.5673
2025-11-14 19:12:41,832 - INFO - Fold 3 Train Epoch 7/200, Batch 20, Loss: 7.3763, Pearson: 0.6073, Spearman: 0.5709
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5927
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5968
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5997
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5960
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5976
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6063
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5824
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6005
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5985
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6020
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6082
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6038
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5945
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6017
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6012
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6044
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6021
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6002
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6137
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6000
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5940
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6026
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5933
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6011
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5979
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6068
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6027
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6064
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6081
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6039
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.73314094543457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 7 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 7.780447 0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.699218   0.         1.1947111  0.598742
 0.49138045 0.43901092 1.6212165  6.0132627 ]
y_true  -> mean=1.8545, std=3.3707, min=0.0000, max=12.4731
y_pred  -> mean=1.9269, std=2.0742, min=0.0000, max=13.1669
Batch 0 Pearson correlation: 0.6077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6035
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6022
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6093
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6091
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6097
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6060
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5968
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6065
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6152
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6043
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6073
2025-11-14 19:12:52,013 - INFO - Fold 3 Train Epoch 7/200, Batch 30, Loss: 7.5180, Pearson: 0.6124, Spearman: 0.5737
2025-11-14 19:13:01,465 - INFO - Fold 3 Train Epoch 7/200, Batch 40, Loss: 7.6756, Pearson: 0.6157, Spearman: 0.5805
2025-11-14 19:13:08,750 - INFO - Fold 3 Train Epoch 7/200, Batch 50, Loss: 7.5085, Pearson: 0.6066, Spearman: 0.5703
2025-11-14 19:13:18,786 - INFO - Fold 3 Train Epoch 7/200, Batch 60, Loss: 7.6821, Pearson: 0.6081, Spearman: 0.5765
2025-11-14 19:13:28,916 - INFO - Fold 3 Train Epoch 7/200, Batch 70, Loss: 7.7821, Pearson: 0.6065, Spearman: 0.5752
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6101
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6026
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6100
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5979
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6058
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6064
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6012
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6075
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6059
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6029
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6080
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6027
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6072
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6045
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6118
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6065
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6153
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6077
2025-11-14 19:13:39,064 - INFO - Fold 3 Train Epoch 7/200, Batch 80, Loss: 7.6525, Pearson: 0.6094, Spearman: 0.5783
2025-11-14 19:13:47,197 - INFO - Fold 3 Train Epoch 7/200, Train Loss: 7.5628, Pearson Mean: 0.6110, Spearman Mean: 0.5752
2025-11-14 19:13:47,197 - INFO - Training Metrics: {'pearson_mean_genewise': 0.385, 'spearman_mean_genewise': 0.3441, 'l1_error_mean': 1.9537, 'l2_errors_mean': 7.5637, 'r2_scores_mean': 0.1585, 'pearson_std': 0.1079, 'l2_error_q1': 4.8603, 'l2_error_q2': 7.1042, 'l2_error_q3': 10.0956, 'r2_score_q1': 0.0918, 'r2_score_q2': 0.139, 'r2_score_q3': 0.1944, 'mape_mean': 61.6872, 'mape_std': 17.9078, 'rmse_mean': 2.6946, 'rmse_std': 0.5502}
2025-11-14 19:13:47,522 - INFO - Fold 3 Val Epoch 7/200, Batch 0, Loss: 10.5665, Pearson: 0.4861, Spearman: 0.4982
2025-11-14 19:13:49,267 - INFO - Fold 3 Val Epoch 7/200, Batch 10, Loss: 8.3762, Pearson: 0.6042, Spearman: 0.5782
2025-11-14 19:13:52,844 - INFO - Fold 3 Val Epoch 7/200, Val Loss: 8.5887, Pearson Mean: 0.5606, Spearman Mean: 0.5571
2025-11-14 19:13:52,844 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3194, 'spearman_mean_genewise': 0.2976, 'l1_error_mean': 2.2407, 'l2_errors_mean': 8.6161, 'r2_scores_mean': 0.1084, 'pearson_std': 0.1336, 'l2_error_q1': 6.0602, 'l2_error_q2': 8.4769, 'l2_error_q3': 10.9179, 'r2_score_q1': 0.0427, 'r2_score_q2': 0.0792, 'r2_score_q3': 0.1379, 'mape_mean': 61.4294, 'mape_std': 20.5838, 'rmse_mean': 2.8883, 'rmse_std': 0.5231}
2025-11-14 19:13:52,844 - INFO - Learning rate for epoch 7: 0.0001
2025-11-14 19:13:52,911 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_3/best_model.pth
2025-11-14 19:13:53,837 - INFO - Fold 3 Train Epoch 8/200, Batch 0, Loss: 7.5360, Pearson: 0.6224, Spearman: 0.5898
2025-11-14 19:14:04,001 - INFO - Fold 3 Train Epoch 8/200, Batch 10, Loss: 7.3769, Pearson: 0.6237, Spearman: 0.5762
2025-11-14 19:14:14,149 - INFO - Fold 3 Train Epoch 8/200, Batch 20, Loss: 7.2487, Pearson: 0.6187, Spearman: 0.5814
2025-11-14 19:14:24,300 - INFO - Fold 3 Train Epoch 8/200, Batch 30, Loss: 7.5067, Pearson: 0.6089, Spearman: 0.5747
2025-11-14 19:14:33,378 - INFO - Fold 3 Train Epoch 8/200, Batch 40, Loss: 7.4968, Pearson: 0.6197, Spearman: 0.5836
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6094
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6084
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6072
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6079
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6164
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.563724040985107
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 8 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       9.567085]
Sample y_pred values (first sample, first 10 genes):
[0.07744905 1.3431991  1.2358203  0.         1.8257635  0.54402524
 0.06249601 0.7324412  1.9888095  7.778036  ]
y_true  -> mean=2.1455, std=3.5033, min=0.0000, max=12.6548
y_pred  -> mean=2.0346, std=2.1157, min=0.0000, max=13.2962
Batch 0 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6041
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6017
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6089
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6156
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6120
2025-11-14 19:14:40,956 - INFO - Fold 3 Train Epoch 8/200, Batch 50, Loss: 7.3735, Pearson: 0.6298, Spearman: 0.5913
2025-11-14 19:14:51,117 - INFO - Fold 3 Train Epoch 8/200, Batch 60, Loss: 7.4453, Pearson: 0.6110, Spearman: 0.5694
2025-11-14 19:15:01,250 - INFO - Fold 3 Train Epoch 8/200, Batch 70, Loss: 7.3355, Pearson: 0.6159, Spearman: 0.5803
2025-11-14 19:15:11,371 - INFO - Fold 3 Train Epoch 8/200, Batch 80, Loss: 7.6102, Pearson: 0.6117, Spearman: 0.5841
2025-11-14 19:15:19,549 - INFO - Fold 3 Train Epoch 8/200, Train Loss: 7.4768, Pearson Mean: 0.6167, Spearman Mean: 0.5797
2025-11-14 19:15:19,549 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3935, 'spearman_mean_genewise': 0.352, 'l1_error_mean': 1.9312, 'l2_errors_mean': 7.477, 'r2_scores_mean': 0.166, 'pearson_std': 0.1101, 'l2_error_q1': 4.8231, 'l2_error_q2': 7.0605, 'l2_error_q3': 9.9142, 'r2_score_q1': 0.0959, 'r2_score_q2': 0.146, 'r2_score_q3': 0.205, 'mape_mean': 61.2668, 'mape_std': 18.0495, 'rmse_mean': 2.6801, 'rmse_std': 0.5424}
2025-11-14 19:15:19,836 - INFO - Fold 3 Val Epoch 8/200, Batch 0, Loss: 10.8667, Pearson: 0.4830, Spearman: 0.4959
2025-11-14 19:15:21,597 - INFO - Fold 3 Val Epoch 8/200, Batch 10, Loss: 8.3850, Pearson: 0.6023, Spearman: 0.5767
2025-11-14 19:15:25,242 - INFO - Fold 3 Val Epoch 8/200, Val Loss: 8.6089, Pearson Mean: 0.5590, Spearman Mean: 0.5564
2025-11-14 19:15:25,243 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3159, 'spearman_mean_genewise': 0.2932, 'l1_error_mean': 2.195, 'l2_errors_mean': 8.6374, 'r2_scores_mean': 0.1057, 'pearson_std': 0.134, 'l2_error_q1': 6.0939, 'l2_error_q2': 8.5907, 'l2_error_q3': 11.0432, 'r2_score_q1': 0.0403, 'r2_score_q2': 0.0756, 'r2_score_q3': 0.1336, 'mape_mean': 64.1722, 'mape_std': 19.9095, 'rmse_mean': 2.8924, 'rmse_std': 0.5209}
2025-11-14 19:15:25,243 - INFO - Learning rate for epoch 8: 0.0001
2025-11-14 19:15:25,243 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 19:15:26,165 - INFO - Fold 3 Train Epoch 9/200, Batch 0, Loss: 7.5989, Pearson: 0.6243, Spearman: 0.5926
2025-11-14 19:15:36,325 - INFO - Fold 3 Train Epoch 9/200, Batch 10, Loss: 7.4078, Pearson: 0.6261, Spearman: 0.5826
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6151
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6204
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6084
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5999
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6033
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5990
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5968
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6117
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6117
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6095
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6112
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6184
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.4770426750183105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 9 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 8.386393 8.386393]
Sample y_pred values (first sample, first 10 genes):
[0.         0.31084543 1.1097796  0.         2.020937   0.54778516
 0.         0.08619046 0.96353316 6.0518126 ]
y_true  -> mean=2.1696, std=3.5183, min=0.0000, max=12.7954
y_pred  -> mean=1.9781, std=2.1053, min=0.0000, max=13.0552
Batch 0 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6261
2025-11-14 19:15:46,471 - INFO - Fold 3 Train Epoch 9/200, Batch 20, Loss: 7.5320, Pearson: 0.6276, Spearman: 0.5925
2025-11-14 19:15:56,615 - INFO - Fold 3 Train Epoch 9/200, Batch 30, Loss: 7.2461, Pearson: 0.6230, Spearman: 0.5804
2025-11-14 19:16:05,172 - INFO - Fold 3 Train Epoch 9/200, Batch 40, Loss: 7.3892, Pearson: 0.6241, Spearman: 0.5862
2025-11-14 19:16:13,230 - INFO - Fold 3 Train Epoch 9/200, Batch 50, Loss: 7.5269, Pearson: 0.6289, Spearman: 0.5982
2025-11-14 19:16:23,407 - INFO - Fold 3 Train Epoch 9/200, Batch 60, Loss: 7.4574, Pearson: 0.6267, Spearman: 0.5835
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6091
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6156
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6062
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6103
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6224
2025-11-14 19:16:33,552 - INFO - Fold 3 Train Epoch 9/200, Batch 70, Loss: 7.4828, Pearson: 0.6052, Spearman: 0.5694
2025-11-14 19:16:43,701 - INFO - Fold 3 Train Epoch 9/200, Batch 80, Loss: 7.6467, Pearson: 0.6216, Spearman: 0.5810
2025-11-14 19:16:51,829 - INFO - Fold 3 Train Epoch 9/200, Train Loss: 7.4377, Pearson Mean: 0.6193, Spearman Mean: 0.5823
2025-11-14 19:16:51,829 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3977, 'spearman_mean_genewise': 0.3558, 'l1_error_mean': 1.9175, 'l2_errors_mean': 7.4383, 'r2_scores_mean': 0.1695, 'pearson_std': 0.1109, 'l2_error_q1': 4.812, 'l2_error_q2': 7.0323, 'l2_error_q3': 9.829, 'r2_score_q1': 0.0983, 'r2_score_q2': 0.1479, 'r2_score_q3': 0.2099, 'mape_mean': 61.0026, 'mape_std': 18.1276, 'rmse_mean': 2.6734, 'rmse_std': 0.5397}
2025-11-14 19:16:52,113 - INFO - Fold 3 Val Epoch 9/200, Batch 0, Loss: 10.7749, Pearson: 0.4873, Spearman: 0.4995
2025-11-14 19:16:53,788 - INFO - Fold 3 Val Epoch 9/200, Batch 10, Loss: 8.7975, Pearson: 0.5927, Spearman: 0.5694
2025-11-14 19:16:57,402 - INFO - Fold 3 Val Epoch 9/200, Val Loss: 8.6954, Pearson Mean: 0.5526, Spearman Mean: 0.5502
2025-11-14 19:16:57,403 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3111, 'spearman_mean_genewise': 0.2922, 'l1_error_mean': 2.1779, 'l2_errors_mean': 8.7238, 'r2_scores_mean': 0.0978, 'pearson_std': 0.1326, 'l2_error_q1': 6.1156, 'l2_error_q2': 8.6348, 'l2_error_q3': 11.1732, 'r2_score_q1': 0.036, 'r2_score_q2': 0.069, 'r2_score_q3': 0.1254, 'mape_mean': 65.3166, 'mape_std': 19.8677, 'rmse_mean': 2.9067, 'rmse_std': 0.524}
2025-11-14 19:16:57,403 - INFO - Learning rate for epoch 9: 0.0001
2025-11-14 19:16:57,403 - INFO - No improvement in spearman genewise. Patience: 2/30
2025-11-14 19:16:58,366 - INFO - Fold 3 Train Epoch 10/200, Batch 0, Loss: 7.2907, Pearson: 0.6123, Spearman: 0.5762
2025-11-14 19:17:08,506 - INFO - Fold 3 Train Epoch 10/200, Batch 10, Loss: 7.3866, Pearson: 0.6333, Spearman: 0.5891
2025-11-14 19:17:18,657 - INFO - Fold 3 Train Epoch 10/200, Batch 20, Loss: 7.4956, Pearson: 0.6291, Spearman: 0.5863
2025-11-14 19:17:28,791 - INFO - Fold 3 Train Epoch 10/200, Batch 30, Loss: 7.3550, Pearson: 0.6176, Spearman: 0.5842
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6052
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6151
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6091
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6089
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6067
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6001
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6063
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6143
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.438314437866211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 10 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 0.        0.        9.1060915]
Sample y_pred values (first sample, first 10 genes):
[0.10797131 0.39783013 1.3215835  0.24861239 2.333346   1.3707315
 0.46215832 0.8352114  2.415436   7.2864585 ]
y_true  -> mean=1.9135, std=3.4141, min=0.0000, max=12.7151
y_pred  -> mean=1.9687, std=2.1305, min=0.0000, max=14.4356
Batch 0 Pearson correlation: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6044
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6095
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6103
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6148
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6156
2025-11-14 19:17:36,706 - INFO - Fold 3 Train Epoch 10/200, Batch 40, Loss: 7.4157, Pearson: 0.6286, Spearman: 0.5931
2025-11-14 19:17:45,353 - INFO - Fold 3 Train Epoch 10/200, Batch 50, Loss: 7.1228, Pearson: 0.6316, Spearman: 0.5880
2025-11-14 19:17:55,510 - INFO - Fold 3 Train Epoch 10/200, Batch 60, Loss: 7.5788, Pearson: 0.6232, Spearman: 0.5871
2025-11-14 19:18:05,685 - INFO - Fold 3 Train Epoch 10/200, Batch 70, Loss: 7.4847, Pearson: 0.6256, Spearman: 0.5860
2025-11-14 19:18:15,824 - INFO - Fold 3 Train Epoch 10/200, Batch 80, Loss: 7.5758, Pearson: 0.6283, Spearman: 0.5913
2025-11-14 19:18:23,985 - INFO - Fold 3 Train Epoch 10/200, Train Loss: 7.4312, Pearson Mean: 0.6199, Spearman Mean: 0.5835
2025-11-14 19:18:23,985 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3983, 'spearman_mean_genewise': 0.357, 'l1_error_mean': 1.9124, 'l2_errors_mean': 7.4312, 'r2_scores_mean': 0.1701, 'pearson_std': 0.1114, 'l2_error_q1': 4.7995, 'l2_error_q2': 7.0319, 'l2_error_q3': 9.8716, 'r2_score_q1': 0.0995, 'r2_score_q2': 0.1485, 'r2_score_q3': 0.2115, 'mape_mean': 61.1239, 'mape_std': 18.1262, 'rmse_mean': 2.6721, 'rmse_std': 0.5397}
2025-11-14 19:18:24,328 - INFO - Fold 3 Val Epoch 10/200, Batch 0, Loss: 10.6520, Pearson: 0.4881, Spearman: 0.4994
2025-11-14 19:18:26,057 - INFO - Fold 3 Val Epoch 10/200, Batch 10, Loss: 8.4142, Pearson: 0.6015, Spearman: 0.5765
2025-11-14 19:18:29,744 - INFO - Fold 3 Val Epoch 10/200, Val Loss: 8.5757, Pearson Mean: 0.5621, Spearman Mean: 0.5585
2025-11-14 19:18:29,745 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3198, 'spearman_mean_genewise': 0.2976, 'l1_error_mean': 2.1841, 'l2_errors_mean': 8.6044, 'r2_scores_mean': 0.1085, 'pearson_std': 0.1345, 'l2_error_q1': 6.0801, 'l2_error_q2': 8.4963, 'l2_error_q3': 10.8963, 'r2_score_q1': 0.0414, 'r2_score_q2': 0.0762, 'r2_score_q3': 0.1387, 'mape_mean': 62.2915, 'mape_std': 20.7403, 'rmse_mean': 2.8868, 'rmse_std': 0.5201}
2025-11-14 19:18:29,745 - INFO - Learning rate for epoch 10: 0.0001
2025-11-14 19:18:29,745 - INFO - No improvement in spearman genewise. Patience: 3/30
2025-11-14 19:18:30,600 - INFO - Fold 3 Train Epoch 11/200, Batch 0, Loss: 7.5190, Pearson: 0.6248, Spearman: 0.5858
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6113
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6148
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6040
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6103
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6043
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6152
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6032
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6321
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6203
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.431234836578369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 11 =====================
Sample y_true values (first sample, first 10 genes):
[7.2551866 0.        0.        0.        7.2551866 7.2551866 0.
 0.        7.9479804 0.       ]
Sample y_pred values (first sample, first 10 genes):
[0.8222189 1.4204994 3.5787086 0.9499825 5.3570547 4.0973253 0.9499674
 2.1865358 4.5073414 6.080405 ]
y_true  -> mean=2.1366, std=3.5108, min=0.0000, max=12.6288
y_pred  -> mean=2.0720, std=2.2033, min=0.0000, max=14.3142
Batch 0 Pearson correlation: 0.6248
2025-11-14 19:18:40,758 - INFO - Fold 3 Train Epoch 11/200, Batch 10, Loss: 7.3331, Pearson: 0.6270, Spearman: 0.5886
2025-11-14 19:18:50,884 - INFO - Fold 3 Train Epoch 11/200, Batch 20, Loss: 7.1934, Pearson: 0.6247, Spearman: 0.5892
2025-11-14 19:19:01,052 - INFO - Fold 3 Train Epoch 11/200, Batch 30, Loss: 7.3432, Pearson: 0.6154, Spearman: 0.5819
2025-11-14 19:19:08,234 - INFO - Fold 3 Train Epoch 11/200, Batch 40, Loss: 7.4035, Pearson: 0.6245, Spearman: 0.5866
2025-11-14 19:19:17,754 - INFO - Fold 3 Train Epoch 11/200, Batch 50, Loss: 7.1645, Pearson: 0.6286, Spearman: 0.5828
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6165
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6038
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6203
2025-11-14 19:19:27,874 - INFO - Fold 3 Train Epoch 11/200, Batch 60, Loss: 7.3879, Pearson: 0.6233, Spearman: 0.5866
2025-11-14 19:19:38,023 - INFO - Fold 3 Train Epoch 11/200, Batch 70, Loss: 7.2345, Pearson: 0.6285, Spearman: 0.5947
2025-11-14 19:19:48,170 - INFO - Fold 3 Train Epoch 11/200, Batch 80, Loss: 7.1977, Pearson: 0.6296, Spearman: 0.5895
2025-11-14 19:19:56,344 - INFO - Fold 3 Train Epoch 11/200, Train Loss: 7.3760, Pearson Mean: 0.6234, Spearman Mean: 0.5865
2025-11-14 19:19:56,344 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4041, 'spearman_mean_genewise': 0.362, 'l1_error_mean': 1.903, 'l2_errors_mean': 7.3759, 'r2_scores_mean': 0.1751, 'pearson_std': 0.1122, 'l2_error_q1': 4.7825, 'l2_error_q2': 6.9818, 'l2_error_q3': 9.7275, 'r2_score_q1': 0.1029, 'r2_score_q2': 0.1512, 'r2_score_q3': 0.2155, 'mape_mean': 60.5776, 'mape_std': 18.1814, 'rmse_mean': 2.6626, 'rmse_std': 0.5351}
2025-11-14 19:19:56,655 - INFO - Fold 3 Val Epoch 11/200, Batch 0, Loss: 10.7472, Pearson: 0.4734, Spearman: 0.4834
2025-11-14 19:19:58,346 - INFO - Fold 3 Val Epoch 11/200, Batch 10, Loss: 8.4129, Pearson: 0.5984, Spearman: 0.5733
2025-11-14 19:20:02,002 - INFO - Fold 3 Val Epoch 11/200, Val Loss: 8.5760, Pearson Mean: 0.5572, Spearman Mean: 0.5546
2025-11-14 19:20:02,002 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3232, 'spearman_mean_genewise': 0.3025, 'l1_error_mean': 2.2016, 'l2_errors_mean': 8.6085, 'r2_scores_mean': 0.1094, 'pearson_std': 0.1329, 'l2_error_q1': 6.0907, 'l2_error_q2': 8.51, 'l2_error_q3': 10.9069, 'r2_score_q1': 0.043, 'r2_score_q2': 0.0781, 'r2_score_q3': 0.1387, 'mape_mean': 62.2113, 'mape_std': 20.6453, 'rmse_mean': 2.8878, 'rmse_std': 0.519}
2025-11-14 19:20:02,002 - INFO - Learning rate for epoch 11: 0.0001
2025-11-14 19:20:02,060 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_3/best_model.pth
2025-11-14 19:20:02,952 - INFO - Fold 3 Train Epoch 12/200, Batch 0, Loss: 7.2185, Pearson: 0.6168, Spearman: 0.5801
2025-11-14 19:20:13,136 - INFO - Fold 3 Train Epoch 12/200, Batch 10, Loss: 7.6520, Pearson: 0.6212, Spearman: 0.5841
2025-11-14 19:20:23,297 - INFO - Fold 3 Train Epoch 12/200, Batch 20, Loss: 7.2762, Pearson: 0.6355, Spearman: 0.5882
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6064
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6242
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6251
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.375853538513184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 12 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 7.5319123 7.5319123 8.224792 ]
Sample y_pred values (first sample, first 10 genes):
[0.         0.9010313  1.1395674  0.51980555 3.1859741  1.622518
 0.91986644 1.2062418  2.489765   7.1450624 ]
y_true  -> mean=1.9274, std=3.4122, min=0.0000, max=12.4782
y_pred  -> mean=1.9304, std=2.1766, min=0.0000, max=14.3282
Batch 0 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6052
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6240
2025-11-14 19:20:32,792 - INFO - Fold 3 Train Epoch 12/200, Batch 30, Loss: 7.3097, Pearson: 0.6323, Spearman: 0.5936
2025-11-14 19:20:40,067 - INFO - Fold 3 Train Epoch 12/200, Batch 40, Loss: 7.3666, Pearson: 0.6116, Spearman: 0.5714
2025-11-14 19:20:50,104 - INFO - Fold 3 Train Epoch 12/200, Batch 50, Loss: 7.5590, Pearson: 0.6313, Spearman: 0.5925
2025-11-14 19:21:00,274 - INFO - Fold 3 Train Epoch 12/200, Batch 60, Loss: 7.5643, Pearson: 0.6142, Spearman: 0.5864
2025-11-14 19:21:10,429 - INFO - Fold 3 Train Epoch 12/200, Batch 70, Loss: 7.3718, Pearson: 0.6253, Spearman: 0.5956
2025-11-14 19:21:20,584 - INFO - Fold 3 Train Epoch 12/200, Batch 80, Loss: 7.2764, Pearson: 0.6240, Spearman: 0.5819
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6061
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6204
2025-11-14 19:21:28,713 - INFO - Fold 3 Train Epoch 12/200, Train Loss: 7.3524, Pearson Mean: 0.6249, Spearman Mean: 0.5881
2025-11-14 19:21:28,713 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4066, 'spearman_mean_genewise': 0.3644, 'l1_error_mean': 1.8961, 'l2_errors_mean': 7.3521, 'r2_scores_mean': 0.1774, 'pearson_std': 0.1126, 'l2_error_q1': 4.778, 'l2_error_q2': 6.9624, 'l2_error_q3': 9.6943, 'r2_score_q1': 0.1049, 'r2_score_q2': 0.154, 'r2_score_q3': 0.2184, 'mape_mean': 60.4795, 'mape_std': 18.2549, 'rmse_mean': 2.6584, 'rmse_std': 0.5337}
2025-11-14 19:21:29,033 - INFO - Fold 3 Val Epoch 12/200, Batch 0, Loss: 10.5561, Pearson: 0.4915, Spearman: 0.5026
2025-11-14 19:21:30,736 - INFO - Fold 3 Val Epoch 12/200, Batch 10, Loss: 8.3265, Pearson: 0.6071, Spearman: 0.5816
2025-11-14 19:21:34,412 - INFO - Fold 3 Val Epoch 12/200, Val Loss: 8.5065, Pearson Mean: 0.5633, Spearman Mean: 0.5603
2025-11-14 19:21:34,412 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3256, 'spearman_mean_genewise': 0.3067, 'l1_error_mean': 2.1914, 'l2_errors_mean': 8.5378, 'r2_scores_mean': 0.1146, 'pearson_std': 0.1371, 'l2_error_q1': 6.0933, 'l2_error_q2': 8.4281, 'l2_error_q3': 10.8025, 'r2_score_q1': 0.0478, 'r2_score_q2': 0.0807, 'r2_score_q3': 0.1412, 'mape_mean': 62.7836, 'mape_std': 20.7562, 'rmse_mean': 2.8761, 'rmse_std': 0.5157}
2025-11-14 19:21:34,412 - INFO - Learning rate for epoch 12: 0.0001
2025-11-14 19:21:34,472 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_3/best_model.pth
2025-11-14 19:21:35,387 - INFO - Fold 3 Train Epoch 13/200, Batch 0, Loss: 7.2461, Pearson: 0.6271, Spearman: 0.5897
2025-11-14 19:21:45,574 - INFO - Fold 3 Train Epoch 13/200, Batch 10, Loss: 7.3712, Pearson: 0.6307, Spearman: 0.5926
2025-11-14 19:21:55,721 - INFO - Fold 3 Train Epoch 13/200, Batch 20, Loss: 7.0780, Pearson: 0.6324, Spearman: 0.5886
2025-11-14 19:22:04,744 - INFO - Fold 3 Train Epoch 13/200, Batch 30, Loss: 7.1287, Pearson: 0.6320, Spearman: 0.5943
2025-11-14 19:22:12,309 - INFO - Fold 3 Train Epoch 13/200, Batch 40, Loss: 7.1734, Pearson: 0.6251, Spearman: 0.5885
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6299
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6208
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.35211706161499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 13 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.41406083 0.         1.2729092  0.04811667 1.595082   0.73390496
 0.38872296 0.931664   1.9720013  7.298665  ]
y_true  -> mean=2.0350, std=3.4557, min=0.0000, max=12.6644
y_pred  -> mean=2.0521, std=2.1703, min=0.0000, max=13.8138
Batch 0 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6117
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6063
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6313
2025-11-14 19:22:22,461 - INFO - Fold 3 Train Epoch 13/200, Batch 50, Loss: 7.3706, Pearson: 0.6274, Spearman: 0.5892
2025-11-14 19:22:32,621 - INFO - Fold 3 Train Epoch 13/200, Batch 60, Loss: 7.3947, Pearson: 0.6298, Spearman: 0.5903
2025-11-14 19:22:42,776 - INFO - Fold 3 Train Epoch 13/200, Batch 70, Loss: 7.3759, Pearson: 0.6187, Spearman: 0.5863
2025-11-14 19:22:52,904 - INFO - Fold 3 Train Epoch 13/200, Batch 80, Loss: 7.2179, Pearson: 0.6359, Spearman: 0.5909
2025-11-14 19:23:01,052 - INFO - Fold 3 Train Epoch 13/200, Train Loss: 7.3158, Pearson Mean: 0.6273, Spearman Mean: 0.5903
2025-11-14 19:23:01,052 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4106, 'spearman_mean_genewise': 0.3676, 'l1_error_mean': 1.8881, 'l2_errors_mean': 7.3157, 'r2_scores_mean': 0.1808, 'pearson_std': 0.1131, 'l2_error_q1': 4.7419, 'l2_error_q2': 6.9301, 'l2_error_q3': 9.6397, 'r2_score_q1': 0.1074, 'r2_score_q2': 0.1573, 'r2_score_q3': 0.2226, 'mape_mean': 60.3133, 'mape_std': 18.2397, 'rmse_mean': 2.652, 'rmse_std': 0.5314}
2025-11-14 19:23:01,350 - INFO - Fold 3 Val Epoch 13/200, Batch 0, Loss: 10.4082, Pearson: 0.5019, Spearman: 0.5114
2025-11-14 19:23:03,051 - INFO - Fold 3 Val Epoch 13/200, Batch 10, Loss: 8.5565, Pearson: 0.5949, Spearman: 0.5674
2025-11-14 19:23:06,715 - INFO - Fold 3 Val Epoch 13/200, Val Loss: 8.6027, Pearson Mean: 0.5582, Spearman Mean: 0.5554
2025-11-14 19:23:06,715 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3165, 'spearman_mean_genewise': 0.2961, 'l1_error_mean': 2.2115, 'l2_errors_mean': 8.6287, 'r2_scores_mean': 0.1067, 'pearson_std': 0.1343, 'l2_error_q1': 6.0745, 'l2_error_q2': 8.472, 'l2_error_q3': 10.9923, 'r2_score_q1': 0.0399, 'r2_score_q2': 0.0763, 'r2_score_q3': 0.1336, 'mape_mean': 62.0248, 'mape_std': 19.9393, 'rmse_mean': 2.8907, 'rmse_std': 0.5221}
2025-11-14 19:23:06,715 - INFO - Learning rate for epoch 13: 0.0001
2025-11-14 19:23:06,715 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 19:23:07,624 - INFO - Fold 3 Train Epoch 14/200, Batch 0, Loss: 6.9230, Pearson: 0.6369, Spearman: 0.5923
2025-11-14 19:23:17,791 - INFO - Fold 3 Train Epoch 14/200, Batch 10, Loss: 7.3693, Pearson: 0.6326, Spearman: 0.5953
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6204
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6199
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6188
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.3156819343566895
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 14 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.232824 0.       0.       0.       0.       0.
 0.       8.618576]
Sample y_pred values (first sample, first 10 genes):
[0.19182366 0.10997161 0.80970335 0.2833728  2.2243774  0.54764616
 0.28898436 0.18882486 2.7394514  7.3756022 ]
y_true  -> mean=1.9520, std=3.4119, min=0.0000, max=12.4631
y_pred  -> mean=2.0027, std=2.2046, min=0.0000, max=13.0675
Batch 0 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6298
2025-11-14 19:23:27,977 - INFO - Fold 3 Train Epoch 14/200, Batch 20, Loss: 7.1590, Pearson: 0.6484, Spearman: 0.6016
2025-11-14 19:23:36,541 - INFO - Fold 3 Train Epoch 14/200, Batch 30, Loss: 7.4371, Pearson: 0.6250, Spearman: 0.5927
2025-11-14 19:23:44,497 - INFO - Fold 3 Train Epoch 14/200, Batch 40, Loss: 7.4875, Pearson: 0.6231, Spearman: 0.5886
2025-11-14 19:23:54,675 - INFO - Fold 3 Train Epoch 14/200, Batch 50, Loss: 7.3899, Pearson: 0.6223, Spearman: 0.5918
2025-11-14 19:24:04,816 - INFO - Fold 3 Train Epoch 14/200, Batch 60, Loss: 7.5232, Pearson: 0.6336, Spearman: 0.5971
2025-11-14 19:24:14,998 - INFO - Fold 3 Train Epoch 14/200, Batch 70, Loss: 7.5074, Pearson: 0.6187, Spearman: 0.5909
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6207
2025-11-14 19:24:25,140 - INFO - Fold 3 Train Epoch 14/200, Batch 80, Loss: 7.2235, Pearson: 0.6201, Spearman: 0.5905
2025-11-14 19:24:33,323 - INFO - Fold 3 Train Epoch 14/200, Train Loss: 7.3283, Pearson Mean: 0.6266, Spearman Mean: 0.5903
2025-11-14 19:24:33,323 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4097, 'spearman_mean_genewise': 0.3668, 'l1_error_mean': 1.891, 'l2_errors_mean': 7.3282, 'r2_scores_mean': 0.1798, 'pearson_std': 0.1121, 'l2_error_q1': 4.7518, 'l2_error_q2': 6.9491, 'l2_error_q3': 9.6411, 'r2_score_q1': 0.1074, 'r2_score_q2': 0.1556, 'r2_score_q3': 0.2204, 'mape_mean': 60.2478, 'mape_std': 18.2617, 'rmse_mean': 2.6542, 'rmse_std': 0.5323}
2025-11-14 19:24:33,610 - INFO - Fold 3 Val Epoch 14/200, Batch 0, Loss: 10.8512, Pearson: 0.4859, Spearman: 0.4965
2025-11-14 19:24:35,283 - INFO - Fold 3 Val Epoch 14/200, Batch 10, Loss: 8.3997, Pearson: 0.6071, Spearman: 0.5798
2025-11-14 19:24:38,912 - INFO - Fold 3 Val Epoch 14/200, Val Loss: 8.6235, Pearson Mean: 0.5611, Spearman Mean: 0.5585
2025-11-14 19:24:38,913 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3144, 'spearman_mean_genewise': 0.2933, 'l1_error_mean': 2.1606, 'l2_errors_mean': 8.656, 'r2_scores_mean': 0.103, 'pearson_std': 0.1372, 'l2_error_q1': 6.112, 'l2_error_q2': 8.577, 'l2_error_q3': 11.0585, 'r2_score_q1': 0.0374, 'r2_score_q2': 0.0684, 'r2_score_q3': 0.1336, 'mape_mean': 64.5585, 'mape_std': 20.492, 'rmse_mean': 2.8956, 'rmse_std': 0.5211}
2025-11-14 19:24:38,913 - INFO - Learning rate for epoch 14: 0.0001
2025-11-14 19:24:38,913 - INFO - No improvement in spearman genewise. Patience: 2/30
2025-11-14 19:24:39,899 - INFO - Fold 3 Train Epoch 15/200, Batch 0, Loss: 7.2735, Pearson: 0.6316, Spearman: 0.5949
2025-11-14 19:24:50,022 - INFO - Fold 3 Train Epoch 15/200, Batch 10, Loss: 7.1404, Pearson: 0.6325, Spearman: 0.5961
2025-11-14 19:25:00,163 - INFO - Fold 3 Train Epoch 15/200, Batch 20, Loss: 7.2330, Pearson: 0.6246, Spearman: 0.5930
2025-11-14 19:25:08,112 - INFO - Fold 3 Train Epoch 15/200, Batch 30, Loss: 6.9900, Pearson: 0.6332, Spearman: 0.5975
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6148
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6437
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.328151226043701
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 15 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.928379  0.        7.2355924 7.928379  0.
 0.        0.        0.       ]
Sample y_pred values (first sample, first 10 genes):
[0.8722602 1.7823179 4.2408714 0.7247472 4.725823  4.064459  0.781889
 2.1344805 4.1786413 6.3258023]
y_true  -> mean=2.0670, std=3.4772, min=0.0000, max=13.8155
y_pred  -> mean=1.9908, std=2.1756, min=0.0000, max=14.2208
Batch 0 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6118
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6246
2025-11-14 19:25:14,278 - INFO - Fold 3 Train Epoch 15/200, Batch 40, Loss: 7.1598, Pearson: 0.6384, Spearman: 0.5971
2025-11-14 19:25:20,415 - INFO - Fold 3 Train Epoch 15/200, Batch 50, Loss: 7.3993, Pearson: 0.6307, Spearman: 0.5888
2025-11-14 19:25:26,537 - INFO - Fold 3 Train Epoch 15/200, Batch 60, Loss: 7.1988, Pearson: 0.6321, Spearman: 0.5938
2025-11-14 19:25:35,434 - INFO - Fold 3 Train Epoch 15/200, Batch 70, Loss: 7.1639, Pearson: 0.6328, Spearman: 0.5924
2025-11-14 19:25:45,113 - INFO - Fold 3 Train Epoch 15/200, Batch 80, Loss: 7.1062, Pearson: 0.6460, Spearman: 0.5971
2025-11-14 19:25:52,824 - INFO - Fold 3 Train Epoch 15/200, Train Loss: 7.2734, Pearson Mean: 0.6301, Spearman Mean: 0.5934
2025-11-14 19:25:52,824 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4159, 'spearman_mean_genewise': 0.3718, 'l1_error_mean': 1.8751, 'l2_errors_mean': 7.2726, 'r2_scores_mean': 0.1851, 'pearson_std': 0.1125, 'l2_error_q1': 4.714, 'l2_error_q2': 6.8862, 'l2_error_q3': 9.5788, 'r2_score_q1': 0.1115, 'r2_score_q2': 0.1606, 'r2_score_q3': 0.2261, 'mape_mean': 60.0413, 'mape_std': 18.2844, 'rmse_mean': 2.6445, 'rmse_std': 0.5283}
2025-11-14 19:25:53,188 - INFO - Fold 3 Val Epoch 15/200, Batch 0, Loss: 10.4811, Pearson: 0.4924, Spearman: 0.5020
2025-11-14 19:25:54,895 - INFO - Fold 3 Val Epoch 15/200, Batch 10, Loss: 8.2738, Pearson: 0.6062, Spearman: 0.5799
2025-11-14 19:25:58,628 - INFO - Fold 3 Val Epoch 15/200, Val Loss: 8.4818, Pearson Mean: 0.5649, Spearman Mean: 0.5615
2025-11-14 19:25:58,629 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3316, 'spearman_mean_genewise': 0.3121, 'l1_error_mean': 2.179, 'l2_errors_mean': 8.5122, 'r2_scores_mean': 0.1173, 'pearson_std': 0.137, 'l2_error_q1': 6.0387, 'l2_error_q2': 8.4131, 'l2_error_q3': 10.7464, 'r2_score_q1': 0.0452, 'r2_score_q2': 0.0817, 'r2_score_q3': 0.1481, 'mape_mean': 60.8341, 'mape_std': 21.0225, 'rmse_mean': 2.8714, 'rmse_std': 0.5169}
2025-11-14 19:25:58,629 - INFO - Learning rate for epoch 15: 0.0001
2025-11-14 19:25:58,689 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_3/best_model.pth
2025-11-14 19:25:59,642 - INFO - Fold 3 Train Epoch 16/200, Batch 0, Loss: 7.3005, Pearson: 0.6179, Spearman: 0.5919
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6331
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6276
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.272576808929443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 16 =====================
Sample y_true values (first sample, first 10 genes):
[0.       8.343478 0.       0.       0.       8.343478 0.       0.
 8.343478 0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.5053035  1.0468673  1.3284805  0.07584076 3.1739237  1.0498637
 0.4991929  1.3582188  2.4722495  6.157774  ]
y_true  -> mean=1.9353, std=3.4344, min=0.0000, max=12.6213
y_pred  -> mean=2.0039, std=2.1830, min=0.0000, max=13.6445
Batch 0 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6368
2025-11-14 19:26:09,787 - INFO - Fold 3 Train Epoch 16/200, Batch 10, Loss: 7.6593, Pearson: 0.6246, Spearman: 0.5941
2025-11-14 19:26:19,887 - INFO - Fold 3 Train Epoch 16/200, Batch 20, Loss: 7.1439, Pearson: 0.6281, Spearman: 0.5902
2025-11-14 19:26:29,796 - INFO - Fold 3 Train Epoch 16/200, Batch 30, Loss: 7.2205, Pearson: 0.6331, Spearman: 0.6019
2025-11-14 19:26:39,242 - INFO - Fold 3 Train Epoch 16/200, Batch 40, Loss: 7.2772, Pearson: 0.6411, Spearman: 0.6016
2025-11-14 19:26:48,719 - INFO - Fold 3 Train Epoch 16/200, Batch 50, Loss: 7.3451, Pearson: 0.6253, Spearman: 0.5924
2025-11-14 19:26:55,844 - INFO - Fold 3 Train Epoch 16/200, Batch 60, Loss: 7.4245, Pearson: 0.6355, Spearman: 0.5988
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6286
2025-11-14 19:27:05,266 - INFO - Fold 3 Train Epoch 16/200, Batch 70, Loss: 7.3728, Pearson: 0.6277, Spearman: 0.5924
2025-11-14 19:27:15,375 - INFO - Fold 3 Train Epoch 16/200, Batch 80, Loss: 7.2926, Pearson: 0.6342, Spearman: 0.5951
2025-11-14 19:27:23,524 - INFO - Fold 3 Train Epoch 16/200, Train Loss: 7.2372, Pearson Mean: 0.6325, Spearman Mean: 0.5955
2025-11-14 19:27:23,524 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4199, 'spearman_mean_genewise': 0.3755, 'l1_error_mean': 1.8696, 'l2_errors_mean': 7.237, 'r2_scores_mean': 0.1885, 'pearson_std': 0.1127, 'l2_error_q1': 4.7098, 'l2_error_q2': 6.8445, 'l2_error_q3': 9.5314, 'r2_score_q1': 0.1144, 'r2_score_q2': 0.1646, 'r2_score_q3': 0.2308, 'mape_mean': 59.7533, 'mape_std': 18.2596, 'rmse_mean': 2.6383, 'rmse_std': 0.5257}
2025-11-14 19:27:23,914 - INFO - Fold 3 Val Epoch 16/200, Batch 0, Loss: 10.4473, Pearson: 0.4980, Spearman: 0.5071
2025-11-14 19:27:25,629 - INFO - Fold 3 Val Epoch 16/200, Batch 10, Loss: 8.2653, Pearson: 0.6102, Spearman: 0.5839
2025-11-14 19:27:29,339 - INFO - Fold 3 Val Epoch 16/200, Val Loss: 8.4905, Pearson Mean: 0.5654, Spearman Mean: 0.5617
2025-11-14 19:27:29,340 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3272, 'spearman_mean_genewise': 0.3069, 'l1_error_mean': 2.1688, 'l2_errors_mean': 8.5217, 'r2_scores_mean': 0.1154, 'pearson_std': 0.1375, 'l2_error_q1': 6.0317, 'l2_error_q2': 8.4097, 'l2_error_q3': 10.7418, 'r2_score_q1': 0.0444, 'r2_score_q2': 0.0817, 'r2_score_q3': 0.1464, 'mape_mean': 62.1592, 'mape_std': 20.3825, 'rmse_mean': 2.8735, 'rmse_std': 0.5144}
2025-11-14 19:27:29,340 - INFO - Learning rate for epoch 16: 0.0001
2025-11-14 19:27:29,340 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 19:27:30,175 - INFO - Fold 3 Train Epoch 17/200, Batch 0, Loss: 7.1830, Pearson: 0.6428, Spearman: 0.5964
2025-11-14 19:27:39,986 - INFO - Fold 3 Train Epoch 17/200, Batch 10, Loss: 7.0973, Pearson: 0.6324, Spearman: 0.5977
2025-11-14 19:27:50,127 - INFO - Fold 3 Train Epoch 17/200, Batch 20, Loss: 7.3462, Pearson: 0.6306, Spearman: 0.5993
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6412
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6271
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.2369890213012695
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 17 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.648471 0.       0.       0.       0.       0.
 0.       9.257527]
Sample y_pred values (first sample, first 10 genes):
[0.63620186 0.35394424 0.97203994 1.6297473  2.7386365  1.8295274
 0.6673622  2.2907233  2.6909368  6.123751  ]
y_true  -> mean=2.1331, std=3.4942, min=0.0000, max=12.3882
y_pred  -> mean=2.0059, std=2.2010, min=0.0000, max=14.0857
Batch 0 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6379
2025-11-14 19:28:00,297 - INFO - Fold 3 Train Epoch 17/200, Batch 30, Loss: 7.1118, Pearson: 0.6414, Spearman: 0.5992
2025-11-14 19:28:10,411 - INFO - Fold 3 Train Epoch 17/200, Batch 40, Loss: 7.0682, Pearson: 0.6420, Spearman: 0.5968
2025-11-14 19:28:20,547 - INFO - Fold 3 Train Epoch 17/200, Batch 50, Loss: 7.3415, Pearson: 0.6249, Spearman: 0.5925
2025-11-14 19:28:27,703 - INFO - Fold 3 Train Epoch 17/200, Batch 60, Loss: 7.3819, Pearson: 0.6203, Spearman: 0.5881
2025-11-14 19:28:37,379 - INFO - Fold 3 Train Epoch 17/200, Batch 70, Loss: 7.0844, Pearson: 0.6432, Spearman: 0.5985
2025-11-14 19:28:47,539 - INFO - Fold 3 Train Epoch 17/200, Batch 80, Loss: 7.4830, Pearson: 0.6254, Spearman: 0.5940
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6378
2025-11-14 19:28:55,686 - INFO - Fold 3 Train Epoch 17/200, Train Loss: 7.2090, Pearson Mean: 0.6343, Spearman Mean: 0.5976
2025-11-14 19:28:55,686 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4233, 'spearman_mean_genewise': 0.3783, 'l1_error_mean': 1.8622, 'l2_errors_mean': 7.209, 'r2_scores_mean': 0.1913, 'pearson_std': 0.1125, 'l2_error_q1': 4.6954, 'l2_error_q2': 6.8387, 'l2_error_q3': 9.4778, 'r2_score_q1': 0.1173, 'r2_score_q2': 0.1668, 'r2_score_q3': 0.2332, 'mape_mean': 59.6787, 'mape_std': 18.2328, 'rmse_mean': 2.6333, 'rmse_std': 0.5242}
2025-11-14 19:28:56,050 - INFO - Fold 3 Val Epoch 17/200, Batch 0, Loss: 10.7506, Pearson: 0.4927, Spearman: 0.5045
2025-11-14 19:28:57,815 - INFO - Fold 3 Val Epoch 17/200, Batch 10, Loss: 8.4244, Pearson: 0.6025, Spearman: 0.5781
2025-11-14 19:29:01,441 - INFO - Fold 3 Val Epoch 17/200, Val Loss: 8.6210, Pearson Mean: 0.5591, Spearman Mean: 0.5563
2025-11-14 19:29:01,441 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.314, 'spearman_mean_genewise': 0.2929, 'l1_error_mean': 2.1681, 'l2_errors_mean': 8.6492, 'r2_scores_mean': 0.1038, 'pearson_std': 0.138, 'l2_error_q1': 6.1076, 'l2_error_q2': 8.5096, 'l2_error_q3': 11.0108, 'r2_score_q1': 0.0363, 'r2_score_q2': 0.0716, 'r2_score_q3': 0.1325, 'mape_mean': 63.8888, 'mape_std': 20.635, 'rmse_mean': 2.8942, 'rmse_std': 0.5224}
2025-11-14 19:29:01,441 - INFO - Learning rate for epoch 17: 0.0001
2025-11-14 19:29:01,441 - INFO - No improvement in spearman genewise. Patience: 2/30
2025-11-14 19:29:02,470 - INFO - Fold 3 Train Epoch 18/200, Batch 0, Loss: 7.3285, Pearson: 0.6409, Spearman: 0.6034
2025-11-14 19:29:12,501 - INFO - Fold 3 Train Epoch 18/200, Batch 10, Loss: 7.2356, Pearson: 0.6428, Spearman: 0.6013
2025-11-14 19:29:22,632 - INFO - Fold 3 Train Epoch 18/200, Batch 20, Loss: 7.0965, Pearson: 0.6412, Spearman: 0.5988
2025-11-14 19:29:32,801 - INFO - Fold 3 Train Epoch 18/200, Batch 30, Loss: 7.0785, Pearson: 0.6393, Spearman: 0.6006
2025-11-14 19:29:42,931 - INFO - Fold 3 Train Epoch 18/200, Batch 40, Loss: 7.1082, Pearson: 0.6330, Spearman: 0.5981
2025-11-14 19:29:53,109 - INFO - Fold 3 Train Epoch 18/200, Batch 50, Loss: 7.2188, Pearson: 0.6298, Spearman: 0.5969
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6280
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.209029197692871
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 18 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.1975203 7.1975203 7.1975203 0.        0.
 0.        8.295633  7.1975203]
Sample y_pred values (first sample, first 10 genes):
[1.2263322 0.4377191 2.2654414 2.4463146 3.3204265 2.0950356 1.074738
 3.6326616 3.6679544 6.8360553]
y_true  -> mean=2.1717, std=3.5211, min=0.0000, max=12.7169
y_pred  -> mean=2.0244, std=2.2200, min=0.0000, max=12.7725
Batch 0 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6369
2025-11-14 19:30:00,315 - INFO - Fold 3 Train Epoch 18/200, Batch 60, Loss: 7.2193, Pearson: 0.6487, Spearman: 0.6078
2025-11-14 19:30:09,312 - INFO - Fold 3 Train Epoch 18/200, Batch 70, Loss: 7.5416, Pearson: 0.6189, Spearman: 0.5924
2025-11-14 19:30:19,462 - INFO - Fold 3 Train Epoch 18/200, Batch 80, Loss: 7.0260, Pearson: 0.6370, Spearman: 0.5977
2025-11-14 19:30:27,607 - INFO - Fold 3 Train Epoch 18/200, Train Loss: 7.1798, Pearson Mean: 0.6364, Spearman Mean: 0.5996
2025-11-14 19:30:27,607 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4268, 'spearman_mean_genewise': 0.3814, 'l1_error_mean': 1.8552, 'l2_errors_mean': 7.178, 'r2_scores_mean': 0.1944, 'pearson_std': 0.1123, 'l2_error_q1': 4.6853, 'l2_error_q2': 6.7915, 'l2_error_q3': 9.4242, 'r2_score_q1': 0.1189, 'r2_score_q2': 0.1703, 'r2_score_q3': 0.2365, 'mape_mean': 59.5908, 'mape_std': 18.2571, 'rmse_mean': 2.6278, 'rmse_std': 0.5222}
2025-11-14 19:30:27,966 - INFO - Fold 3 Val Epoch 18/200, Batch 0, Loss: 10.5616, Pearson: 0.4966, Spearman: 0.5077
2025-11-14 19:30:29,735 - INFO - Fold 3 Val Epoch 18/200, Batch 10, Loss: 8.2503, Pearson: 0.6113, Spearman: 0.5841
2025-11-14 19:30:33,378 - INFO - Fold 3 Val Epoch 18/200, Val Loss: 8.5224, Pearson Mean: 0.5638, Spearman Mean: 0.5600
2025-11-14 19:30:33,378 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3235, 'spearman_mean_genewise': 0.3031, 'l1_error_mean': 2.1674, 'l2_errors_mean': 8.551, 'r2_scores_mean': 0.1128, 'pearson_std': 0.1381, 'l2_error_q1': 6.071, 'l2_error_q2': 8.4993, 'l2_error_q3': 10.8047, 'r2_score_q1': 0.0435, 'r2_score_q2': 0.0798, 'r2_score_q3': 0.1422, 'mape_mean': 62.6519, 'mape_std': 20.564, 'rmse_mean': 2.8783, 'rmse_std': 0.5161}
2025-11-14 19:30:33,379 - INFO - Learning rate for epoch 18: 0.0001
2025-11-14 19:30:33,379 - INFO - No improvement in spearman genewise. Patience: 3/30
2025-11-14 19:30:34,361 - INFO - Fold 3 Train Epoch 19/200, Batch 0, Loss: 7.1612, Pearson: 0.6283, Spearman: 0.5908
2025-11-14 19:30:44,512 - INFO - Fold 3 Train Epoch 19/200, Batch 10, Loss: 7.2622, Pearson: 0.6254, Spearman: 0.5963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6447
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6321
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.178020477294922
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 19 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.4075885 0.        7.4075885 0.        0.
 7.4075885 0.        0.       ]
Sample y_pred values (first sample, first 10 genes):
[0.80340624 0.         1.3433027  1.9280615  2.6639788  1.8557899
 1.3460411  2.7857552  2.9644375  5.751445  ]
y_true  -> mean=1.9716, std=3.4384, min=0.0000, max=12.8039
y_pred  -> mean=2.0311, std=2.2006, min=0.0000, max=13.4474
Batch 0 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6375
2025-11-14 19:30:54,683 - INFO - Fold 3 Train Epoch 19/200, Batch 20, Loss: 7.0528, Pearson: 0.6428, Spearman: 0.6006
2025-11-14 19:31:04,853 - INFO - Fold 3 Train Epoch 19/200, Batch 30, Loss: 7.2381, Pearson: 0.6390, Spearman: 0.6062
2025-11-14 19:31:14,985 - INFO - Fold 3 Train Epoch 19/200, Batch 40, Loss: 6.9896, Pearson: 0.6434, Spearman: 0.6017
2025-11-14 19:31:25,122 - INFO - Fold 3 Train Epoch 19/200, Batch 50, Loss: 7.0830, Pearson: 0.6428, Spearman: 0.5971
2025-11-14 19:31:32,836 - INFO - Fold 3 Train Epoch 19/200, Batch 60, Loss: 7.1375, Pearson: 0.6319, Spearman: 0.5949
2025-11-14 19:31:41,760 - INFO - Fold 3 Train Epoch 19/200, Batch 70, Loss: 7.0558, Pearson: 0.6250, Spearman: 0.5970
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6308
2025-11-14 19:31:51,914 - INFO - Fold 3 Train Epoch 19/200, Batch 80, Loss: 7.2859, Pearson: 0.6428, Spearman: 0.6071
2025-11-14 19:32:00,058 - INFO - Fold 3 Train Epoch 19/200, Train Loss: 7.1476, Pearson Mean: 0.6383, Spearman Mean: 0.6016
2025-11-14 19:32:00,059 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4302, 'spearman_mean_genewise': 0.3842, 'l1_error_mean': 1.8566, 'l2_errors_mean': 7.1478, 'r2_scores_mean': 0.1974, 'pearson_std': 0.1125, 'l2_error_q1': 4.6591, 'l2_error_q2': 6.7649, 'l2_error_q3': 9.3553, 'r2_score_q1': 0.1216, 'r2_score_q2': 0.173, 'r2_score_q3': 0.2394, 'mape_mean': 59.2152, 'mape_std': 18.2276, 'rmse_mean': 2.6222, 'rmse_std': 0.5212}
2025-11-14 19:32:00,431 - INFO - Fold 3 Val Epoch 19/200, Batch 0, Loss: 10.4707, Pearson: 0.4954, Spearman: 0.5066
2025-11-14 19:32:02,225 - INFO - Fold 3 Val Epoch 19/200, Batch 10, Loss: 8.3313, Pearson: 0.6054, Spearman: 0.5796
2025-11-14 19:32:05,870 - INFO - Fold 3 Val Epoch 19/200, Val Loss: 8.5431, Pearson Mean: 0.5619, Spearman Mean: 0.5578
2025-11-14 19:32:05,870 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3195, 'spearman_mean_genewise': 0.3019, 'l1_error_mean': 2.1875, 'l2_errors_mean': 8.5719, 'r2_scores_mean': 0.1102, 'pearson_std': 0.1392, 'l2_error_q1': 6.1007, 'l2_error_q2': 8.4497, 'l2_error_q3': 10.8524, 'r2_score_q1': 0.0406, 'r2_score_q2': 0.0783, 'r2_score_q3': 0.1404, 'mape_mean': 61.9622, 'mape_std': 20.614, 'rmse_mean': 2.8822, 'rmse_std': 0.5146}
2025-11-14 19:32:05,870 - INFO - Learning rate for epoch 19: 0.0001
2025-11-14 19:32:05,870 - INFO - No improvement in spearman genewise. Patience: 4/30
2025-11-14 19:32:06,895 - INFO - Fold 3 Train Epoch 20/200, Batch 0, Loss: 6.9330, Pearson: 0.6386, Spearman: 0.6014
2025-11-14 19:32:16,950 - INFO - Fold 3 Train Epoch 20/200, Batch 10, Loss: 6.8121, Pearson: 0.6494, Spearman: 0.6037
2025-11-14 19:32:27,067 - INFO - Fold 3 Train Epoch 20/200, Batch 20, Loss: 7.3795, Pearson: 0.6480, Spearman: 0.6081
2025-11-14 19:32:37,215 - INFO - Fold 3 Train Epoch 20/200, Batch 30, Loss: 7.2236, Pearson: 0.6428, Spearman: 0.6037
2025-11-14 19:32:47,354 - INFO - Fold 3 Train Epoch 20/200, Batch 40, Loss: 7.0600, Pearson: 0.6562, Spearman: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6313
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6458
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.147779941558838
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 20 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       9.358347]
Sample y_pred values (first sample, first 10 genes):
[0.31085774 0.         1.1716491  0.12519667 1.781091   0.68057615
 0.4014728  0.71089786 1.7759037  7.866829  ]
y_true  -> mean=1.9191, std=3.4197, min=0.0000, max=12.5390
y_pred  -> mean=2.0062, std=2.2043, min=0.0000, max=13.7152
Batch 0 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6447
2025-11-14 19:32:57,511 - INFO - Fold 3 Train Epoch 20/200, Batch 50, Loss: 6.8506, Pearson: 0.6490, Spearman: 0.6109
2025-11-14 19:33:05,211 - INFO - Fold 3 Train Epoch 20/200, Batch 60, Loss: 7.3241, Pearson: 0.6409, Spearman: 0.6085
2025-11-14 19:33:14,280 - INFO - Fold 3 Train Epoch 20/200, Batch 70, Loss: 7.1879, Pearson: 0.6513, Spearman: 0.6067
2025-11-14 19:33:24,415 - INFO - Fold 3 Train Epoch 20/200, Batch 80, Loss: 6.9744, Pearson: 0.6397, Spearman: 0.6043
2025-11-14 19:33:32,574 - INFO - Fold 3 Train Epoch 20/200, Train Loss: 7.1010, Pearson Mean: 0.6413, Spearman Mean: 0.6047
2025-11-14 19:33:32,574 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4356, 'spearman_mean_genewise': 0.3889, 'l1_error_mean': 1.844, 'l2_errors_mean': 7.1013, 'r2_scores_mean': 0.202, 'pearson_std': 0.1124, 'l2_error_q1': 4.6256, 'l2_error_q2': 6.7306, 'l2_error_q3': 9.3145, 'r2_score_q1': 0.1258, 'r2_score_q2': 0.1769, 'r2_score_q3': 0.2438, 'mape_mean': 59.128, 'mape_std': 18.2358, 'rmse_mean': 2.6139, 'rmse_std': 0.5187}
2025-11-14 19:33:32,909 - INFO - Fold 3 Val Epoch 20/200, Batch 0, Loss: 10.6130, Pearson: 0.4915, Spearman: 0.5013
2025-11-14 19:33:34,623 - INFO - Fold 3 Val Epoch 20/200, Batch 10, Loss: 8.3985, Pearson: 0.6061, Spearman: 0.5803
2025-11-14 19:33:38,294 - INFO - Fold 3 Val Epoch 20/200, Val Loss: 8.5622, Pearson Mean: 0.5628, Spearman Mean: 0.5590
2025-11-14 19:33:38,294 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3196, 'spearman_mean_genewise': 0.3014, 'l1_error_mean': 2.1571, 'l2_errors_mean': 8.5936, 'r2_scores_mean': 0.1083, 'pearson_std': 0.14, 'l2_error_q1': 6.0858, 'l2_error_q2': 8.4675, 'l2_error_q3': 10.8299, 'r2_score_q1': 0.0378, 'r2_score_q2': 0.073, 'r2_score_q3': 0.1368, 'mape_mean': 63.2388, 'mape_std': 21.1382, 'rmse_mean': 2.8855, 'rmse_std': 0.5173}
2025-11-14 19:33:38,294 - INFO - Learning rate for epoch 20: 0.0001
2025-11-14 19:33:38,294 - INFO - No improvement in spearman genewise. Patience: 5/30
2025-11-14 19:33:39,215 - INFO - Fold 3 Train Epoch 21/200, Batch 0, Loss: 7.0035, Pearson: 0.6517, Spearman: 0.6081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6398
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6339
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.101343631744385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 21 =====================
Sample y_true values (first sample, first 10 genes):
[0.       8.025877 0.       0.       7.333057 8.718861 0.       0.
 7.333057 8.025877]
Sample y_pred values (first sample, first 10 genes):
[1.2082762  2.567408   3.7291656  0.79508483 4.815675   4.8889885
 1.1688021  2.223877   3.88233    5.963527  ]
y_true  -> mean=2.1273, std=3.4876, min=0.0000, max=12.7075
y_pred  -> mean=2.0471, std=2.2618, min=0.0000, max=13.5505
Batch 0 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6370
2025-11-14 19:33:49,378 - INFO - Fold 3 Train Epoch 21/200, Batch 10, Loss: 6.9064, Pearson: 0.6296, Spearman: 0.5955
2025-11-14 19:33:59,532 - INFO - Fold 3 Train Epoch 21/200, Batch 20, Loss: 6.9280, Pearson: 0.6469, Spearman: 0.6073
2025-11-14 19:34:09,690 - INFO - Fold 3 Train Epoch 21/200, Batch 30, Loss: 7.0947, Pearson: 0.6487, Spearman: 0.6100
2025-11-14 19:34:19,840 - INFO - Fold 3 Train Epoch 21/200, Batch 40, Loss: 7.0497, Pearson: 0.6430, Spearman: 0.6139
2025-11-14 19:34:29,994 - INFO - Fold 3 Train Epoch 21/200, Batch 50, Loss: 7.1373, Pearson: 0.6471, Spearman: 0.6123
2025-11-14 19:34:37,670 - INFO - Fold 3 Train Epoch 21/200, Batch 60, Loss: 7.0843, Pearson: 0.6391, Spearman: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6508
2025-11-14 19:34:46,277 - INFO - Fold 3 Train Epoch 21/200, Batch 70, Loss: 7.2342, Pearson: 0.6304, Spearman: 0.6020
2025-11-14 19:34:56,427 - INFO - Fold 3 Train Epoch 21/200, Batch 80, Loss: 7.1389, Pearson: 0.6498, Spearman: 0.6132
2025-11-14 19:35:04,598 - INFO - Fold 3 Train Epoch 21/200, Train Loss: 7.0780, Pearson Mean: 0.6429, Spearman Mean: 0.6068
2025-11-14 19:35:04,598 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4388, 'spearman_mean_genewise': 0.3918, 'l1_error_mean': 1.8418, 'l2_errors_mean': 7.077, 'r2_scores_mean': 0.2046, 'pearson_std': 0.1115, 'l2_error_q1': 4.622, 'l2_error_q2': 6.7248, 'l2_error_q3': 9.2637, 'r2_score_q1': 0.1284, 'r2_score_q2': 0.1801, 'r2_score_q3': 0.2463, 'mape_mean': 59.0463, 'mape_std': 18.1961, 'rmse_mean': 2.6095, 'rmse_std': 0.5171}
2025-11-14 19:35:04,979 - INFO - Fold 3 Val Epoch 21/200, Batch 0, Loss: 10.5185, Pearson: 0.4908, Spearman: 0.4995
2025-11-14 19:35:06,759 - INFO - Fold 3 Val Epoch 21/200, Batch 10, Loss: 8.3343, Pearson: 0.6061, Spearman: 0.5803
2025-11-14 19:35:10,430 - INFO - Fold 3 Val Epoch 21/200, Val Loss: 8.5174, Pearson Mean: 0.5620, Spearman Mean: 0.5574
2025-11-14 19:35:10,431 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.325, 'spearman_mean_genewise': 0.3068, 'l1_error_mean': 2.1772, 'l2_errors_mean': 8.5481, 'r2_scores_mean': 0.1127, 'pearson_std': 0.1377, 'l2_error_q1': 6.0754, 'l2_error_q2': 8.4315, 'l2_error_q3': 10.8285, 'r2_score_q1': 0.0427, 'r2_score_q2': 0.0789, 'r2_score_q3': 0.1414, 'mape_mean': 61.8405, 'mape_std': 20.5518, 'rmse_mean': 2.8782, 'rmse_std': 0.514}
2025-11-14 19:35:10,431 - INFO - Learning rate for epoch 21: 1e-05
2025-11-14 19:35:10,431 - INFO - No improvement in spearman genewise. Patience: 6/30
2025-11-14 19:35:11,421 - INFO - Fold 3 Train Epoch 22/200, Batch 0, Loss: 7.0837, Pearson: 0.6383, Spearman: 0.6092
2025-11-14 19:35:21,462 - INFO - Fold 3 Train Epoch 22/200, Batch 10, Loss: 6.8129, Pearson: 0.6511, Spearman: 0.6152
2025-11-14 19:35:31,614 - INFO - Fold 3 Train Epoch 22/200, Batch 20, Loss: 7.0704, Pearson: 0.6400, Spearman: 0.5980
2025-11-14 19:35:41,750 - INFO - Fold 3 Train Epoch 22/200, Batch 30, Loss: 6.9261, Pearson: 0.6490, Spearman: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6457
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6360
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.076986789703369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 22 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.297041 0.       7.297041 7.297041 0.       7.297041
 7.297041 8.905937]
Sample y_pred values (first sample, first 10 genes):
[1.0978508  0.85619485 4.940648   0.2984441  6.0930157  3.4389486
 1.0045657  2.196367   6.0945144  6.910348  ]
y_true  -> mean=1.9945, std=3.4572, min=0.0000, max=12.5810
y_pred  -> mean=2.0251, std=2.1835, min=0.0000, max=13.1032
Batch 0 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6452
2025-11-14 19:35:51,899 - INFO - Fold 3 Train Epoch 22/200, Batch 40, Loss: 7.0257, Pearson: 0.6378, Spearman: 0.6048
2025-11-14 19:36:02,080 - INFO - Fold 3 Train Epoch 22/200, Batch 50, Loss: 6.9590, Pearson: 0.6475, Spearman: 0.6129
2025-11-14 19:36:10,158 - INFO - Fold 3 Train Epoch 22/200, Batch 60, Loss: 7.0526, Pearson: 0.6498, Spearman: 0.6191
2025-11-14 19:36:18,101 - INFO - Fold 3 Train Epoch 22/200, Batch 70, Loss: 7.0655, Pearson: 0.6449, Spearman: 0.6080
2025-11-14 19:36:28,260 - INFO - Fold 3 Train Epoch 22/200, Batch 80, Loss: 7.0547, Pearson: 0.6504, Spearman: 0.6150
2025-11-14 19:36:36,408 - INFO - Fold 3 Train Epoch 22/200, Train Loss: 7.0178, Pearson Mean: 0.6466, Spearman Mean: 0.6105
2025-11-14 19:36:36,408 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4463, 'spearman_mean_genewise': 0.3982, 'l1_error_mean': 1.8315, 'l2_errors_mean': 7.0181, 'r2_scores_mean': 0.2108, 'pearson_std': 0.1103, 'l2_error_q1': 4.5785, 'l2_error_q2': 6.6754, 'l2_error_q3': 9.1757, 'r2_score_q1': 0.1355, 'r2_score_q2': 0.1866, 'r2_score_q3': 0.2527, 'mape_mean': 58.7095, 'mape_std': 18.1392, 'rmse_mean': 2.5988, 'rmse_std': 0.514}
2025-11-14 19:36:36,749 - INFO - Fold 3 Val Epoch 22/200, Batch 0, Loss: 10.4851, Pearson: 0.4954, Spearman: 0.5050
2025-11-14 19:36:38,519 - INFO - Fold 3 Val Epoch 22/200, Batch 10, Loss: 8.3299, Pearson: 0.6067, Spearman: 0.5805
2025-11-14 19:36:42,199 - INFO - Fold 3 Val Epoch 22/200, Val Loss: 8.5411, Pearson Mean: 0.5632, Spearman Mean: 0.5586
2025-11-14 19:36:42,200 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3209, 'spearman_mean_genewise': 0.3026, 'l1_error_mean': 2.1816, 'l2_errors_mean': 8.5706, 'r2_scores_mean': 0.1106, 'pearson_std': 0.139, 'l2_error_q1': 6.0997, 'l2_error_q2': 8.4441, 'l2_error_q3': 10.8274, 'r2_score_q1': 0.0405, 'r2_score_q2': 0.0768, 'r2_score_q3': 0.1414, 'mape_mean': 61.6643, 'mape_std': 20.8886, 'rmse_mean': 2.8815, 'rmse_std': 0.517}
2025-11-14 19:36:42,200 - INFO - Learning rate for epoch 22: 1e-05
2025-11-14 19:36:42,200 - INFO - No improvement in spearman genewise. Patience: 7/30
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6594
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6336
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.018099784851074
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 23 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.745508]
Sample y_pred values (first sample, first 10 genes):
2025-11-14 19:36:43,240 - INFO - Fold 3 Train Epoch 23/200, Batch 0, Loss: 6.9435, Pearson: 0.6452, Spearman: 0.6055
2025-11-14 19:36:53,372 - INFO - Fold 3 Train Epoch 23/200, Batch 10, Loss: 7.1535, Pearson: 0.6358, Spearman: 0.6058
2025-11-14 19:37:03,499 - INFO - Fold 3 Train Epoch 23/200, Batch 20, Loss: 7.0018, Pearson: 0.6371, Spearman: 0.6057
2025-11-14 19:37:13,676 - INFO - Fold 3 Train Epoch 23/200, Batch 30, Loss: 6.9748, Pearson: 0.6436, Spearman: 0.6087
2025-11-14 19:37:23,805 - INFO - Fold 3 Train Epoch 23/200, Batch 40, Loss: 6.8920, Pearson: 0.6498, Spearman: 0.6138
2025-11-14 19:37:33,975 - INFO - Fold 3 Train Epoch 23/200, Batch 50, Loss: 6.9911, Pearson: 0.6473, Spearman: 0.6145
[0.42630234 0.9713729  1.1576161  1.3236082  3.7308922  1.0457512
 0.7545384  2.1243117  3.9756322  8.34086   ]
y_true  -> mean=1.9981, std=3.4487, min=0.0000, max=12.9888
y_pred  -> mean=2.0321, std=2.2443, min=0.0000, max=14.5303
Batch 0 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6568
2025-11-14 19:37:42,374 - INFO - Fold 3 Train Epoch 23/200, Batch 60, Loss: 6.7563, Pearson: 0.6433, Spearman: 0.6094
2025-11-14 19:37:50,662 - INFO - Fold 3 Train Epoch 23/200, Batch 70, Loss: 7.1115, Pearson: 0.6485, Spearman: 0.6121
2025-11-14 19:38:00,813 - INFO - Fold 3 Train Epoch 23/200, Batch 80, Loss: 6.9871, Pearson: 0.6454, Spearman: 0.6097
2025-11-14 19:38:08,968 - INFO - Fold 3 Train Epoch 23/200, Train Loss: 7.0065, Pearson Mean: 0.6475, Spearman Mean: 0.6113
2025-11-14 19:38:08,969 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4477, 'spearman_mean_genewise': 0.3992, 'l1_error_mean': 1.8292, 'l2_errors_mean': 7.0065, 'r2_scores_mean': 0.2119, 'pearson_std': 0.11, 'l2_error_q1': 4.5691, 'l2_error_q2': 6.6654, 'l2_error_q3': 9.1488, 'r2_score_q1': 0.1355, 'r2_score_q2': 0.1875, 'r2_score_q3': 0.2544, 'mape_mean': 58.7749, 'mape_std': 18.1424, 'rmse_mean': 2.5967, 'rmse_std': 0.5133}
2025-11-14 19:38:09,283 - INFO - Fold 3 Val Epoch 23/200, Batch 0, Loss: 10.4992, Pearson: 0.4950, Spearman: 0.5045
2025-11-14 19:38:10,974 - INFO - Fold 3 Val Epoch 23/200, Batch 10, Loss: 8.4180, Pearson: 0.6048, Spearman: 0.5783
2025-11-14 19:38:14,653 - INFO - Fold 3 Val Epoch 23/200, Val Loss: 8.5505, Pearson Mean: 0.5623, Spearman Mean: 0.5577
2025-11-14 19:38:14,654 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3189, 'spearman_mean_genewise': 0.3015, 'l1_error_mean': 2.1738, 'l2_errors_mean': 8.58, 'r2_scores_mean': 0.1094, 'pearson_std': 0.1394, 'l2_error_q1': 6.072, 'l2_error_q2': 8.4591, 'l2_error_q3': 10.8068, 'r2_score_q1': 0.0398, 'r2_score_q2': 0.0761, 'r2_score_q3': 0.1384, 'mape_mean': 62.4067, 'mape_std': 20.879, 'rmse_mean': 2.8833, 'rmse_std': 0.5161}
2025-11-14 19:38:14,654 - INFO - Learning rate for epoch 23: 1e-05
2025-11-14 19:38:14,654 - INFO - No improvement in spearman genewise. Patience: 8/30
2025-11-14 19:38:15,548 - INFO - Fold 3 Train Epoch 24/200, Batch 0, Loss: 7.0291, Pearson: 0.6401, Spearman: 0.6017
2025-11-14 19:38:25,708 - INFO - Fold 3 Train Epoch 24/200, Batch 10, Loss: 7.1268, Pearson: 0.6366, Spearman: 0.6096
2025-11-14 19:38:35,862 - INFO - Fold 3 Train Epoch 24/200, Batch 20, Loss: 7.0778, Pearson: 0.6500, Spearman: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6612
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6562
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.006499290466309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 24 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       9.458879]
Sample y_pred values (first sample, first 10 genes):
[0.21066265 1.0292532  1.3939564  0.4334983  1.8506731  0.6299752
 0.2169482  0.4955628  1.9554809  8.774764  ]
y_true  -> mean=1.9880, std=3.4502, min=0.0000, max=12.8039
y_pred  -> mean=2.0319, std=2.2264, min=0.0000, max=13.6638
Batch 0 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6521
2025-11-14 19:38:46,016 - INFO - Fold 3 Train Epoch 24/200, Batch 30, Loss: 7.1036, Pearson: 0.6475, Spearman: 0.6155
2025-11-14 19:38:56,179 - INFO - Fold 3 Train Epoch 24/200, Batch 40, Loss: 7.0150, Pearson: 0.6490, Spearman: 0.6132
2025-11-14 19:39:06,311 - INFO - Fold 3 Train Epoch 24/200, Batch 50, Loss: 7.3023, Pearson: 0.6377, Spearman: 0.6001
2025-11-14 19:39:14,757 - INFO - Fold 3 Train Epoch 24/200, Batch 60, Loss: 6.8864, Pearson: 0.6577, Spearman: 0.6138
2025-11-14 19:39:23,098 - INFO - Fold 3 Train Epoch 24/200, Batch 70, Loss: 7.1669, Pearson: 0.6553, Spearman: 0.6184
2025-11-14 19:39:33,269 - INFO - Fold 3 Train Epoch 24/200, Batch 80, Loss: 6.9760, Pearson: 0.6471, Spearman: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6471
2025-11-14 19:39:41,380 - INFO - Fold 3 Train Epoch 24/200, Train Loss: 7.0000, Pearson Mean: 0.6477, Spearman Mean: 0.6117
2025-11-14 19:39:41,380 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4483, 'spearman_mean_genewise': 0.3998, 'l1_error_mean': 1.8303, 'l2_errors_mean': 7.0007, 'r2_scores_mean': 0.2126, 'pearson_std': 0.11, 'l2_error_q1': 4.5608, 'l2_error_q2': 6.6503, 'l2_error_q3': 9.1223, 'r2_score_q1': 0.1368, 'r2_score_q2': 0.188, 'r2_score_q3': 0.2544, 'mape_mean': 58.6125, 'mape_std': 18.148, 'rmse_mean': 2.5956, 'rmse_std': 0.5133}
2025-11-14 19:39:41,664 - INFO - Fold 3 Val Epoch 24/200, Batch 0, Loss: 10.5106, Pearson: 0.4940, Spearman: 0.5041
2025-11-14 19:39:43,393 - INFO - Fold 3 Val Epoch 24/200, Batch 10, Loss: 8.3922, Pearson: 0.6036, Spearman: 0.5778
2025-11-14 19:39:47,098 - INFO - Fold 3 Val Epoch 24/200, Val Loss: 8.5524, Pearson Mean: 0.5625, Spearman Mean: 0.5583
2025-11-14 19:39:47,099 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.319, 'spearman_mean_genewise': 0.3008, 'l1_error_mean': 2.1882, 'l2_errors_mean': 8.5819, 'r2_scores_mean': 0.1099, 'pearson_std': 0.1393, 'l2_error_q1': 6.0723, 'l2_error_q2': 8.4487, 'l2_error_q3': 10.8521, 'r2_score_q1': 0.0401, 'r2_score_q2': 0.0758, 'r2_score_q3': 0.1395, 'mape_mean': 61.9445, 'mape_std': 21.0088, 'rmse_mean': 2.8832, 'rmse_std': 0.5186}
2025-11-14 19:39:47,099 - INFO - Learning rate for epoch 24: 1e-05
2025-11-14 19:39:47,099 - INFO - No improvement in spearman genewise. Patience: 9/30
2025-11-14 19:39:47,966 - INFO - Fold 3 Train Epoch 25/200, Batch 0, Loss: 6.9853, Pearson: 0.6480, Spearman: 0.6107
2025-11-14 19:39:58,136 - INFO - Fold 3 Train Epoch 25/200, Batch 10, Loss: 7.1711, Pearson: 0.6354, Spearman: 0.6074
2025-11-14 19:40:08,279 - INFO - Fold 3 Train Epoch 25/200, Batch 20, Loss: 6.9427, Pearson: 0.6528, Spearman: 0.6143
2025-11-14 19:40:18,454 - INFO - Fold 3 Train Epoch 25/200, Batch 30, Loss: 7.1896, Pearson: 0.6517, Spearman: 0.6192
2025-11-14 19:40:28,604 - INFO - Fold 3 Train Epoch 25/200, Batch 40, Loss: 7.2788, Pearson: 0.6499, Spearman: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6417
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6379
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.0006513595581055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 25 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.778712]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.31765863 0.         0.800969   0.
 0.07748047 0.18995357 1.6770942  5.9637995 ]
y_true  -> mean=2.0427, std=3.4698, min=0.0000, max=12.9888
y_pred  -> mean=2.0319, std=2.2196, min=0.0000, max=12.9470
Batch 0 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6487
2025-11-14 19:40:38,753 - INFO - Fold 3 Train Epoch 25/200, Batch 50, Loss: 7.0671, Pearson: 0.6435, Spearman: 0.6120
2025-11-14 19:40:47,145 - INFO - Fold 3 Train Epoch 25/200, Batch 60, Loss: 6.9153, Pearson: 0.6456, Spearman: 0.6145
2025-11-14 19:40:55,449 - INFO - Fold 3 Train Epoch 25/200, Batch 70, Loss: 7.0571, Pearson: 0.6405, Spearman: 0.6087
2025-11-14 19:41:05,627 - INFO - Fold 3 Train Epoch 25/200, Batch 80, Loss: 7.0450, Pearson: 0.6406, Spearman: 0.6074
2025-11-14 19:41:13,775 - INFO - Fold 3 Train Epoch 25/200, Train Loss: 6.9946, Pearson Mean: 0.6482, Spearman Mean: 0.6120
2025-11-14 19:41:13,775 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4488, 'spearman_mean_genewise': 0.4001, 'l1_error_mean': 1.8293, 'l2_errors_mean': 6.9952, 'r2_scores_mean': 0.2131, 'pearson_std': 0.1103, 'l2_error_q1': 4.5658, 'l2_error_q2': 6.6562, 'l2_error_q3': 9.1427, 'r2_score_q1': 0.1361, 'r2_score_q2': 0.1893, 'r2_score_q3': 0.2562, 'mape_mean': 58.6616, 'mape_std': 18.1354, 'rmse_mean': 2.5946, 'rmse_std': 0.513}
2025-11-14 19:41:14,123 - INFO - Fold 3 Val Epoch 25/200, Batch 0, Loss: 10.4506, Pearson: 0.4958, Spearman: 0.5053
2025-11-14 19:41:15,878 - INFO - Fold 3 Val Epoch 25/200, Batch 10, Loss: 8.3514, Pearson: 0.6053, Spearman: 0.5795
2025-11-14 19:41:19,487 - INFO - Fold 3 Val Epoch 25/200, Val Loss: 8.5455, Pearson Mean: 0.5628, Spearman Mean: 0.5584
2025-11-14 19:41:19,487 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.321, 'spearman_mean_genewise': 0.303, 'l1_error_mean': 2.1907, 'l2_errors_mean': 8.5749, 'r2_scores_mean': 0.1105, 'pearson_std': 0.1389, 'l2_error_q1': 6.073, 'l2_error_q2': 8.4427, 'l2_error_q3': 10.8369, 'r2_score_q1': 0.0403, 'r2_score_q2': 0.0767, 'r2_score_q3': 0.1408, 'mape_mean': 61.3235, 'mape_std': 20.8986, 'rmse_mean': 2.8821, 'rmse_std': 0.5182}
2025-11-14 19:41:19,488 - INFO - Learning rate for epoch 25: 1e-05
2025-11-14 19:41:19,488 - INFO - No improvement in spearman genewise. Patience: 10/30
2025-11-14 19:41:20,369 - INFO - Fold 3 Train Epoch 26/200, Batch 0, Loss: 7.0079, Pearson: 0.6526, Spearman: 0.6147
2025-11-14 19:41:30,566 - INFO - Fold 3 Train Epoch 26/200, Batch 10, Loss: 7.1578, Pearson: 0.6441, Spearman: 0.6141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6378
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6425
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.995180130004883
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 26 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 8.622734 0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.3904404  0.         0.9083365  0.
 0.         0.09701425 1.1171209  5.7103195 ]
y_true  -> mean=2.1223, std=3.4907, min=0.0000, max=12.6955
y_pred  -> mean=2.0312, std=2.2138, min=0.0000, max=14.0368
Batch 0 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6429
2025-11-14 19:41:40,722 - INFO - Fold 3 Train Epoch 26/200, Batch 20, Loss: 6.8788, Pearson: 0.6469, Spearman: 0.6160
2025-11-14 19:41:50,866 - INFO - Fold 3 Train Epoch 26/200, Batch 30, Loss: 6.9485, Pearson: 0.6523, Spearman: 0.6169
2025-11-14 19:42:01,030 - INFO - Fold 3 Train Epoch 26/200, Batch 40, Loss: 6.7814, Pearson: 0.6489, Spearman: 0.6129
2025-11-14 19:42:11,180 - INFO - Fold 3 Train Epoch 26/200, Batch 50, Loss: 6.9938, Pearson: 0.6580, Spearman: 0.6207
2025-11-14 19:42:19,593 - INFO - Fold 3 Train Epoch 26/200, Batch 60, Loss: 6.9379, Pearson: 0.6456, Spearman: 0.6086
2025-11-14 19:42:27,860 - INFO - Fold 3 Train Epoch 26/200, Batch 70, Loss: 7.0802, Pearson: 0.6444, Spearman: 0.6048
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6704
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6444
2025-11-14 19:42:38,021 - INFO - Fold 3 Train Epoch 26/200, Batch 80, Loss: 7.1125, Pearson: 0.6498, Spearman: 0.6152
2025-11-14 19:42:46,175 - INFO - Fold 3 Train Epoch 26/200, Train Loss: 6.9904, Pearson Mean: 0.6485, Spearman Mean: 0.6124
2025-11-14 19:42:46,175 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4494, 'spearman_mean_genewise': 0.4007, 'l1_error_mean': 1.8283, 'l2_errors_mean': 6.9911, 'r2_scores_mean': 0.2135, 'pearson_std': 0.1099, 'l2_error_q1': 4.5539, 'l2_error_q2': 6.6461, 'l2_error_q3': 9.1453, 'r2_score_q1': 0.1366, 'r2_score_q2': 0.1876, 'r2_score_q3': 0.2555, 'mape_mean': 58.6751, 'mape_std': 18.1333, 'rmse_mean': 2.5939, 'rmse_std': 0.5127}
2025-11-14 19:42:46,570 - INFO - Fold 3 Val Epoch 26/200, Batch 0, Loss: 10.4847, Pearson: 0.4950, Spearman: 0.5041
2025-11-14 19:42:48,359 - INFO - Fold 3 Val Epoch 26/200, Batch 10, Loss: 8.3632, Pearson: 0.6053, Spearman: 0.5795
2025-11-14 19:42:51,993 - INFO - Fold 3 Val Epoch 26/200, Val Loss: 8.5433, Pearson Mean: 0.5624, Spearman Mean: 0.5579
2025-11-14 19:42:51,993 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3198, 'spearman_mean_genewise': 0.3022, 'l1_error_mean': 2.1783, 'l2_errors_mean': 8.573, 'r2_scores_mean': 0.1101, 'pearson_std': 0.1394, 'l2_error_q1': 6.076, 'l2_error_q2': 8.4453, 'l2_error_q3': 10.8194, 'r2_score_q1': 0.0395, 'r2_score_q2': 0.0777, 'r2_score_q3': 0.1402, 'mape_mean': 61.9795, 'mape_std': 20.8175, 'rmse_mean': 2.8822, 'rmse_std': 0.5155}
2025-11-14 19:42:51,994 - INFO - Learning rate for epoch 26: 1e-05
2025-11-14 19:42:51,994 - INFO - No improvement in spearman genewise. Patience: 11/30
2025-11-14 19:42:53,019 - INFO - Fold 3 Train Epoch 27/200, Batch 0, Loss: 7.0929, Pearson: 0.6397, Spearman: 0.6064
2025-11-14 19:43:03,080 - INFO - Fold 3 Train Epoch 27/200, Batch 10, Loss: 6.9598, Pearson: 0.6406, Spearman: 0.6059
2025-11-14 19:43:13,243 - INFO - Fold 3 Train Epoch 27/200, Batch 20, Loss: 6.7594, Pearson: 0.6460, Spearman: 0.6141
2025-11-14 19:43:23,418 - INFO - Fold 3 Train Epoch 27/200, Batch 30, Loss: 6.8434, Pearson: 0.6546, Spearman: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6477
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6450
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.991140842437744
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 27 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.50342   6.810824  8.314044  6.810824  0.
 6.1187778 0.        9.29472  ]
Sample y_pred values (first sample, first 10 genes):
[2.184634  5.2844677 5.1357474 4.7129664 7.851982  5.7518225 1.8484434
 4.9109316 4.512282  9.07498  ]
y_true  -> mean=1.9994, std=3.4646, min=0.0000, max=12.4985
y_pred  -> mean=2.0273, std=2.2026, min=0.0000, max=13.3499
Batch 0 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6584
2025-11-14 19:43:33,574 - INFO - Fold 3 Train Epoch 27/200, Batch 40, Loss: 6.9828, Pearson: 0.6552, Spearman: 0.6131
2025-11-14 19:43:43,726 - INFO - Fold 3 Train Epoch 27/200, Batch 50, Loss: 7.3152, Pearson: 0.6494, Spearman: 0.6177
2025-11-14 19:43:52,163 - INFO - Fold 3 Train Epoch 27/200, Batch 60, Loss: 6.9203, Pearson: 0.6450, Spearman: 0.6105
2025-11-14 19:44:00,607 - INFO - Fold 3 Train Epoch 27/200, Batch 70, Loss: 7.2270, Pearson: 0.6470, Spearman: 0.6132
2025-11-14 19:44:10,753 - INFO - Fold 3 Train Epoch 27/200, Batch 80, Loss: 6.8685, Pearson: 0.6489, Spearman: 0.6107
2025-11-14 19:44:18,969 - INFO - Fold 3 Train Epoch 27/200, Train Loss: 6.9783, Pearson Mean: 0.6495, Spearman Mean: 0.6133
2025-11-14 19:44:18,969 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4507, 'spearman_mean_genewise': 0.4015, 'l1_error_mean': 1.8271, 'l2_errors_mean': 6.9772, 'r2_scores_mean': 0.2148, 'pearson_std': 0.1105, 'l2_error_q1': 4.5551, 'l2_error_q2': 6.6335, 'l2_error_q3': 9.0985, 'r2_score_q1': 0.1379, 'r2_score_q2': 0.19, 'r2_score_q3': 0.257, 'mape_mean': 58.7034, 'mape_std': 18.0905, 'rmse_mean': 2.5913, 'rmse_std': 0.512}
2025-11-14 19:44:19,362 - INFO - Fold 3 Val Epoch 27/200, Batch 0, Loss: 10.5173, Pearson: 0.4954, Spearman: 0.5044
2025-11-14 19:44:21,097 - INFO - Fold 3 Val Epoch 27/200, Batch 10, Loss: 8.4066, Pearson: 0.6050, Spearman: 0.5791
2025-11-14 19:44:24,751 - INFO - Fold 3 Val Epoch 27/200, Val Loss: 8.5400, Pearson Mean: 0.5629, Spearman Mean: 0.5584
2025-11-14 19:44:24,751 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.319, 'spearman_mean_genewise': 0.3016, 'l1_error_mean': 2.1655, 'l2_errors_mean': 8.57, 'r2_scores_mean': 0.1101, 'pearson_std': 0.14, 'l2_error_q1': 6.0669, 'l2_error_q2': 8.4523, 'l2_error_q3': 10.8218, 'r2_score_q1': 0.0405, 'r2_score_q2': 0.0752, 'r2_score_q3': 0.1399, 'mape_mean': 62.6551, 'mape_std': 20.7528, 'rmse_mean': 2.8818, 'rmse_std': 0.5148}
2025-11-14 19:44:24,752 - INFO - Learning rate for epoch 27: 1.0000000000000002e-06
2025-11-14 19:44:24,752 - INFO - No improvement in spearman genewise. Patience: 12/30
2025-11-14 19:44:25,744 - INFO - Fold 3 Train Epoch 28/200, Batch 0, Loss: 6.9870, Pearson: 0.6552, Spearman: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6595
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6535
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.977197647094727
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 28 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       8.386393 0.       0.       0.
 0.       9.079426]
Sample y_pred values (first sample, first 10 genes):
[0.         0.44619018 0.37285787 0.7472122  2.1370895  0.5103849
 0.52324826 0.29844004 1.7361003  6.673663  ]
y_true  -> mean=2.1199, std=3.4959, min=0.0000, max=12.8160
y_pred  -> mean=2.0286, std=2.2273, min=0.0000, max=13.5385
Batch 0 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6520
2025-11-14 19:44:35,794 - INFO - Fold 3 Train Epoch 28/200, Batch 10, Loss: 6.9772, Pearson: 0.6476, Spearman: 0.6137
2025-11-14 19:44:45,928 - INFO - Fold 3 Train Epoch 28/200, Batch 20, Loss: 6.7864, Pearson: 0.6529, Spearman: 0.6135
2025-11-14 19:44:56,089 - INFO - Fold 3 Train Epoch 28/200, Batch 30, Loss: 6.8531, Pearson: 0.6603, Spearman: 0.6228
2025-11-14 19:45:06,264 - INFO - Fold 3 Train Epoch 28/200, Batch 40, Loss: 6.9550, Pearson: 0.6518, Spearman: 0.6188
2025-11-14 19:45:16,388 - INFO - Fold 3 Train Epoch 28/200, Batch 50, Loss: 6.9710, Pearson: 0.6652, Spearman: 0.6153
2025-11-14 19:45:24,837 - INFO - Fold 3 Train Epoch 28/200, Batch 60, Loss: 7.1819, Pearson: 0.6509, Spearman: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6509
2025-11-14 19:45:33,134 - INFO - Fold 3 Train Epoch 28/200, Batch 70, Loss: 6.9004, Pearson: 0.6433, Spearman: 0.6118
2025-11-14 19:45:43,291 - INFO - Fold 3 Train Epoch 28/200, Batch 80, Loss: 7.0315, Pearson: 0.6456, Spearman: 0.6136
2025-11-14 19:45:51,440 - INFO - Fold 3 Train Epoch 28/200, Train Loss: 6.9637, Pearson Mean: 0.6502, Spearman Mean: 0.6141
2025-11-14 19:45:51,440 - INFO - Training Metrics: {'pearson_mean_genewise': 0.452, 'spearman_mean_genewise': 0.4025, 'l1_error_mean': 1.827, 'l2_errors_mean': 6.9647, 'r2_scores_mean': 0.216, 'pearson_std': 0.1108, 'l2_error_q1': 4.5497, 'l2_error_q2': 6.6385, 'l2_error_q3': 9.0931, 'r2_score_q1': 0.1383, 'r2_score_q2': 0.192, 'r2_score_q3': 0.2586, 'mape_mean': 58.646, 'mape_std': 18.0758, 'rmse_mean': 2.589, 'rmse_std': 0.5118}
2025-11-14 19:45:51,789 - INFO - Fold 3 Val Epoch 28/200, Batch 0, Loss: 10.4836, Pearson: 0.4942, Spearman: 0.5032
2025-11-14 19:45:53,499 - INFO - Fold 3 Val Epoch 28/200, Batch 10, Loss: 8.3929, Pearson: 0.6044, Spearman: 0.5788
2025-11-14 19:45:57,119 - INFO - Fold 3 Val Epoch 28/200, Val Loss: 8.5992, Pearson Mean: 0.5586, Spearman Mean: 0.5540
2025-11-14 19:45:57,120 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3125, 'spearman_mean_genewise': 0.3007, 'l1_error_mean': 2.1905, 'l2_errors_mean': 8.6294, 'r2_scores_mean': 0.1023, 'pearson_std': 0.1427, 'l2_error_q1': 6.16, 'l2_error_q2': 8.481, 'l2_error_q3': 10.8901, 'r2_score_q1': 0.0323, 'r2_score_q2': 0.0699, 'r2_score_q3': 0.1353, 'mape_mean': 61.8833, 'mape_std': 20.8279, 'rmse_mean': 2.8928, 'rmse_std': 0.5108}
2025-11-14 19:45:57,120 - INFO - Learning rate for epoch 28: 1.0000000000000002e-06
2025-11-14 19:45:57,120 - INFO - No improvement in spearman genewise. Patience: 13/30
2025-11-14 19:45:58,034 - INFO - Fold 3 Train Epoch 29/200, Batch 0, Loss: 7.0232, Pearson: 0.6614, Spearman: 0.6218
2025-11-14 19:46:08,209 - INFO - Fold 3 Train Epoch 29/200, Batch 10, Loss: 6.8519, Pearson: 0.6483, Spearman: 0.6174
2025-11-14 19:46:18,390 - INFO - Fold 3 Train Epoch 29/200, Batch 20, Loss: 6.8735, Pearson: 0.6500, Spearman: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6503
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6548
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.964674949645996
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 29 =====================
Sample y_true values (first sample, first 10 genes):
[6.590676 0.       0.       0.       0.       0.       0.       0.
 6.590676 8.199015]
Sample y_pred values (first sample, first 10 genes):
[0.37938958 0.25362325 1.4971426  0.251352   3.997438   1.4040544
 0.58016    1.1514941  4.974889   8.373535  ]
y_true  -> mean=2.2213, std=3.5209, min=0.0000, max=12.6955
y_pred  -> mean=2.0275, std=2.2207, min=0.0000, max=12.6433
Batch 0 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6461
2025-11-14 19:46:28,544 - INFO - Fold 3 Train Epoch 29/200, Batch 30, Loss: 7.2168, Pearson: 0.6461, Spearman: 0.6120
2025-11-14 19:46:38,712 - INFO - Fold 3 Train Epoch 29/200, Batch 40, Loss: 6.9002, Pearson: 0.6495, Spearman: 0.6091
2025-11-14 19:46:48,887 - INFO - Fold 3 Train Epoch 29/200, Batch 50, Loss: 7.0489, Pearson: 0.6508, Spearman: 0.6202
2025-11-14 19:46:57,307 - INFO - Fold 3 Train Epoch 29/200, Batch 60, Loss: 6.9434, Pearson: 0.6536, Spearman: 0.6186
2025-11-14 19:47:05,697 - INFO - Fold 3 Train Epoch 29/200, Batch 70, Loss: 6.9705, Pearson: 0.6581, Spearman: 0.6250
2025-11-14 19:47:15,855 - INFO - Fold 3 Train Epoch 29/200, Batch 80, Loss: 6.8512, Pearson: 0.6456, Spearman: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6496
2025-11-14 19:47:24,027 - INFO - Fold 3 Train Epoch 29/200, Train Loss: 6.9622, Pearson Mean: 0.6503, Spearman Mean: 0.6141
2025-11-14 19:47:24,027 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4521, 'spearman_mean_genewise': 0.4026, 'l1_error_mean': 1.8271, 'l2_errors_mean': 6.9629, 'r2_scores_mean': 0.2161, 'pearson_std': 0.1109, 'l2_error_q1': 4.5486, 'l2_error_q2': 6.6426, 'l2_error_q3': 9.0784, 'r2_score_q1': 0.1383, 'r2_score_q2': 0.19, 'r2_score_q3': 0.2585, 'mape_mean': 58.6417, 'mape_std': 18.0902, 'rmse_mean': 2.5886, 'rmse_std': 0.5117}
2025-11-14 19:47:24,404 - INFO - Fold 3 Val Epoch 29/200, Batch 0, Loss: 10.4945, Pearson: 0.4943, Spearman: 0.5032
2025-11-14 19:47:26,146 - INFO - Fold 3 Val Epoch 29/200, Batch 10, Loss: 8.3784, Pearson: 0.6051, Spearman: 0.5792
2025-11-14 19:47:29,804 - INFO - Fold 3 Val Epoch 29/200, Val Loss: 8.5366, Pearson Mean: 0.5630, Spearman Mean: 0.5586
2025-11-14 19:47:29,804 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3209, 'spearman_mean_genewise': 0.3025, 'l1_error_mean': 2.1784, 'l2_errors_mean': 8.5665, 'r2_scores_mean': 0.1111, 'pearson_std': 0.1391, 'l2_error_q1': 6.0565, 'l2_error_q2': 8.4489, 'l2_error_q3': 10.8119, 'r2_score_q1': 0.0413, 'r2_score_q2': 0.0773, 'r2_score_q3': 0.1396, 'mape_mean': 61.9814, 'mape_std': 20.8495, 'rmse_mean': 2.8808, 'rmse_std': 0.5172}
2025-11-14 19:47:29,804 - INFO - Learning rate for epoch 29: 1.0000000000000002e-06
2025-11-14 19:47:29,805 - INFO - No improvement in spearman genewise. Patience: 14/30
2025-11-14 19:47:30,867 - INFO - Fold 3 Train Epoch 30/200, Batch 0, Loss: 7.0184, Pearson: 0.6542, Spearman: 0.6109
2025-11-14 19:47:40,951 - INFO - Fold 3 Train Epoch 30/200, Batch 10, Loss: 6.9041, Pearson: 0.6573, Spearman: 0.6159
2025-11-14 19:47:51,092 - INFO - Fold 3 Train Epoch 30/200, Batch 20, Loss: 6.9143, Pearson: 0.6513, Spearman: 0.6126
2025-11-14 19:48:01,247 - INFO - Fold 3 Train Epoch 30/200, Batch 30, Loss: 7.2364, Pearson: 0.6457, Spearman: 0.6111
2025-11-14 19:48:11,397 - INFO - Fold 3 Train Epoch 30/200, Batch 40, Loss: 6.7658, Pearson: 0.6606, Spearman: 0.6207
2025-11-14 19:48:21,559 - INFO - Fold 3 Train Epoch 30/200, Batch 50, Loss: 7.0932, Pearson: 0.6495, Spearman: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6446
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6477
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.9629058837890625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 30 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        8.268639
 0.        7.5757475 7.5757475]
Sample y_pred values (first sample, first 10 genes):
[0.90198874 1.6558982  3.1665542  0.7820475  3.2532213  3.4196002
 0.471415   0.8220134  3.3708553  5.54217   ]
y_true  -> mean=2.1499, std=3.4981, min=0.0000, max=12.7270
y_pred  -> mean=2.0299, std=2.2280, min=0.0000, max=13.0561
Batch 0 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6495
2025-11-14 19:48:30,043 - INFO - Fold 3 Train Epoch 30/200, Batch 60, Loss: 6.8068, Pearson: 0.6540, Spearman: 0.6137
2025-11-14 19:48:38,494 - INFO - Fold 3 Train Epoch 30/200, Batch 70, Loss: 6.9869, Pearson: 0.6525, Spearman: 0.6198
2025-11-14 19:48:48,616 - INFO - Fold 3 Train Epoch 30/200, Batch 80, Loss: 6.7140, Pearson: 0.6482, Spearman: 0.6131
2025-11-14 19:48:56,797 - INFO - Fold 3 Train Epoch 30/200, Train Loss: 6.9631, Pearson Mean: 0.6503, Spearman Mean: 0.6141
2025-11-14 19:48:56,797 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4521, 'spearman_mean_genewise': 0.4025, 'l1_error_mean': 1.8277, 'l2_errors_mean': 6.9638, 'r2_scores_mean': 0.2161, 'pearson_std': 0.1109, 'l2_error_q1': 4.5493, 'l2_error_q2': 6.6349, 'l2_error_q3': 9.0711, 'r2_score_q1': 0.1382, 'r2_score_q2': 0.191, 'r2_score_q3': 0.2584, 'mape_mean': 58.6403, 'mape_std': 18.0877, 'rmse_mean': 2.5888, 'rmse_std': 0.5118}
2025-11-14 19:48:57,135 - INFO - Fold 3 Val Epoch 30/200, Batch 0, Loss: 10.4944, Pearson: 0.4940, Spearman: 0.5029
2025-11-14 19:48:58,910 - INFO - Fold 3 Val Epoch 30/200, Batch 10, Loss: 8.3659, Pearson: 0.6049, Spearman: 0.5792
2025-11-14 19:49:02,544 - INFO - Fold 3 Val Epoch 30/200, Val Loss: 8.5566, Pearson Mean: 0.5617, Spearman Mean: 0.5572
2025-11-14 19:49:02,545 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3182, 'spearman_mean_genewise': 0.3019, 'l1_error_mean': 2.1857, 'l2_errors_mean': 8.5861, 'r2_scores_mean': 0.1087, 'pearson_std': 0.14, 'l2_error_q1': 6.0824, 'l2_error_q2': 8.4448, 'l2_error_q3': 10.8519, 'r2_score_q1': 0.0384, 'r2_score_q2': 0.0751, 'r2_score_q3': 0.1394, 'mape_mean': 61.765, 'mape_std': 20.8992, 'rmse_mean': 2.8844, 'rmse_std': 0.5164}
2025-11-14 19:49:02,545 - INFO - Learning rate for epoch 30: 1.0000000000000002e-06
2025-11-14 19:49:02,545 - INFO - No improvement in spearman genewise. Patience: 15/30
2025-11-14 19:49:03,473 - INFO - Fold 3 Train Epoch 31/200, Batch 0, Loss: 6.7179, Pearson: 0.6554, Spearman: 0.6121
2025-11-14 19:49:13,644 - INFO - Fold 3 Train Epoch 31/200, Batch 10, Loss: 6.8986, Pearson: 0.6421, Spearman: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6696
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6447
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6585
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.963791370391846
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 31 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.43466786 1.0589437  0.41127133 0.         2.1216326  0.9482423
 0.59509444 1.3988236  3.2053967  4.190097  ]
y_true  -> mean=1.9652, std=3.4306, min=0.0000, max=12.4631
y_pred  -> mean=2.0262, std=2.2365, min=0.0000, max=13.8237
Batch 0 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6494
2025-11-14 19:49:23,811 - INFO - Fold 3 Train Epoch 31/200, Batch 20, Loss: 7.0327, Pearson: 0.6517, Spearman: 0.6174
2025-11-14 19:49:33,959 - INFO - Fold 3 Train Epoch 31/200, Batch 30, Loss: 6.8594, Pearson: 0.6538, Spearman: 0.6157
2025-11-14 19:49:44,108 - INFO - Fold 3 Train Epoch 31/200, Batch 40, Loss: 7.0829, Pearson: 0.6445, Spearman: 0.6102
2025-11-14 19:49:54,270 - INFO - Fold 3 Train Epoch 31/200, Batch 50, Loss: 7.2610, Pearson: 0.6410, Spearman: 0.6159
2025-11-14 19:50:02,662 - INFO - Fold 3 Train Epoch 31/200, Batch 60, Loss: 6.9279, Pearson: 0.6442, Spearman: 0.6128
2025-11-14 19:50:10,411 - INFO - Fold 3 Train Epoch 31/200, Batch 70, Loss: 6.8263, Pearson: 0.6571, Spearman: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6496
2025-11-14 19:50:20,584 - INFO - Fold 3 Train Epoch 31/200, Batch 80, Loss: 6.9138, Pearson: 0.6458, Spearman: 0.6068
2025-11-14 19:50:28,751 - INFO - Fold 3 Train Epoch 31/200, Train Loss: 6.9605, Pearson Mean: 0.6505, Spearman Mean: 0.6142
2025-11-14 19:50:28,751 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4524, 'spearman_mean_genewise': 0.4027, 'l1_error_mean': 1.8269, 'l2_errors_mean': 6.9607, 'r2_scores_mean': 0.2164, 'pearson_std': 0.111, 'l2_error_q1': 4.5507, 'l2_error_q2': 6.642, 'l2_error_q3': 9.0674, 'r2_score_q1': 0.1386, 'r2_score_q2': 0.1913, 'r2_score_q3': 0.2595, 'mape_mean': 58.6403, 'mape_std': 18.1024, 'rmse_mean': 2.5882, 'rmse_std': 0.5116}
2025-11-14 19:50:29,123 - INFO - Fold 3 Val Epoch 31/200, Batch 0, Loss: 10.4942, Pearson: 0.4950, Spearman: 0.5041
2025-11-14 19:50:30,854 - INFO - Fold 3 Val Epoch 31/200, Batch 10, Loss: 8.4202, Pearson: 0.6040, Spearman: 0.5782
2025-11-14 19:50:34,431 - INFO - Fold 3 Val Epoch 31/200, Val Loss: 8.5550, Pearson Mean: 0.5615, Spearman Mean: 0.5571
2025-11-14 19:50:34,432 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3177, 'spearman_mean_genewise': 0.3017, 'l1_error_mean': 2.1751, 'l2_errors_mean': 8.5852, 'r2_scores_mean': 0.1084, 'pearson_std': 0.1402, 'l2_error_q1': 6.119, 'l2_error_q2': 8.4544, 'l2_error_q3': 10.8279, 'r2_score_q1': 0.0382, 'r2_score_q2': 0.0751, 'r2_score_q3': 0.1396, 'mape_mean': 62.3184, 'mape_std': 20.8557, 'rmse_mean': 2.8845, 'rmse_std': 0.5146}
2025-11-14 19:50:34,432 - INFO - Learning rate for epoch 31: 1.0000000000000002e-06
2025-11-14 19:50:34,432 - INFO - No improvement in spearman genewise. Patience: 16/30
2025-11-14 19:50:35,342 - INFO - Fold 3 Train Epoch 32/200, Batch 0, Loss: 6.9370, Pearson: 0.6504, Spearman: 0.6114
2025-11-14 19:50:45,552 - INFO - Fold 3 Train Epoch 32/200, Batch 10, Loss: 7.2747, Pearson: 0.6506, Spearman: 0.6106
2025-11-14 19:50:55,693 - INFO - Fold 3 Train Epoch 32/200, Batch 20, Loss: 7.0662, Pearson: 0.6546, Spearman: 0.6194
2025-11-14 19:51:05,864 - INFO - Fold 3 Train Epoch 32/200, Batch 30, Loss: 7.1723, Pearson: 0.6537, Spearman: 0.6219
2025-11-14 19:51:16,036 - INFO - Fold 3 Train Epoch 32/200, Batch 40, Loss: 6.9306, Pearson: 0.6508, Spearman: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6516
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6420
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.96067476272583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 32 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        6.9097543 6.9097543 6.9097543 0.
 7.602402  0.        9.547884 ]
Sample y_pred values (first sample, first 10 genes):
[0.9007191  2.7062542  3.0626159  1.7426705  4.8528833  3.6293533
 0.37912166 2.5015929  3.0034487  9.133188  ]
y_true  -> mean=2.0656, std=3.4666, min=0.0000, max=12.8299
y_pred  -> mean=2.0274, std=2.2120, min=0.0000, max=12.8981
Batch 0 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6508
2025-11-14 19:51:26,210 - INFO - Fold 3 Train Epoch 32/200, Batch 50, Loss: 7.0687, Pearson: 0.6540, Spearman: 0.6184
2025-11-14 19:51:35,004 - INFO - Fold 3 Train Epoch 32/200, Batch 60, Loss: 6.8680, Pearson: 0.6578, Spearman: 0.6226
2025-11-14 19:51:43,041 - INFO - Fold 3 Train Epoch 32/200, Batch 70, Loss: 6.8188, Pearson: 0.6598, Spearman: 0.6176
2025-11-14 19:51:53,199 - INFO - Fold 3 Train Epoch 32/200, Batch 80, Loss: 6.8990, Pearson: 0.6549, Spearman: 0.6156
2025-11-14 19:52:01,312 - INFO - Fold 3 Train Epoch 32/200, Train Loss: 6.9609, Pearson Mean: 0.6505, Spearman Mean: 0.6142
2025-11-14 19:52:01,313 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4523, 'spearman_mean_genewise': 0.4027, 'l1_error_mean': 1.8272, 'l2_errors_mean': 6.9615, 'r2_scores_mean': 0.2163, 'pearson_std': 0.1108, 'l2_error_q1': 4.5491, 'l2_error_q2': 6.6235, 'l2_error_q3': 9.0879, 'r2_score_q1': 0.1388, 'r2_score_q2': 0.1908, 'r2_score_q3': 0.259, 'mape_mean': 58.6125, 'mape_std': 18.0931, 'rmse_mean': 2.5884, 'rmse_std': 0.5118}
2025-11-14 19:52:01,657 - INFO - Fold 3 Val Epoch 32/200, Batch 0, Loss: 10.4834, Pearson: 0.4953, Spearman: 0.5044
2025-11-14 19:52:03,393 - INFO - Fold 3 Val Epoch 32/200, Batch 10, Loss: 8.3790, Pearson: 0.6046, Spearman: 0.5789
2025-11-14 19:52:07,049 - INFO - Fold 3 Val Epoch 32/200, Val Loss: 8.5560, Pearson Mean: 0.5617, Spearman Mean: 0.5573
2025-11-14 19:52:07,050 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3182, 'spearman_mean_genewise': 0.3013, 'l1_error_mean': 2.1831, 'l2_errors_mean': 8.5855, 'r2_scores_mean': 0.1089, 'pearson_std': 0.1398, 'l2_error_q1': 6.0733, 'l2_error_q2': 8.4463, 'l2_error_q3': 10.8363, 'r2_score_q1': 0.0389, 'r2_score_q2': 0.0754, 'r2_score_q3': 0.1383, 'mape_mean': 61.966, 'mape_std': 20.8897, 'rmse_mean': 2.8842, 'rmse_std': 0.5164}
2025-11-14 19:52:07,050 - INFO - Learning rate for epoch 32: 1.0000000000000002e-06
2025-11-14 19:52:07,050 - INFO - No improvement in spearman genewise. Patience: 17/30
2025-11-14 19:52:07,968 - INFO - Fold 3 Train Epoch 33/200, Batch 0, Loss: 7.1886, Pearson: 0.6443, Spearman: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6431
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6570
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.961534023284912
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 33 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.025179 0.       7.717881 7.025179 0.       0.
 0.       7.717881]
Sample y_pred values (first sample, first 10 genes):
[0.         2.8877487  3.5860767  2.0339556  4.9731493  4.069298
 0.62429446 1.7897782  2.8675041  8.854044  ]
y_true  -> mean=2.1048, std=3.5040, min=0.0000, max=12.5594
y_pred  -> mean=2.0304, std=2.2111, min=0.0000, max=13.0608
Batch 0 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6620
2025-11-14 19:52:18,145 - INFO - Fold 3 Train Epoch 33/200, Batch 10, Loss: 7.0886, Pearson: 0.6575, Spearman: 0.6186
2025-11-14 19:52:28,312 - INFO - Fold 3 Train Epoch 33/200, Batch 20, Loss: 7.0227, Pearson: 0.6518, Spearman: 0.6164
2025-11-14 19:52:38,463 - INFO - Fold 3 Train Epoch 33/200, Batch 30, Loss: 6.7758, Pearson: 0.6549, Spearman: 0.6125
2025-11-14 19:52:48,612 - INFO - Fold 3 Train Epoch 33/200, Batch 40, Loss: 6.8958, Pearson: 0.6409, Spearman: 0.6131
2025-11-14 19:52:58,752 - INFO - Fold 3 Train Epoch 33/200, Batch 50, Loss: 7.0777, Pearson: 0.6550, Spearman: 0.6184
2025-11-14 19:53:07,423 - INFO - Fold 3 Train Epoch 33/200, Batch 60, Loss: 7.0159, Pearson: 0.6482, Spearman: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6524
2025-11-14 19:53:15,333 - INFO - Fold 3 Train Epoch 33/200, Batch 70, Loss: 6.9769, Pearson: 0.6535, Spearman: 0.6149
2025-11-14 19:53:25,484 - INFO - Fold 3 Train Epoch 33/200, Batch 80, Loss: 7.1502, Pearson: 0.6444, Spearman: 0.6069
2025-11-14 19:53:33,614 - INFO - Fold 3 Train Epoch 33/200, Train Loss: 6.9588, Pearson Mean: 0.6506, Spearman Mean: 0.6144
2025-11-14 19:53:33,614 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4526, 'spearman_mean_genewise': 0.4029, 'l1_error_mean': 1.8266, 'l2_errors_mean': 6.9585, 'r2_scores_mean': 0.2166, 'pearson_std': 0.1109, 'l2_error_q1': 4.5437, 'l2_error_q2': 6.6246, 'l2_error_q3': 9.0725, 'r2_score_q1': 0.139, 'r2_score_q2': 0.191, 'r2_score_q3': 0.259, 'mape_mean': 58.5914, 'mape_std': 18.1038, 'rmse_mean': 2.5878, 'rmse_std': 0.5115}
2025-11-14 19:53:33,902 - INFO - Fold 3 Val Epoch 33/200, Batch 0, Loss: 10.5195, Pearson: 0.4933, Spearman: 0.5021
2025-11-14 19:53:35,608 - INFO - Fold 3 Val Epoch 33/200, Batch 10, Loss: 8.4368, Pearson: 0.6036, Spearman: 0.5777
2025-11-14 19:53:39,279 - INFO - Fold 3 Val Epoch 33/200, Val Loss: 8.5586, Pearson Mean: 0.5613, Spearman Mean: 0.5569
2025-11-14 19:53:39,280 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3174, 'spearman_mean_genewise': 0.3011, 'l1_error_mean': 2.1745, 'l2_errors_mean': 8.5885, 'r2_scores_mean': 0.1083, 'pearson_std': 0.14, 'l2_error_q1': 6.1022, 'l2_error_q2': 8.4602, 'l2_error_q3': 10.8421, 'r2_score_q1': 0.0377, 'r2_score_q2': 0.0744, 'r2_score_q3': 0.1376, 'mape_mean': 62.4789, 'mape_std': 20.8132, 'rmse_mean': 2.885, 'rmse_std': 0.5152}
2025-11-14 19:53:39,280 - INFO - Learning rate for epoch 33: 1.0000000000000002e-07
2025-11-14 19:53:39,280 - INFO - No improvement in spearman genewise. Patience: 18/30
2025-11-14 19:53:40,188 - INFO - Fold 3 Train Epoch 34/200, Batch 0, Loss: 6.9737, Pearson: 0.6493, Spearman: 0.6184
2025-11-14 19:53:50,378 - INFO - Fold 3 Train Epoch 34/200, Batch 10, Loss: 6.8751, Pearson: 0.6530, Spearman: 0.6183
2025-11-14 19:54:00,553 - INFO - Fold 3 Train Epoch 34/200, Batch 20, Loss: 6.9341, Pearson: 0.6387, Spearman: 0.6081
2025-11-14 19:54:10,717 - INFO - Fold 3 Train Epoch 34/200, Batch 30, Loss: 6.8898, Pearson: 0.6469, Spearman: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6478
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6614
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.9584641456604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 34 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        8.061667
 0.        0.        7.3688354]
Sample y_pred values (first sample, first 10 genes):
[1.5214927 0.5614544 2.5671835 1.9129467 3.2989764 2.6031113 1.3804495
 3.6555195 4.3251953 6.3257527]
y_true  -> mean=2.0446, std=3.4712, min=0.0000, max=12.6320
y_pred  -> mean=2.0268, std=2.1890, min=0.0000, max=13.2069
Batch 0 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6469
2025-11-14 19:54:20,857 - INFO - Fold 3 Train Epoch 34/200, Batch 40, Loss: 7.0844, Pearson: 0.6488, Spearman: 0.6135
2025-11-14 19:54:31,003 - INFO - Fold 3 Train Epoch 34/200, Batch 50, Loss: 7.0012, Pearson: 0.6484, Spearman: 0.6184
2025-11-14 19:54:39,707 - INFO - Fold 3 Train Epoch 34/200, Batch 60, Loss: 7.0378, Pearson: 0.6492, Spearman: 0.6145
2025-11-14 19:54:47,815 - INFO - Fold 3 Train Epoch 34/200, Batch 70, Loss: 6.8839, Pearson: 0.6508, Spearman: 0.6155
2025-11-14 19:54:57,980 - INFO - Fold 3 Train Epoch 34/200, Batch 80, Loss: 6.7752, Pearson: 0.6480, Spearman: 0.6091
2025-11-14 19:55:06,102 - INFO - Fold 3 Train Epoch 34/200, Train Loss: 6.9616, Pearson Mean: 0.6504, Spearman Mean: 0.6142
2025-11-14 19:55:06,102 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4523, 'spearman_mean_genewise': 0.4028, 'l1_error_mean': 1.8278, 'l2_errors_mean': 6.9619, 'r2_scores_mean': 0.2163, 'pearson_std': 0.1108, 'l2_error_q1': 4.5505, 'l2_error_q2': 6.6347, 'l2_error_q3': 9.0836, 'r2_score_q1': 0.1388, 'r2_score_q2': 0.1909, 'r2_score_q3': 0.2585, 'mape_mean': 58.6219, 'mape_std': 18.1085, 'rmse_mean': 2.5884, 'rmse_std': 0.5117}
2025-11-14 19:55:06,434 - INFO - Fold 3 Val Epoch 34/200, Batch 0, Loss: 10.4907, Pearson: 0.4945, Spearman: 0.5037
2025-11-14 19:55:08,138 - INFO - Fold 3 Val Epoch 34/200, Batch 10, Loss: 8.3850, Pearson: 0.6044, Spearman: 0.5784
2025-11-14 19:55:11,788 - INFO - Fold 3 Val Epoch 34/200, Val Loss: 8.5383, Pearson Mean: 0.5630, Spearman Mean: 0.5586
2025-11-14 19:55:11,789 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3206, 'spearman_mean_genewise': 0.3023, 'l1_error_mean': 2.181, 'l2_errors_mean': 8.5682, 'r2_scores_mean': 0.1109, 'pearson_std': 0.1391, 'l2_error_q1': 6.0618, 'l2_error_q2': 8.4402, 'l2_error_q3': 10.8084, 'r2_score_q1': 0.0409, 'r2_score_q2': 0.0771, 'r2_score_q3': 0.141, 'mape_mean': 61.8804, 'mape_std': 20.8954, 'rmse_mean': 2.8811, 'rmse_std': 0.5172}
2025-11-14 19:55:11,789 - INFO - Learning rate for epoch 34: 1.0000000000000002e-07
2025-11-14 19:55:11,789 - INFO - No improvement in spearman genewise. Patience: 19/30
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6535
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6530
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.961857318878174
2025-11-14 19:55:12,670 - INFO - Fold 3 Train Epoch 35/200, Batch 0, Loss: 6.8829, Pearson: 0.6387, Spearman: 0.6067
2025-11-14 19:55:22,870 - INFO - Fold 3 Train Epoch 35/200, Batch 10, Loss: 7.1144, Pearson: 0.6520, Spearman: 0.6150
2025-11-14 19:55:33,026 - INFO - Fold 3 Train Epoch 35/200, Batch 20, Loss: 6.9443, Pearson: 0.6450, Spearman: 0.6148
2025-11-14 19:55:43,171 - INFO - Fold 3 Train Epoch 35/200, Batch 30, Loss: 6.6921, Pearson: 0.6536, Spearman: 0.6115
2025-11-14 19:55:53,332 - INFO - Fold 3 Train Epoch 35/200, Batch 40, Loss: 7.1465, Pearson: 0.6469, Spearman: 0.6168
2025-11-14 19:56:03,469 - INFO - Fold 3 Train Epoch 35/200, Batch 50, Loss: 6.8068, Pearson: 0.6540, Spearman: 0.6119
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 35 =====================
Sample y_true values (first sample, first 10 genes):
[0.       8.135631 0.       0.       8.135631 0.       0.       0.
 0.       0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.58040303 0.5484053  1.2589445  1.1911426  2.0876389  1.7041007
 0.54596    2.24529    2.562651   5.96102   ]
y_true  -> mean=1.8795, std=3.4047, min=0.0000, max=12.5020
y_pred  -> mean=2.0219, std=2.1878, min=0.0000, max=13.8191
Batch 0 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6553
2025-11-14 19:56:12,176 - INFO - Fold 3 Train Epoch 35/200, Batch 60, Loss: 6.8510, Pearson: 0.6562, Spearman: 0.6183
2025-11-14 19:56:20,465 - INFO - Fold 3 Train Epoch 35/200, Batch 70, Loss: 7.2098, Pearson: 0.6521, Spearman: 0.6162
2025-11-14 19:56:30,620 - INFO - Fold 3 Train Epoch 35/200, Batch 80, Loss: 7.0101, Pearson: 0.6562, Spearman: 0.6207
2025-11-14 19:56:38,778 - INFO - Fold 3 Train Epoch 35/200, Train Loss: 6.9586, Pearson Mean: 0.6507, Spearman Mean: 0.6145
2025-11-14 19:56:38,778 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4527, 'spearman_mean_genewise': 0.403, 'l1_error_mean': 1.8268, 'l2_errors_mean': 6.958, 'r2_scores_mean': 0.2166, 'pearson_std': 0.1109, 'l2_error_q1': 4.541, 'l2_error_q2': 6.6432, 'l2_error_q3': 9.0713, 'r2_score_q1': 0.139, 'r2_score_q2': 0.1918, 'r2_score_q3': 0.2597, 'mape_mean': 58.6041, 'mape_std': 18.116, 'rmse_mean': 2.5877, 'rmse_std': 0.5116}
2025-11-14 19:56:39,097 - INFO - Fold 3 Val Epoch 35/200, Batch 0, Loss: 10.5253, Pearson: 0.4930, Spearman: 0.5021
2025-11-14 19:56:40,802 - INFO - Fold 3 Val Epoch 35/200, Batch 10, Loss: 8.4091, Pearson: 0.6042, Spearman: 0.5784
2025-11-14 19:56:44,487 - INFO - Fold 3 Val Epoch 35/200, Val Loss: 8.5877, Pearson Mean: 0.5591, Spearman Mean: 0.5545
2025-11-14 19:56:44,488 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3131, 'spearman_mean_genewise': 0.3003, 'l1_error_mean': 2.1792, 'l2_errors_mean': 8.6177, 'r2_scores_mean': 0.104, 'pearson_std': 0.1421, 'l2_error_q1': 6.133, 'l2_error_q2': 8.472, 'l2_error_q3': 10.8616, 'r2_score_q1': 0.0336, 'r2_score_q2': 0.0709, 'r2_score_q3': 0.1355, 'mape_mean': 62.4662, 'mape_std': 20.7999, 'rmse_mean': 2.8907, 'rmse_std': 0.5114}
2025-11-14 19:56:44,488 - INFO - Learning rate for epoch 35: 1.0000000000000002e-07
2025-11-14 19:56:44,488 - INFO - No improvement in spearman genewise. Patience: 20/30
2025-11-14 19:56:45,380 - INFO - Fold 3 Train Epoch 36/200, Batch 0, Loss: 6.8776, Pearson: 0.6567, Spearman: 0.6188
2025-11-14 19:56:55,553 - INFO - Fold 3 Train Epoch 36/200, Batch 10, Loss: 6.8514, Pearson: 0.6549, Spearman: 0.6217
2025-11-14 19:57:05,738 - INFO - Fold 3 Train Epoch 36/200, Batch 20, Loss: 6.9928, Pearson: 0.6507, Spearman: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6641
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6454
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.958029747009277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 36 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        8.405578  0.        0.
 0.        6.6149354 8.559696 ]
Sample y_pred values (first sample, first 10 genes):
[0.6467553  2.6719556  2.754034   1.2712423  5.3168283  3.5304708
 0.82504934 2.410137   4.310027   8.665856  ]
y_true  -> mean=2.1264, std=3.4742, min=0.0000, max=13.8155
y_pred  -> mean=2.0313, std=2.2172, min=0.0000, max=12.9211
Batch 0 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6507
2025-11-14 19:57:15,858 - INFO - Fold 3 Train Epoch 36/200, Batch 30, Loss: 7.0654, Pearson: 0.6536, Spearman: 0.6182
2025-11-14 19:57:26,030 - INFO - Fold 3 Train Epoch 36/200, Batch 40, Loss: 6.8896, Pearson: 0.6527, Spearman: 0.6186
2025-11-14 19:57:36,162 - INFO - Fold 3 Train Epoch 36/200, Batch 50, Loss: 6.9626, Pearson: 0.6638, Spearman: 0.6272
2025-11-14 19:57:44,906 - INFO - Fold 3 Train Epoch 36/200, Batch 60, Loss: 6.9607, Pearson: 0.6465, Spearman: 0.6123
2025-11-14 19:57:52,983 - INFO - Fold 3 Train Epoch 36/200, Batch 70, Loss: 6.7692, Pearson: 0.6465, Spearman: 0.6063
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6498
2025-11-14 19:58:03,146 - INFO - Fold 3 Train Epoch 36/200, Batch 80, Loss: 7.0808, Pearson: 0.6495, Spearman: 0.6115
2025-11-14 19:58:11,295 - INFO - Fold 3 Train Epoch 36/200, Train Loss: 6.9603, Pearson Mean: 0.6505, Spearman Mean: 0.6144
2025-11-14 19:58:11,295 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4525, 'spearman_mean_genewise': 0.4029, 'l1_error_mean': 1.8275, 'l2_errors_mean': 6.96, 'r2_scores_mean': 0.2165, 'pearson_std': 0.1109, 'l2_error_q1': 4.5463, 'l2_error_q2': 6.6316, 'l2_error_q3': 9.0966, 'r2_score_q1': 0.1387, 'r2_score_q2': 0.1918, 'r2_score_q3': 0.2589, 'mape_mean': 58.6104, 'mape_std': 18.1136, 'rmse_mean': 2.5881, 'rmse_std': 0.5116}
2025-11-14 19:58:11,650 - INFO - Fold 3 Val Epoch 36/200, Batch 0, Loss: 10.4944, Pearson: 0.4937, Spearman: 0.5024
2025-11-14 19:58:13,362 - INFO - Fold 3 Val Epoch 36/200, Batch 10, Loss: 8.4204, Pearson: 0.6036, Spearman: 0.5780
2025-11-14 19:58:16,987 - INFO - Fold 3 Val Epoch 36/200, Val Loss: 8.5549, Pearson Mean: 0.5618, Spearman Mean: 0.5574
2025-11-14 19:58:16,988 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3183, 'spearman_mean_genewise': 0.3018, 'l1_error_mean': 2.1812, 'l2_errors_mean': 8.5847, 'r2_scores_mean': 0.109, 'pearson_std': 0.1399, 'l2_error_q1': 6.0603, 'l2_error_q2': 8.4502, 'l2_error_q3': 10.8489, 'r2_score_q1': 0.0395, 'r2_score_q2': 0.0751, 'r2_score_q3': 0.1385, 'mape_mean': 62.0996, 'mape_std': 20.9488, 'rmse_mean': 2.8841, 'rmse_std': 0.5165}
2025-11-14 19:58:16,988 - INFO - Learning rate for epoch 36: 1.0000000000000002e-07
2025-11-14 19:58:16,988 - INFO - No improvement in spearman genewise. Patience: 21/30
2025-11-14 19:58:17,904 - INFO - Fold 3 Train Epoch 37/200, Batch 0, Loss: 6.8466, Pearson: 0.6419, Spearman: 0.6150
2025-11-14 19:58:28,108 - INFO - Fold 3 Train Epoch 37/200, Batch 10, Loss: 7.0372, Pearson: 0.6561, Spearman: 0.6194
2025-11-14 19:58:38,266 - INFO - Fold 3 Train Epoch 37/200, Batch 20, Loss: 6.9297, Pearson: 0.6580, Spearman: 0.6195
2025-11-14 19:58:48,446 - INFO - Fold 3 Train Epoch 37/200, Batch 30, Loss: 6.7248, Pearson: 0.6589, Spearman: 0.6201
2025-11-14 19:58:58,584 - INFO - Fold 3 Train Epoch 37/200, Batch 40, Loss: 6.6813, Pearson: 0.6493, Spearman: 0.6085
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6505
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6506
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.959963798522949
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 37 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        7.5025144 0.        0.
 0.        7.5025144 8.195386 ]
Sample y_pred values (first sample, first 10 genes):
[1.2396647e-01 5.9791654e-04 7.7026105e-01 3.2542592e-01 2.8351269e+00
 1.1115553e+00 6.6014171e-02 1.0047327e+00 3.2158403e+00 7.3840284e+00]
y_true  -> mean=1.9054, std=3.4088, min=0.0000, max=12.4692
y_pred  -> mean=2.0251, std=2.1761, min=0.0000, max=13.5704
Batch 0 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6495
2025-11-14 19:59:08,739 - INFO - Fold 3 Train Epoch 37/200, Batch 50, Loss: 6.9654, Pearson: 0.6514, Spearman: 0.6169
2025-11-14 19:59:17,419 - INFO - Fold 3 Train Epoch 37/200, Batch 60, Loss: 6.8490, Pearson: 0.6436, Spearman: 0.6078
2025-11-14 19:59:25,609 - INFO - Fold 3 Train Epoch 37/200, Batch 70, Loss: 6.8335, Pearson: 0.6467, Spearman: 0.6125
2025-11-14 19:59:35,803 - INFO - Fold 3 Train Epoch 37/200, Batch 80, Loss: 6.9418, Pearson: 0.6573, Spearman: 0.6180
2025-11-14 19:59:43,972 - INFO - Fold 3 Train Epoch 37/200, Train Loss: 6.9569, Pearson Mean: 0.6507, Spearman Mean: 0.6145
2025-11-14 19:59:43,972 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4529, 'spearman_mean_genewise': 0.4032, 'l1_error_mean': 1.8265, 'l2_errors_mean': 6.9567, 'r2_scores_mean': 0.2168, 'pearson_std': 0.1108, 'l2_error_q1': 4.5476, 'l2_error_q2': 6.6266, 'l2_error_q3': 9.0753, 'r2_score_q1': 0.1394, 'r2_score_q2': 0.1919, 'r2_score_q3': 0.2594, 'mape_mean': 58.5846, 'mape_std': 18.1086, 'rmse_mean': 2.5875, 'rmse_std': 0.5115}
2025-11-14 19:59:44,323 - INFO - Fold 3 Val Epoch 37/200, Batch 0, Loss: 10.4899, Pearson: 0.4954, Spearman: 0.5047
2025-11-14 19:59:46,056 - INFO - Fold 3 Val Epoch 37/200, Batch 10, Loss: 8.3882, Pearson: 0.6047, Spearman: 0.5787
2025-11-14 19:59:49,664 - INFO - Fold 3 Val Epoch 37/200, Val Loss: 8.5756, Pearson Mean: 0.5601, Spearman Mean: 0.5556
2025-11-14 19:59:49,665 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3139, 'spearman_mean_genewise': 0.3016, 'l1_error_mean': 2.1784, 'l2_errors_mean': 8.606, 'r2_scores_mean': 0.1047, 'pearson_std': 0.1427, 'l2_error_q1': 6.1386, 'l2_error_q2': 8.477, 'l2_error_q3': 10.8377, 'r2_score_q1': 0.0329, 'r2_score_q2': 0.0713, 'r2_score_q3': 0.1377, 'mape_mean': 62.1309, 'mape_std': 20.8878, 'rmse_mean': 2.8889, 'rmse_std': 0.5104}
2025-11-14 19:59:49,665 - INFO - Learning rate for epoch 37: 1.0000000000000002e-07
2025-11-14 19:59:49,665 - INFO - No improvement in spearman genewise. Patience: 22/30
2025-11-14 19:59:50,569 - INFO - Fold 3 Train Epoch 38/200, Batch 0, Loss: 6.8195, Pearson: 0.6520, Spearman: 0.6125
2025-11-14 20:00:00,759 - INFO - Fold 3 Train Epoch 38/200, Batch 10, Loss: 6.8438, Pearson: 0.6532, Spearman: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6514
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6429
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.956668376922607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 38 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        7.8194604
 0.        7.8194604 7.8194604]
Sample y_pred values (first sample, first 10 genes):
[0.44634902 0.         1.0095276  0.16258271 1.781746   0.7783519
 0.5991528  0.66177833 2.2590246  7.4176354 ]
y_true  -> mean=1.9991, std=3.4438, min=0.0000, max=13.0080
y_pred  -> mean=2.0295, std=2.2382, min=0.0000, max=13.5246
Batch 0 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6532
2025-11-14 20:00:10,920 - INFO - Fold 3 Train Epoch 38/200, Batch 20, Loss: 6.8705, Pearson: 0.6554, Spearman: 0.6190
2025-11-14 20:00:21,075 - INFO - Fold 3 Train Epoch 38/200, Batch 30, Loss: 6.9365, Pearson: 0.6621, Spearman: 0.6188
2025-11-14 20:00:31,224 - INFO - Fold 3 Train Epoch 38/200, Batch 40, Loss: 6.7122, Pearson: 0.6550, Spearman: 0.6157
2025-11-14 20:00:41,390 - INFO - Fold 3 Train Epoch 38/200, Batch 50, Loss: 6.8534, Pearson: 0.6420, Spearman: 0.6121
2025-11-14 20:00:50,091 - INFO - Fold 3 Train Epoch 38/200, Batch 60, Loss: 6.7383, Pearson: 0.6521, Spearman: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6454
2025-11-14 20:00:58,350 - INFO - Fold 3 Train Epoch 38/200, Batch 70, Loss: 6.9557, Pearson: 0.6544, Spearman: 0.6216
2025-11-14 20:01:08,531 - INFO - Fold 3 Train Epoch 38/200, Batch 80, Loss: 6.9516, Pearson: 0.6442, Spearman: 0.6090
2025-11-14 20:01:16,716 - INFO - Fold 3 Train Epoch 38/200, Train Loss: 6.9562, Pearson Mean: 0.6507, Spearman Mean: 0.6145
2025-11-14 20:01:16,716 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4529, 'spearman_mean_genewise': 0.4032, 'l1_error_mean': 1.8265, 'l2_errors_mean': 6.9565, 'r2_scores_mean': 0.2168, 'pearson_std': 0.1108, 'l2_error_q1': 4.5499, 'l2_error_q2': 6.6281, 'l2_error_q3': 9.0639, 'r2_score_q1': 0.1391, 'r2_score_q2': 0.1914, 'r2_score_q3': 0.2594, 'mape_mean': 58.582, 'mape_std': 18.1083, 'rmse_mean': 2.5875, 'rmse_std': 0.5113}
2025-11-14 20:01:17,092 - INFO - Fold 3 Val Epoch 38/200, Batch 0, Loss: 10.4879, Pearson: 0.4945, Spearman: 0.5036
2025-11-14 20:01:18,845 - INFO - Fold 3 Val Epoch 38/200, Batch 10, Loss: 8.3448, Pearson: 0.6058, Spearman: 0.5798
2025-11-14 20:01:22,485 - INFO - Fold 3 Val Epoch 38/200, Val Loss: 8.5391, Pearson Mean: 0.5626, Spearman Mean: 0.5582
2025-11-14 20:01:22,485 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3201, 'spearman_mean_genewise': 0.3024, 'l1_error_mean': 2.1841, 'l2_errors_mean': 8.5688, 'r2_scores_mean': 0.1107, 'pearson_std': 0.1393, 'l2_error_q1': 6.0561, 'l2_error_q2': 8.445, 'l2_error_q3': 10.832, 'r2_score_q1': 0.0404, 'r2_score_q2': 0.0771, 'r2_score_q3': 0.1406, 'mape_mean': 61.7521, 'mape_std': 20.7874, 'rmse_mean': 2.8813, 'rmse_std': 0.5165}
2025-11-14 20:01:22,485 - INFO - Learning rate for epoch 38: 1.0000000000000002e-07
2025-11-14 20:01:22,485 - INFO - No improvement in spearman genewise. Patience: 23/30
2025-11-14 20:01:23,560 - INFO - Fold 3 Train Epoch 39/200, Batch 0, Loss: 6.8078, Pearson: 0.6571, Spearman: 0.6148
2025-11-14 20:01:33,625 - INFO - Fold 3 Train Epoch 39/200, Batch 10, Loss: 6.8397, Pearson: 0.6447, Spearman: 0.6116
2025-11-14 20:01:43,812 - INFO - Fold 3 Train Epoch 39/200, Batch 20, Loss: 7.1447, Pearson: 0.6436, Spearman: 0.6120
2025-11-14 20:01:53,959 - INFO - Fold 3 Train Epoch 39/200, Batch 30, Loss: 7.1138, Pearson: 0.6495, Spearman: 0.6178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6542
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6443
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.95646858215332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 39 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.782841 0.       7.782841 0.       0.       7.782841
 0.       8.47578 ]
Sample y_pred values (first sample, first 10 genes):
[1.0657979  0.84739643 4.1547217  0.61359453 4.5411453  3.3129654
 0.70322573 2.9928102  3.313072   4.4087424 ]
y_true  -> mean=2.0768, std=3.4606, min=0.0000, max=12.5162
y_pred  -> mean=2.0346, std=2.2404, min=0.0000, max=13.0365
Batch 0 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6572
2025-11-14 20:02:04,124 - INFO - Fold 3 Train Epoch 39/200, Batch 40, Loss: 7.0260, Pearson: 0.6463, Spearman: 0.6206
2025-11-14 20:02:14,268 - INFO - Fold 3 Train Epoch 39/200, Batch 50, Loss: 6.6693, Pearson: 0.6484, Spearman: 0.6119
2025-11-14 20:02:22,932 - INFO - Fold 3 Train Epoch 39/200, Batch 60, Loss: 6.9192, Pearson: 0.6536, Spearman: 0.6138
2025-11-14 20:02:31,116 - INFO - Fold 3 Train Epoch 39/200, Batch 70, Loss: 6.9797, Pearson: 0.6586, Spearman: 0.6218
2025-11-14 20:02:41,287 - INFO - Fold 3 Train Epoch 39/200, Batch 80, Loss: 6.6505, Pearson: 0.6622, Spearman: 0.6169
2025-11-14 20:02:49,457 - INFO - Fold 3 Train Epoch 39/200, Train Loss: 6.9593, Pearson Mean: 0.6505, Spearman Mean: 0.6143
2025-11-14 20:02:49,457 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4524, 'spearman_mean_genewise': 0.4029, 'l1_error_mean': 1.8274, 'l2_errors_mean': 6.96, 'r2_scores_mean': 0.2164, 'pearson_std': 0.111, 'l2_error_q1': 4.5479, 'l2_error_q2': 6.6348, 'l2_error_q3': 9.0788, 'r2_score_q1': 0.1386, 'r2_score_q2': 0.1909, 'r2_score_q3': 0.2595, 'mape_mean': 58.6137, 'mape_std': 18.1186, 'rmse_mean': 2.5881, 'rmse_std': 0.5115}
2025-11-14 20:02:49,822 - INFO - Fold 3 Val Epoch 39/200, Batch 0, Loss: 10.4800, Pearson: 0.4950, Spearman: 0.5041
2025-11-14 20:02:51,565 - INFO - Fold 3 Val Epoch 39/200, Batch 10, Loss: 8.3329, Pearson: 0.6057, Spearman: 0.5798
2025-11-14 20:02:55,181 - INFO - Fold 3 Val Epoch 39/200, Val Loss: 8.5448, Pearson Mean: 0.5628, Spearman Mean: 0.5584
2025-11-14 20:02:55,181 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3204, 'spearman_mean_genewise': 0.3021, 'l1_error_mean': 2.1878, 'l2_errors_mean': 8.5741, 'r2_scores_mean': 0.1104, 'pearson_std': 0.1394, 'l2_error_q1': 6.0707, 'l2_error_q2': 8.464, 'l2_error_q3': 10.8307, 'r2_score_q1': 0.0407, 'r2_score_q2': 0.0768, 'r2_score_q3': 0.1415, 'mape_mean': 61.4949, 'mape_std': 20.9579, 'rmse_mean': 2.8819, 'rmse_std': 0.5182}
2025-11-14 20:02:55,181 - INFO - Learning rate for epoch 39: 1.0000000000000004e-08
2025-11-14 20:02:55,181 - INFO - No improvement in spearman genewise. Patience: 24/30
2025-11-14 20:02:56,109 - INFO - Fold 3 Train Epoch 40/200, Batch 0, Loss: 6.8482, Pearson: 0.6527, Spearman: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6543
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6559
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.959985733032227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 40 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        7.9493957 0.
 0.        0.        7.9493957]
Sample y_pred values (first sample, first 10 genes):
[0.8557205 1.4748073 3.0454261 0.9469297 3.4217997 4.6780896 1.3017894
 1.5347759 1.9301751 4.9600797]
y_true  -> mean=2.0338, std=3.4535, min=0.0000, max=12.4238
y_pred  -> mean=2.0257, std=2.2142, min=0.0000, max=13.1451
Batch 0 Pearson correlation: 0.6527
2025-11-14 20:03:06,295 - INFO - Fold 3 Train Epoch 40/200, Batch 10, Loss: 6.8309, Pearson: 0.6507, Spearman: 0.6109
2025-11-14 20:03:16,424 - INFO - Fold 3 Train Epoch 40/200, Batch 20, Loss: 6.8400, Pearson: 0.6473, Spearman: 0.6097
2025-11-14 20:03:26,583 - INFO - Fold 3 Train Epoch 40/200, Batch 30, Loss: 6.7736, Pearson: 0.6532, Spearman: 0.6142
2025-11-14 20:03:36,742 - INFO - Fold 3 Train Epoch 40/200, Batch 40, Loss: 7.1725, Pearson: 0.6367, Spearman: 0.6040
2025-11-14 20:03:46,923 - INFO - Fold 3 Train Epoch 40/200, Batch 50, Loss: 6.8859, Pearson: 0.6533, Spearman: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6392
2025-11-14 20:03:55,653 - INFO - Fold 3 Train Epoch 40/200, Batch 60, Loss: 7.1075, Pearson: 0.6556, Spearman: 0.6186
2025-11-14 20:04:03,695 - INFO - Fold 3 Train Epoch 40/200, Batch 70, Loss: 6.9318, Pearson: 0.6617, Spearman: 0.6203
2025-11-14 20:04:13,858 - INFO - Fold 3 Train Epoch 40/200, Batch 80, Loss: 6.9339, Pearson: 0.6514, Spearman: 0.6137
2025-11-14 20:04:21,941 - INFO - Fold 3 Train Epoch 40/200, Train Loss: 6.9569, Pearson Mean: 0.6507, Spearman Mean: 0.6145
2025-11-14 20:04:21,942 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4528, 'spearman_mean_genewise': 0.4031, 'l1_error_mean': 1.8265, 'l2_errors_mean': 6.9568, 'r2_scores_mean': 0.2168, 'pearson_std': 0.1109, 'l2_error_q1': 4.5376, 'l2_error_q2': 6.6224, 'l2_error_q3': 9.075, 'r2_score_q1': 0.1392, 'r2_score_q2': 0.1915, 'r2_score_q3': 0.2594, 'mape_mean': 58.5837, 'mape_std': 18.1065, 'rmse_mean': 2.5875, 'rmse_std': 0.5117}
2025-11-14 20:04:22,237 - INFO - Fold 3 Val Epoch 40/200, Batch 0, Loss: 10.4774, Pearson: 0.4957, Spearman: 0.5048
2025-11-14 20:04:23,929 - INFO - Fold 3 Val Epoch 40/200, Batch 10, Loss: 8.3378, Pearson: 0.6060, Spearman: 0.5801
2025-11-14 20:04:27,620 - INFO - Fold 3 Val Epoch 40/200, Val Loss: 8.5549, Pearson Mean: 0.5615, Spearman Mean: 0.5571
2025-11-14 20:04:27,621 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3182, 'spearman_mean_genewise': 0.3015, 'l1_error_mean': 2.1852, 'l2_errors_mean': 8.5841, 'r2_scores_mean': 0.1089, 'pearson_std': 0.1399, 'l2_error_q1': 6.097, 'l2_error_q2': 8.4444, 'l2_error_q3': 10.8546, 'r2_score_q1': 0.0379, 'r2_score_q2': 0.0753, 'r2_score_q3': 0.1392, 'mape_mean': 61.7697, 'mape_std': 20.8323, 'rmse_mean': 2.8841, 'rmse_std': 0.516}
2025-11-14 20:04:27,621 - INFO - Learning rate for epoch 40: 1.0000000000000004e-08
2025-11-14 20:04:27,621 - INFO - No improvement in spearman genewise. Patience: 25/30
2025-11-14 20:04:28,532 - INFO - Fold 3 Train Epoch 41/200, Batch 0, Loss: 6.9123, Pearson: 0.6518, Spearman: 0.6137
2025-11-14 20:04:38,716 - INFO - Fold 3 Train Epoch 41/200, Batch 10, Loss: 6.9703, Pearson: 0.6510, Spearman: 0.6152
2025-11-14 20:04:48,875 - INFO - Fold 3 Train Epoch 41/200, Batch 20, Loss: 6.8986, Pearson: 0.6584, Spearman: 0.6173
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6372
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6536
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.956812381744385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 41 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       6.959998 0.       8.34558  0.
 6.959998 8.34558 ]
Sample y_pred values (first sample, first 10 genes):
[0.37581992 0.         0.8885561  0.13328758 2.5447354  0.6967311
 1.165584   0.5253874  4.068469   7.704015  ]
y_true  -> mean=2.0149, std=3.4665, min=0.0000, max=12.8135
y_pred  -> mean=2.0298, std=2.2376, min=0.0000, max=13.9778
Batch 0 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6480
2025-11-14 20:04:59,036 - INFO - Fold 3 Train Epoch 41/200, Batch 30, Loss: 7.2842, Pearson: 0.6547, Spearman: 0.6216
2025-11-14 20:05:09,201 - INFO - Fold 3 Train Epoch 41/200, Batch 40, Loss: 6.9931, Pearson: 0.6468, Spearman: 0.6134
2025-11-14 20:05:19,336 - INFO - Fold 3 Train Epoch 41/200, Batch 50, Loss: 6.8633, Pearson: 0.6565, Spearman: 0.6192
2025-11-14 20:05:28,037 - INFO - Fold 3 Train Epoch 41/200, Batch 60, Loss: 7.2891, Pearson: 0.6478, Spearman: 0.6174
2025-11-14 20:05:36,277 - INFO - Fold 3 Train Epoch 41/200, Batch 70, Loss: 6.9813, Pearson: 0.6478, Spearman: 0.6142
2025-11-14 20:05:46,441 - INFO - Fold 3 Train Epoch 41/200, Batch 80, Loss: 6.7641, Pearson: 0.6627, Spearman: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6703
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6436
2025-11-14 20:05:54,585 - INFO - Fold 3 Train Epoch 41/200, Train Loss: 6.9606, Pearson Mean: 0.6505, Spearman Mean: 0.6143
2025-11-14 20:05:54,585 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4524, 'spearman_mean_genewise': 0.4028, 'l1_error_mean': 1.8275, 'l2_errors_mean': 6.9604, 'r2_scores_mean': 0.2164, 'pearson_std': 0.1109, 'l2_error_q1': 4.5535, 'l2_error_q2': 6.6285, 'l2_error_q3': 9.0618, 'r2_score_q1': 0.1386, 'r2_score_q2': 0.1913, 'r2_score_q3': 0.2591, 'mape_mean': 58.6106, 'mape_std': 18.1105, 'rmse_mean': 2.5882, 'rmse_std': 0.5117}
2025-11-14 20:05:54,886 - INFO - Fold 3 Val Epoch 41/200, Batch 0, Loss: 10.4708, Pearson: 0.4953, Spearman: 0.5043
2025-11-14 20:05:56,575 - INFO - Fold 3 Val Epoch 41/200, Batch 10, Loss: 8.3671, Pearson: 0.6049, Spearman: 0.5791
2025-11-14 20:06:00,248 - INFO - Fold 3 Val Epoch 41/200, Val Loss: 8.5927, Pearson Mean: 0.5587, Spearman Mean: 0.5541
2025-11-14 20:06:00,249 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3129, 'spearman_mean_genewise': 0.301, 'l1_error_mean': 2.1888, 'l2_errors_mean': 8.6227, 'r2_scores_mean': 0.1029, 'pearson_std': 0.1426, 'l2_error_q1': 6.1584, 'l2_error_q2': 8.4827, 'l2_error_q3': 10.8788, 'r2_score_q1': 0.0322, 'r2_score_q2': 0.0704, 'r2_score_q3': 0.1356, 'mape_mean': 61.8217, 'mape_std': 20.7335, 'rmse_mean': 2.8918, 'rmse_std': 0.5103}
2025-11-14 20:06:00,249 - INFO - Learning rate for epoch 41: 1.0000000000000004e-08
2025-11-14 20:06:00,249 - INFO - No improvement in spearman genewise. Patience: 26/30
2025-11-14 20:06:01,178 - INFO - Fold 3 Train Epoch 42/200, Batch 0, Loss: 6.9084, Pearson: 0.6584, Spearman: 0.6200
2025-11-14 20:06:11,369 - INFO - Fold 3 Train Epoch 42/200, Batch 10, Loss: 6.9137, Pearson: 0.6512, Spearman: 0.6206
2025-11-14 20:06:21,526 - INFO - Fold 3 Train Epoch 42/200, Batch 20, Loss: 6.9466, Pearson: 0.6501, Spearman: 0.6171
2025-11-14 20:06:31,681 - INFO - Fold 3 Train Epoch 42/200, Batch 30, Loss: 7.1936, Pearson: 0.6548, Spearman: 0.6192
2025-11-14 20:06:41,849 - INFO - Fold 3 Train Epoch 42/200, Batch 40, Loss: 6.8801, Pearson: 0.6506, Spearman: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6515
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6515
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.960386753082275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 42 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       8.765811 0.       0.       0.       0.
 8.765811 9.458879]
Sample y_pred values (first sample, first 10 genes):
[0.         0.04947457 1.1402296  0.         1.9923444  0.28894073
 0.16341887 0.2659394  1.4955702  6.510176  ]
y_true  -> mean=2.1188, std=3.4890, min=0.0000, max=12.6672
y_pred  -> mean=2.0300, std=2.2264, min=0.0000, max=13.6794
Batch 0 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6486
2025-11-14 20:06:51,978 - INFO - Fold 3 Train Epoch 42/200, Batch 50, Loss: 6.8615, Pearson: 0.6569, Spearman: 0.6145
2025-11-14 20:07:00,708 - INFO - Fold 3 Train Epoch 42/200, Batch 60, Loss: 7.2499, Pearson: 0.6471, Spearman: 0.6076
2025-11-14 20:07:08,865 - INFO - Fold 3 Train Epoch 42/200, Batch 70, Loss: 6.9536, Pearson: 0.6438, Spearman: 0.6131
2025-11-14 20:07:19,022 - INFO - Fold 3 Train Epoch 42/200, Batch 80, Loss: 6.8619, Pearson: 0.6557, Spearman: 0.6161
2025-11-14 20:07:27,209 - INFO - Fold 3 Train Epoch 42/200, Train Loss: 6.9557, Pearson Mean: 0.6507, Spearman Mean: 0.6145
2025-11-14 20:07:27,209 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4528, 'spearman_mean_genewise': 0.4032, 'l1_error_mean': 1.8265, 'l2_errors_mean': 6.9563, 'r2_scores_mean': 0.2168, 'pearson_std': 0.1109, 'l2_error_q1': 4.5461, 'l2_error_q2': 6.6219, 'l2_error_q3': 9.0687, 'r2_score_q1': 0.1387, 'r2_score_q2': 0.1917, 'r2_score_q3': 0.2594, 'mape_mean': 58.5821, 'mape_std': 18.1143, 'rmse_mean': 2.5875, 'rmse_std': 0.5112}
2025-11-14 20:07:27,605 - INFO - Fold 3 Val Epoch 42/200, Batch 0, Loss: 10.4700, Pearson: 0.4955, Spearman: 0.5044
2025-11-14 20:07:29,339 - INFO - Fold 3 Val Epoch 42/200, Batch 10, Loss: 8.3583, Pearson: 0.6049, Spearman: 0.5792
2025-11-14 20:07:32,969 - INFO - Fold 3 Val Epoch 42/200, Val Loss: 8.5547, Pearson Mean: 0.5620, Spearman Mean: 0.5576
2025-11-14 20:07:32,969 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3192, 'spearman_mean_genewise': 0.3012, 'l1_error_mean': 2.1902, 'l2_errors_mean': 8.5839, 'r2_scores_mean': 0.1096, 'pearson_std': 0.1392, 'l2_error_q1': 6.073, 'l2_error_q2': 8.4546, 'l2_error_q3': 10.8484, 'r2_score_q1': 0.0399, 'r2_score_q2': 0.0765, 'r2_score_q3': 0.1398, 'mape_mean': 61.6875, 'mape_std': 20.8581, 'rmse_mean': 2.8836, 'rmse_std': 0.5183}
2025-11-14 20:07:32,970 - INFO - Learning rate for epoch 42: 1.0000000000000004e-08
2025-11-14 20:07:32,970 - INFO - No improvement in spearman genewise. Patience: 27/30
2025-11-14 20:07:34,007 - INFO - Fold 3 Train Epoch 43/200, Batch 0, Loss: 6.9377, Pearson: 0.6576, Spearman: 0.6221
2025-11-14 20:07:44,111 - INFO - Fold 3 Train Epoch 43/200, Batch 10, Loss: 6.8675, Pearson: 0.6434, Spearman: 0.6072
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6492
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6453
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.956290245056152
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 43 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       8.074423 0.       0.       7.381587
 8.479784 8.074423]
Sample y_pred values (first sample, first 10 genes):
[0.40096784 0.38173115 2.2353835  0.29271978 4.7681866  1.1596783
 0.         1.7214429  5.1349807  9.49205   ]
y_true  -> mean=2.1187, std=3.4927, min=0.0000, max=12.6014
y_pred  -> mean=2.0323, std=2.2150, min=0.0000, max=12.6961
Batch 0 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6514
2025-11-14 20:07:54,288 - INFO - Fold 3 Train Epoch 43/200, Batch 20, Loss: 7.0006, Pearson: 0.6468, Spearman: 0.6126
2025-11-14 20:08:04,438 - INFO - Fold 3 Train Epoch 43/200, Batch 30, Loss: 7.1482, Pearson: 0.6427, Spearman: 0.6129
2025-11-14 20:08:14,613 - INFO - Fold 3 Train Epoch 43/200, Batch 40, Loss: 6.8834, Pearson: 0.6572, Spearman: 0.6182
2025-11-14 20:08:24,751 - INFO - Fold 3 Train Epoch 43/200, Batch 50, Loss: 6.8931, Pearson: 0.6603, Spearman: 0.6229
2025-11-14 20:08:33,435 - INFO - Fold 3 Train Epoch 43/200, Batch 60, Loss: 7.1124, Pearson: 0.6601, Spearman: 0.6155
2025-11-14 20:08:41,581 - INFO - Fold 3 Train Epoch 43/200, Batch 70, Loss: 6.9064, Pearson: 0.6470, Spearman: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6550
2025-11-14 20:08:51,744 - INFO - Fold 3 Train Epoch 43/200, Batch 80, Loss: 6.9779, Pearson: 0.6488, Spearman: 0.6216
2025-11-14 20:08:59,859 - INFO - Fold 3 Train Epoch 43/200, Train Loss: 6.9580, Pearson Mean: 0.6506, Spearman Mean: 0.6143
2025-11-14 20:08:59,859 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4526, 'spearman_mean_genewise': 0.403, 'l1_error_mean': 1.8274, 'l2_errors_mean': 6.959, 'r2_scores_mean': 0.2166, 'pearson_std': 0.1108, 'l2_error_q1': 4.5445, 'l2_error_q2': 6.6291, 'l2_error_q3': 9.0601, 'r2_score_q1': 0.138, 'r2_score_q2': 0.191, 'r2_score_q3': 0.2588, 'mape_mean': 58.6104, 'mape_std': 18.1088, 'rmse_mean': 2.5879, 'rmse_std': 0.5115}
2025-11-14 20:09:00,182 - INFO - Fold 3 Val Epoch 43/200, Batch 0, Loss: 10.4904, Pearson: 0.4949, Spearman: 0.5039
2025-11-14 20:09:01,880 - INFO - Fold 3 Val Epoch 43/200, Batch 10, Loss: 8.3589, Pearson: 0.6052, Spearman: 0.5793
2025-11-14 20:09:05,553 - INFO - Fold 3 Val Epoch 43/200, Val Loss: 8.5582, Pearson Mean: 0.5618, Spearman Mean: 0.5573
2025-11-14 20:09:05,554 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3185, 'spearman_mean_genewise': 0.3014, 'l1_error_mean': 2.1845, 'l2_errors_mean': 8.5874, 'r2_scores_mean': 0.1089, 'pearson_std': 0.1396, 'l2_error_q1': 6.0658, 'l2_error_q2': 8.4486, 'l2_error_q3': 10.855, 'r2_score_q1': 0.0395, 'r2_score_q2': 0.0757, 'r2_score_q3': 0.1385, 'mape_mean': 61.7871, 'mape_std': 20.9295, 'rmse_mean': 2.8843, 'rmse_std': 0.5176}
2025-11-14 20:09:05,554 - INFO - Learning rate for epoch 43: 1.0000000000000004e-08
2025-11-14 20:09:05,554 - INFO - No improvement in spearman genewise. Patience: 28/30
2025-11-14 20:09:06,433 - INFO - Fold 3 Train Epoch 44/200, Batch 0, Loss: 7.0766, Pearson: 0.6414, Spearman: 0.6101
2025-11-14 20:09:16,613 - INFO - Fold 3 Train Epoch 44/200, Batch 10, Loss: 6.9943, Pearson: 0.6457, Spearman: 0.6096
2025-11-14 20:09:26,772 - INFO - Fold 3 Train Epoch 44/200, Batch 20, Loss: 6.9230, Pearson: 0.6495, Spearman: 0.6114
2025-11-14 20:09:36,946 - INFO - Fold 3 Train Epoch 44/200, Batch 30, Loss: 6.7246, Pearson: 0.6567, Spearman: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6605
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6589
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.9589524269104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 44 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        6.8162723 0.        0.        0.
 0.        0.        8.201745 ]
Sample y_pred values (first sample, first 10 genes):
[0.23477374 0.84951276 1.1460505  0.4163343  3.8971982  2.32902
 0.5612047  0.6269439  4.432116   8.113889  ]
y_true  -> mean=2.0039, std=3.4671, min=0.0000, max=12.4524
y_pred  -> mean=2.0275, std=2.1915, min=0.0000, max=13.1183
Batch 0 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6560
2025-11-14 20:09:47,089 - INFO - Fold 3 Train Epoch 44/200, Batch 40, Loss: 6.7580, Pearson: 0.6515, Spearman: 0.6116
2025-11-14 20:09:57,234 - INFO - Fold 3 Train Epoch 44/200, Batch 50, Loss: 7.1894, Pearson: 0.6471, Spearman: 0.6123
2025-11-14 20:10:05,963 - INFO - Fold 3 Train Epoch 44/200, Batch 60, Loss: 6.9176, Pearson: 0.6568, Spearman: 0.6148
2025-11-14 20:10:14,117 - INFO - Fold 3 Train Epoch 44/200, Batch 70, Loss: 6.8927, Pearson: 0.6400, Spearman: 0.6051
2025-11-14 20:10:24,273 - INFO - Fold 3 Train Epoch 44/200, Batch 80, Loss: 7.0211, Pearson: 0.6502, Spearman: 0.6179
2025-11-14 20:10:32,436 - INFO - Fold 3 Train Epoch 44/200, Train Loss: 6.9571, Pearson Mean: 0.6507, Spearman Mean: 0.6144
2025-11-14 20:10:32,436 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4528, 'spearman_mean_genewise': 0.403, 'l1_error_mean': 1.8267, 'l2_errors_mean': 6.9574, 'r2_scores_mean': 0.2167, 'pearson_std': 0.1109, 'l2_error_q1': 4.5502, 'l2_error_q2': 6.6275, 'l2_error_q3': 9.0721, 'r2_score_q1': 0.139, 'r2_score_q2': 0.1906, 'r2_score_q3': 0.2603, 'mape_mean': 58.5814, 'mape_std': 18.1105, 'rmse_mean': 2.5876, 'rmse_std': 0.5115}
2025-11-14 20:10:32,752 - INFO - Fold 3 Val Epoch 44/200, Batch 0, Loss: 10.5028, Pearson: 0.4944, Spearman: 0.5035
2025-11-14 20:10:34,490 - INFO - Fold 3 Val Epoch 44/200, Batch 10, Loss: 8.3873, Pearson: 0.6049, Spearman: 0.5790
2025-11-14 20:10:38,142 - INFO - Fold 3 Val Epoch 44/200, Val Loss: 8.5431, Pearson Mean: 0.5624, Spearman Mean: 0.5580
2025-11-14 20:10:38,142 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3198, 'spearman_mean_genewise': 0.302, 'l1_error_mean': 2.1767, 'l2_errors_mean': 8.5728, 'r2_scores_mean': 0.1103, 'pearson_std': 0.1395, 'l2_error_q1': 6.0584, 'l2_error_q2': 8.4502, 'l2_error_q3': 10.8046, 'r2_score_q1': 0.0403, 'r2_score_q2': 0.0765, 'r2_score_q3': 0.1388, 'mape_mean': 62.239, 'mape_std': 20.8428, 'rmse_mean': 2.882, 'rmse_std': 0.5164}
2025-11-14 20:10:38,143 - INFO - Learning rate for epoch 44: 1.0000000000000004e-08
2025-11-14 20:10:38,143 - INFO - No improvement in spearman genewise. Patience: 29/30
2025-11-14 20:10:39,027 - INFO - Fold 3 Train Epoch 45/200, Batch 0, Loss: 6.9599, Pearson: 0.6433, Spearman: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6540
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6573
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.957359313964844
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 45 =====================
Sample y_true values (first sample, first 10 genes):
[7.6195574 0.        0.        0.        0.        8.312459  0.
 0.        0.        8.312459 ]
Sample y_pred values (first sample, first 10 genes):
[1.055552   0.80250335 4.886601   0.6737207  6.2006307  3.981009
 1.0147964  3.1275682  4.845688   5.8049526 ]
y_true  -> mean=1.9794, std=3.4450, min=0.0000, max=12.6253
y_pred  -> mean=2.0312, std=2.2150, min=0.0000, max=12.9394
Batch 0 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6487
2025-11-14 20:10:49,226 - INFO - Fold 3 Train Epoch 45/200, Batch 10, Loss: 6.7071, Pearson: 0.6540, Spearman: 0.6169
2025-11-14 20:10:59,373 - INFO - Fold 3 Train Epoch 45/200, Batch 20, Loss: 7.0475, Pearson: 0.6436, Spearman: 0.6133
2025-11-14 20:11:09,526 - INFO - Fold 3 Train Epoch 45/200, Batch 30, Loss: 7.1613, Pearson: 0.6448, Spearman: 0.6127
2025-11-14 20:11:19,685 - INFO - Fold 3 Train Epoch 45/200, Batch 40, Loss: 6.9709, Pearson: 0.6523, Spearman: 0.6124
2025-11-14 20:11:29,858 - INFO - Fold 3 Train Epoch 45/200, Batch 50, Loss: 7.0066, Pearson: 0.6420, Spearman: 0.6109
2025-11-14 20:11:38,551 - INFO - Fold 3 Train Epoch 45/200, Batch 60, Loss: 7.0886, Pearson: 0.6474, Spearman: 0.6093
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6561
2025-11-14 20:11:46,737 - INFO - Fold 3 Train Epoch 45/200, Batch 70, Loss: 6.9533, Pearson: 0.6561, Spearman: 0.6205
2025-11-14 20:11:56,899 - INFO - Fold 3 Train Epoch 45/200, Batch 80, Loss: 7.0547, Pearson: 0.6457, Spearman: 0.6140
2025-11-14 20:12:05,089 - INFO - Fold 3 Train Epoch 45/200, Train Loss: 6.9592, Pearson Mean: 0.6506, Spearman Mean: 0.6144
2025-11-14 20:12:05,090 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4525, 'spearman_mean_genewise': 0.4029, 'l1_error_mean': 1.8272, 'l2_errors_mean': 6.9592, 'r2_scores_mean': 0.2165, 'pearson_std': 0.1109, 'l2_error_q1': 4.5468, 'l2_error_q2': 6.6461, 'l2_error_q3': 9.0704, 'r2_score_q1': 0.1388, 'r2_score_q2': 0.1911, 'r2_score_q3': 0.2587, 'mape_mean': 58.5999, 'mape_std': 18.1178, 'rmse_mean': 2.588, 'rmse_std': 0.5115}
2025-11-14 20:12:05,463 - INFO - Fold 3 Val Epoch 45/200, Batch 0, Loss: 10.5088, Pearson: 0.4944, Spearman: 0.5035
2025-11-14 20:12:07,210 - INFO - Fold 3 Val Epoch 45/200, Batch 10, Loss: 8.4280, Pearson: 0.6041, Spearman: 0.5783
2025-11-14 20:12:10,835 - INFO - Fold 3 Val Epoch 45/200, Val Loss: 8.5505, Pearson Mean: 0.5620, Spearman Mean: 0.5576
2025-11-14 20:12:10,835 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3182, 'spearman_mean_genewise': 0.3019, 'l1_error_mean': 2.1715, 'l2_errors_mean': 8.5806, 'r2_scores_mean': 0.109, 'pearson_std': 0.1402, 'l2_error_q1': 6.0867, 'l2_error_q2': 8.4658, 'l2_error_q3': 10.8287, 'r2_score_q1': 0.0395, 'r2_score_q2': 0.075, 'r2_score_q3': 0.138, 'mape_mean': 62.4433, 'mape_std': 20.9268, 'rmse_mean': 2.8836, 'rmse_std': 0.5152}
2025-11-14 20:12:10,836 - INFO - Learning rate for epoch 45: 1.0000000000000004e-08
2025-11-14 20:12:10,836 - INFO - No improvement in spearman genewise. Patience: 30/30
2025-11-14 20:12:10,836 - INFO - Early stopping triggered. Breaking training loop.
2025-11-14 20:12:10,837 - INFO - ===== Completed Fold 3/5 =====
2025-11-14 20:12:10,837 - INFO - 
===== Starting Fold 4/5 =====
2025-11-14 20:12:10,837 - INFO - Fold 4: Train=29, Val=7
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 85 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 86 Pearson correlation: 0.6547
image : torch.Size([89, 3, 224, 224]), y_true: torch.Size([89, 785]), y_pred: torch.Size([89, 785])
Batch 87 Pearson correlation: 0.6664
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.95920991897583
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A1.h5ad
 Loaded images: (346, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (346, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A2.h5ad
 Loaded images: (325, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (325, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A3.h5ad
 Loaded images: (359, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (359, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A5.h5ad
 Loaded images: (332, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (332, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A6.h5ad
 Loaded images: (360, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (360, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B1.h5ad
 Loaded images: (295, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (295, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B2.h5ad
 Loaded images: (270, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (270, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B3.h5ad
 Loaded images: (298, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (298, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B4.h5ad
 Loaded images: (283, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (283, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B6.h5ad
 Loaded images: (277, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (277, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C1.h5ad
 Loaded images: (176, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (176, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C2.h5ad
 Loaded images: (187, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (187, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C3.h5ad
 Loaded images: (180, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (180, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C4.h5ad
 Loaded images: (184, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (184, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C5.h5ad
 Loaded images: (181, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (181, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C6.h5ad
 Loaded images: (178, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (178, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D1.h5ad
 Loaded images: (306, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (306, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D2.h5ad
 Loaded images: (303, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (303, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D3.h5ad
 Loaded images: (301, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (301, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D4.h5ad
 Loaded images: (302, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (302, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E1.h5ad
 Loaded images: (587, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (587, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E2.h5ad
 Loaded images: (572, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (572, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E3.h5ad
 Loaded images: (570, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (570, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F2.h5ad
 Loaded images: (695, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (695, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F3.h5ad
 Loaded images: (712, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (712, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G1.h5ad
 Loaded images: (441, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (441, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G2.h5ad
 Loaded images: (467, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (467, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H2.h5ad
 Loaded images: (603, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (603, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H3.h5ad
 Loaded images: (510, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (510, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A4.h5ad
 Loaded images: (343, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (343, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B5.h5ad
 Loaded images: (289, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (289, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D5.h5ad
 Loaded images: (306, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (306, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D6.h5ad
 Loaded images: (315, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (315, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F1.h5ad
 Loaded images: (691, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
2025-11-14 20:12:28,467 - INFO - Fold 4: Train=10600, Val=3020
2025-11-14 20:12:28,467 - INFO - train_datasets length:  10600
2025-11-14 20:12:28,467 - INFO - Number of train batches: 83
2025-11-14 20:12:28,467 - INFO - Initializing model...
2025-11-14 20:12:28,566 - INFO - Model
STNet(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (denseblock1): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition1): _Transition(
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock2): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition2): _Transition(
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock3): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer17): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer18): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer19): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer20): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer21): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer22): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer23): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer24): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition3): _Transition(
      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock4): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=1024, out_features=785, bias=True)
)
2025-11-14 20:12:28,568 - INFO - Using device: cuda
2025-11-14 20:12:29,451 - INFO - Fold 4 Train Epoch 1/200, Batch 0, Loss: 16.8883, Pearson: 0.0006, Spearman: 0.0097
2025-11-14 20:12:39,523 - INFO - Fold 4 Train Epoch 1/200, Batch 10, Loss: 15.2743, Pearson: 0.1337, Spearman: 0.0757
2025-11-14 20:12:49,633 - INFO - Fold 4 Train Epoch 1/200, Batch 20, Loss: 13.9332, Pearson: 0.2303, Spearman: 0.1429
2025-11-14 20:12:58,695 - INFO - Fold 4 Train Epoch 1/200, Batch 30, Loss: 13.2544, Pearson: 0.2971, Spearman: 0.2357
2025-11-14 20:13:05,882 - INFO - Fold 4 Train Epoch 1/200, Batch 40, Loss: 12.1690, Pearson: 0.3250, Spearman: 0.3249
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (691, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G3.h5ad
 Loaded images: (463, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (463, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H1.h5ad
 Loaded images: (613, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (613, 785)
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 1 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.9898496 0.        0.        0.        7.297041
 7.297041  0.        8.905937 ]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.         0.01835527 0.15780148 0.30354548
 0.         0.30301172 0.19142963 0.        ]
y_true  -> mean=2.2551, std=3.5301, min=0.0000, max=13.1224
y_pred  -> mean=0.1650, std=0.2433, min=0.0000, max=1.8336
Batch 0 Pearson correlation: 0.0006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.0140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.0347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.0443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.0603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.0832
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.0840
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.1102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.1137
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.1224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.1337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.1382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.1636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.1789
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.1828
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.1847
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.1931
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.2275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.2157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.2240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.2303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.2447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.2382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.2410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.2741
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.2343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.2362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.2801
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.2742
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.2419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.2971
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.3275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.3509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.3077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.3058
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.3625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.3122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.3104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.3399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.2809
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.3250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.4101
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.3539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.3538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.3701
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.3684
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.3915
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.4395
2025-11-14 20:13:15,973 - INFO - Fold 4 Train Epoch 1/200, Batch 50, Loss: 10.0653, Pearson: 0.4081, Spearman: 0.4060
2025-11-14 20:13:26,103 - INFO - Fold 4 Train Epoch 1/200, Batch 60, Loss: 9.7401, Pearson: 0.4584, Spearman: 0.4394
2025-11-14 20:13:36,214 - INFO - Fold 4 Train Epoch 1/200, Batch 70, Loss: 9.4602, Pearson: 0.4949, Spearman: 0.4662
2025-11-14 20:13:46,356 - INFO - Fold 4 Train Epoch 1/200, Batch 80, Loss: 9.1538, Pearson: 0.4777, Spearman: 0.4721
2025-11-14 20:13:49,641 - INFO - Fold 4 Train Epoch 1/200, Train Loss: 11.9600, Pearson Mean: 0.3284, Spearman Mean: 0.2964
2025-11-14 20:13:49,641 - INFO - Training Metrics: {'pearson_mean_genewise': 0.179, 'spearman_mean_genewise': 0.164, 'l1_error_mean': 2.2671, 'l2_errors_mean': 11.9648, 'r2_scores_mean': -0.1858, 'pearson_std': 0.067, 'l2_error_q1': 5.8662, 'l2_error_q2': 8.8813, 'l2_error_q3': 14.9993, 'r2_score_q1': -0.1275, 'r2_score_q2': -0.0202, 'r2_score_q3': 0.0091, 'mape_mean': 79.9391, 'mape_std': 6.9206, 'rmse_mean': 3.2746, 'rmse_std': 1.1144}
2025-11-14 20:13:49,986 - INFO - Fold 4 Val Epoch 1/200, Batch 0, Loss: 10.1622, Pearson: 0.5262, Spearman: 0.5317
2025-11-14 20:13:51,696 - INFO - Fold 4 Val Epoch 1/200, Batch 10, Loss: 9.5757, Pearson: 0.5192, Spearman: 0.5234
2025-11-14 20:13:53,330 - INFO - Fold 4 Val Epoch 1/200, Batch 20, Loss: 8.4623, Pearson: 0.2957, Spearman: 0.3259
2025-11-14 20:13:56,107 - INFO - Fold 4 Val Epoch 1/200, Val Loss: 9.5050, Pearson Mean: 0.4318, Spearman Mean: 0.4484
2025-11-14 20:13:56,107 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2293, 'spearman_mean_genewise': 0.2096, 'l1_error_mean': 2.2445, 'l2_errors_mean': 9.5149, 'r2_scores_mean': -0.0322, 'pearson_std': 0.0949, 'l2_error_q1': 5.2579, 'l2_error_q2': 8.1426, 'l2_error_q3': 12.7099, 'r2_score_q1': -0.0013, 'r2_score_q2': 0.0236, 'r2_score_q3': 0.0566, 'mape_mean': 70.2147, 'mape_std': 14.4068, 'rmse_mean': 2.9701, 'rmse_std': 0.8326}
2025-11-14 20:13:56,107 - INFO - Learning rate for epoch 1: 0.0001
2025-11-14 20:13:56,155 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:13:57,126 - INFO - Fold 4 Train Epoch 2/200, Batch 0, Loss: 9.1504, Pearson: 0.5086, Spearman: 0.4894
2025-11-14 20:14:07,181 - INFO - Fold 4 Train Epoch 2/200, Batch 10, Loss: 9.6456, Pearson: 0.5271, Spearman: 0.5101
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.4138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.4229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.4081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.4075
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.4277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.4289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.4212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.4314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.4357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.4379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.4682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.4490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.4584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.4479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.4591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.4789
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.4522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.4631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.4901
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.4836
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.4600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.4460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.4949
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5012
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.4730
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.4638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.4828
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.4660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.4826
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.4862
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.4651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.4873
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.4777
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.4851
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.4915
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 11.964810371398926
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 2 =====================
Sample y_true values (first sample, first 10 genes):
[0.        9.1060915 0.        0.        0.        0.        0.
 0.        0.        8.413055 ]
Sample y_pred values (first sample, first 10 genes):
[1.1976928  1.8094786  2.3748384  1.2344764  4.0034924  3.132358
 0.79303813 1.772418   3.5269175  4.5744104 ]
y_true  -> mean=2.1193, std=3.4913, min=0.0000, max=12.6524
y_pred  -> mean=1.8146, std=1.6287, min=0.0000, max=9.3445
Batch 0 Pearson correlation: 0.5086
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5063
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5011
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5111
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5095
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.4937
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5049
2025-11-14 20:14:17,338 - INFO - Fold 4 Train Epoch 2/200, Batch 20, Loss: 8.7500, Pearson: 0.5355, Spearman: 0.5091
2025-11-14 20:14:27,495 - INFO - Fold 4 Train Epoch 2/200, Batch 30, Loss: 9.1291, Pearson: 0.5170, Spearman: 0.5079
2025-11-14 20:14:35,261 - INFO - Fold 4 Train Epoch 2/200, Batch 40, Loss: 8.8951, Pearson: 0.5368, Spearman: 0.5176
2025-11-14 20:14:43,806 - INFO - Fold 4 Train Epoch 2/200, Batch 50, Loss: 8.6478, Pearson: 0.5479, Spearman: 0.5234
2025-11-14 20:14:53,994 - INFO - Fold 4 Train Epoch 2/200, Batch 60, Loss: 8.7600, Pearson: 0.5363, Spearman: 0.5240
2025-11-14 20:15:04,143 - INFO - Fold 4 Train Epoch 2/200, Batch 70, Loss: 8.2912, Pearson: 0.5470, Spearman: 0.5227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.4948
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5119
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5031
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5589
2025-11-14 20:15:14,300 - INFO - Fold 4 Train Epoch 2/200, Batch 80, Loss: 8.5136, Pearson: 0.5536, Spearman: 0.5372
2025-11-14 20:15:17,600 - INFO - Fold 4 Train Epoch 2/200, Train Loss: 8.7798, Pearson Mean: 0.5331, Spearman Mean: 0.5115
2025-11-14 20:15:17,600 - INFO - Training Metrics: {'pearson_mean_genewise': 0.305, 'spearman_mean_genewise': 0.2839, 'l1_error_mean': 2.171, 'l2_errors_mean': 8.781, 'r2_scores_mean': 0.0699, 'pearson_std': 0.0944, 'l2_error_q1': 5.4441, 'l2_error_q2': 7.8674, 'l2_error_q3': 11.6427, 'r2_score_q1': 0.045, 'r2_score_q2': 0.0788, 'r2_score_q3': 0.1168, 'mape_mean': 66.7064, 'mape_std': 15.5434, 'rmse_mean': 2.8903, 'rmse_std': 0.6534}
2025-11-14 20:15:17,957 - INFO - Fold 4 Val Epoch 2/200, Batch 0, Loss: 9.7603, Pearson: 0.5470, Spearman: 0.5513
2025-11-14 20:15:19,693 - INFO - Fold 4 Val Epoch 2/200, Batch 10, Loss: 8.9944, Pearson: 0.5537, Spearman: 0.5586
2025-11-14 20:15:21,433 - INFO - Fold 4 Val Epoch 2/200, Batch 20, Loss: 7.6712, Pearson: 0.3909, Spearman: 0.4165
2025-11-14 20:15:24,278 - INFO - Fold 4 Val Epoch 2/200, Val Loss: 8.8908, Pearson Mean: 0.4845, Spearman Mean: 0.4945
2025-11-14 20:15:24,279 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2594, 'spearman_mean_genewise': 0.2393, 'l1_error_mean': 2.1367, 'l2_errors_mean': 8.9034, 'r2_scores_mean': 0.0284, 'pearson_std': 0.0988, 'l2_error_q1': 5.1677, 'l2_error_q2': 8.0215, 'l2_error_q3': 12.2448, 'r2_score_q1': 0.0202, 'r2_score_q2': 0.0463, 'r2_score_q3': 0.0856, 'mape_mean': 69.5338, 'mape_std': 16.0655, 'rmse_mean': 2.8919, 'rmse_std': 0.7351}
2025-11-14 20:15:24,279 - INFO - Learning rate for epoch 2: 0.0001
2025-11-14 20:15:24,344 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:15:25,376 - INFO - Fold 4 Train Epoch 3/200, Batch 0, Loss: 8.5868, Pearson: 0.5513, Spearman: 0.5272
2025-11-14 20:15:35,578 - INFO - Fold 4 Train Epoch 3/200, Batch 10, Loss: 8.2846, Pearson: 0.5618, Spearman: 0.5311
2025-11-14 20:15:45,748 - INFO - Fold 4 Train Epoch 3/200, Batch 20, Loss: 8.5243, Pearson: 0.5606, Spearman: 0.5352
2025-11-14 20:15:55,916 - INFO - Fold 4 Train Epoch 3/200, Batch 30, Loss: 8.4251, Pearson: 0.5583, Spearman: 0.5331
2025-11-14 20:16:04,843 - INFO - Fold 4 Train Epoch 3/200, Batch 40, Loss: 8.0936, Pearson: 0.5683, Spearman: 0.5351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5641
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.5409
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.780969619750977
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 3 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        7.5679846 0.        0.
 8.260874  7.5679846 8.260874 ]
Sample y_pred values (first sample, first 10 genes):
[0.5234345  2.6814122  2.1250348  1.1758778  4.6477995  3.0385034
 0.55019206 1.8546329  2.888647   7.489318  ]
y_true  -> mean=2.1660, std=3.5018, min=0.0000, max=12.7568
y_pred  -> mean=1.9566, std=1.8463, min=0.0000, max=11.7211
Batch 0 Pearson correlation: 0.5513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5769
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5759
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5737
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5696
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5762
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5733
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5762
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5683
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5713
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5680
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5664
2025-11-14 20:16:12,711 - INFO - Fold 4 Train Epoch 3/200, Batch 50, Loss: 8.5703, Pearson: 0.5807, Spearman: 0.5412
2025-11-14 20:16:22,903 - INFO - Fold 4 Train Epoch 3/200, Batch 60, Loss: 8.0364, Pearson: 0.5684, Spearman: 0.5369
2025-11-14 20:16:33,041 - INFO - Fold 4 Train Epoch 3/200, Batch 70, Loss: 8.3583, Pearson: 0.5867, Spearman: 0.5584
2025-11-14 20:16:43,203 - INFO - Fold 4 Train Epoch 3/200, Batch 80, Loss: 8.0588, Pearson: 0.5721, Spearman: 0.5428
2025-11-14 20:16:46,511 - INFO - Fold 4 Train Epoch 3/200, Train Loss: 8.3332, Pearson Mean: 0.5639, Spearman Mean: 0.5364
2025-11-14 20:16:46,512 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3373, 'spearman_mean_genewise': 0.3113, 'l1_error_mean': 2.0924, 'l2_errors_mean': 8.3329, 'r2_scores_mean': 0.1084, 'pearson_std': 0.1002, 'l2_error_q1': 5.3323, 'l2_error_q2': 7.7156, 'l2_error_q3': 11.1434, 'r2_score_q1': 0.0624, 'r2_score_q2': 0.102, 'r2_score_q3': 0.1498, 'mape_mean': 64.1601, 'mape_std': 16.7321, 'rmse_mean': 2.8262, 'rmse_std': 0.588}
2025-11-14 20:16:46,854 - INFO - Fold 4 Val Epoch 3/200, Batch 0, Loss: 9.8222, Pearson: 0.5497, Spearman: 0.5563
2025-11-14 20:16:48,496 - INFO - Fold 4 Val Epoch 3/200, Batch 10, Loss: 8.7116, Pearson: 0.5672, Spearman: 0.5732
2025-11-14 20:16:50,218 - INFO - Fold 4 Val Epoch 3/200, Batch 20, Loss: 7.2455, Pearson: 0.4391, Spearman: 0.4556
2025-11-14 20:16:53,075 - INFO - Fold 4 Val Epoch 3/200, Val Loss: 8.6292, Pearson Mean: 0.5070, Spearman Mean: 0.5143
2025-11-14 20:16:53,076 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2748, 'spearman_mean_genewise': 0.2533, 'l1_error_mean': 2.1005, 'l2_errors_mean': 8.6356, 'r2_scores_mean': 0.0523, 'pearson_std': 0.1019, 'l2_error_q1': 5.119, 'l2_error_q2': 7.8768, 'l2_error_q3': 11.9112, 'r2_score_q1': 0.0261, 'r2_score_q2': 0.0545, 'r2_score_q3': 0.0985, 'mape_mean': 68.3306, 'mape_std': 17.0156, 'rmse_mean': 2.8562, 'rmse_std': 0.6912}
2025-11-14 20:16:53,076 - INFO - Learning rate for epoch 3: 0.0001
2025-11-14 20:16:53,140 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:16:54,157 - INFO - Fold 4 Train Epoch 4/200, Batch 0, Loss: 8.1932, Pearson: 0.5682, Spearman: 0.5439
2025-11-14 20:17:04,278 - INFO - Fold 4 Train Epoch 4/200, Batch 10, Loss: 7.7258, Pearson: 0.5909, Spearman: 0.5552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5688
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5807
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5823
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5748
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5673
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5766
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5763
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5684
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5743
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5725
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5762
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5705
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5867
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5804
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5681
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5735
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5840
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5711
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5721
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5597
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.5485
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.332884788513184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 4 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        6.600162  5.9083743 5.9083743 0.        0.
 5.9083743 7.2926292 9.306942 ]
Sample y_pred values (first sample, first 10 genes):
[1.1619487 4.168643  3.6500392 2.1747665 6.9541264 5.1892157 1.4714247
 3.5981767 4.6751757 9.950363 ]
y_true  -> mean=2.0814, std=3.4771, min=0.0000, max=12.5442
y_pred  -> mean=2.0134, std=2.0226, min=0.0000, max=12.8220
Batch 0 Pearson correlation: 0.5682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5721
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5779
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5826
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5742
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5778
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5793
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5739
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5801
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5752
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5909
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5769
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5914
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5913
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6010
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5923
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5793
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5863
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5820
2025-11-14 20:17:14,464 - INFO - Fold 4 Train Epoch 4/200, Batch 20, Loss: 7.6833, Pearson: 0.5874, Spearman: 0.5518
2025-11-14 20:17:24,622 - INFO - Fold 4 Train Epoch 4/200, Batch 30, Loss: 8.0331, Pearson: 0.5965, Spearman: 0.5566
2025-11-14 20:17:34,346 - INFO - Fold 4 Train Epoch 4/200, Batch 40, Loss: 7.8870, Pearson: 0.5939, Spearman: 0.5599
2025-11-14 20:17:41,348 - INFO - Fold 4 Train Epoch 4/200, Batch 50, Loss: 8.0177, Pearson: 0.5857, Spearman: 0.5467
2025-11-14 20:17:51,461 - INFO - Fold 4 Train Epoch 4/200, Batch 60, Loss: 8.1889, Pearson: 0.5790, Spearman: 0.5498
2025-11-14 20:18:01,616 - INFO - Fold 4 Train Epoch 4/200, Batch 70, Loss: 7.7725, Pearson: 0.5902, Spearman: 0.5583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5886
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5874
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5840
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5854
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5776
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6026
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5856
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5853
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5939
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5834
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5842
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5965
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5848
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5890
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5813
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5860
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5726
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5731
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5768
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5830
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5809
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5939
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5815
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5949
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5772
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5822
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5855
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5783
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5720
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5779
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5857
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5857
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5868
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5975
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5701
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5877
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5816
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5921
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5674
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5790
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5840
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5808
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5777
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5886
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5802
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5762
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5827
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5748
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5854
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5902
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5852
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5977
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5875
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5958
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5807
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5869
2025-11-14 20:18:11,768 - INFO - Fold 4 Train Epoch 4/200, Batch 80, Loss: 8.1850, Pearson: 0.5752, Spearman: 0.5448
2025-11-14 20:18:15,075 - INFO - Fold 4 Train Epoch 4/200, Train Loss: 8.0611, Pearson Mean: 0.5834, Spearman Mean: 0.5508
2025-11-14 20:18:15,075 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3606, 'spearman_mean_genewise': 0.3295, 'l1_error_mean': 2.0464, 'l2_errors_mean': 8.0609, 'r2_scores_mean': 0.132, 'pearson_std': 0.1036, 'l2_error_q1': 5.268, 'l2_error_q2': 7.5569, 'l2_error_q3': 10.7254, 'r2_score_q1': 0.0765, 'r2_score_q2': 0.1195, 'r2_score_q3': 0.1694, 'mape_mean': 62.7674, 'mape_std': 17.0907, 'rmse_mean': 2.7844, 'rmse_std': 0.5552}
2025-11-14 20:18:15,448 - INFO - Fold 4 Val Epoch 4/200, Batch 0, Loss: 9.7553, Pearson: 0.5478, Spearman: 0.5551
2025-11-14 20:18:17,113 - INFO - Fold 4 Val Epoch 4/200, Batch 10, Loss: 8.8184, Pearson: 0.5588, Spearman: 0.5673
2025-11-14 20:18:18,819 - INFO - Fold 4 Val Epoch 4/200, Batch 20, Loss: 7.3763, Pearson: 0.4307, Spearman: 0.4494
2025-11-14 20:18:21,674 - INFO - Fold 4 Val Epoch 4/200, Val Loss: 8.6775, Pearson Mean: 0.5045, Spearman Mean: 0.5121
2025-11-14 20:18:21,675 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2753, 'spearman_mean_genewise': 0.255, 'l1_error_mean': 2.0804, 'l2_errors_mean': 8.6868, 'r2_scores_mean': 0.0481, 'pearson_std': 0.1044, 'l2_error_q1': 5.1253, 'l2_error_q2': 7.8897, 'l2_error_q3': 11.941, 'r2_score_q1': 0.0239, 'r2_score_q2': 0.0533, 'r2_score_q3': 0.0953, 'mape_mean': 68.1301, 'mape_std': 16.9817, 'rmse_mean': 2.8627, 'rmse_std': 0.7011}
2025-11-14 20:18:21,675 - INFO - Learning rate for epoch 4: 0.0001
2025-11-14 20:18:21,739 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:18:22,695 - INFO - Fold 4 Train Epoch 5/200, Batch 0, Loss: 8.1280, Pearson: 0.5822, Spearman: 0.5560
2025-11-14 20:18:32,815 - INFO - Fold 4 Train Epoch 5/200, Batch 10, Loss: 8.0258, Pearson: 0.5950, Spearman: 0.5575
2025-11-14 20:18:42,955 - INFO - Fold 4 Train Epoch 5/200, Batch 20, Loss: 8.2080, Pearson: 0.5853, Spearman: 0.5531
2025-11-14 20:18:53,077 - INFO - Fold 4 Train Epoch 5/200, Batch 30, Loss: 8.1610, Pearson: 0.5850, Spearman: 0.5529
2025-11-14 20:19:03,256 - INFO - Fold 4 Train Epoch 5/200, Batch 40, Loss: 8.2785, Pearson: 0.5819, Spearman: 0.5525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5941
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5832
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5752
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5729
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.5744
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.060893058776855
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 5 =====================
Sample y_true values (first sample, first 10 genes):
[7.16989   0.        7.16989   0.        7.8626523 7.16989   0.
 0.        7.16989   0.       ]
Sample y_pred values (first sample, first 10 genes):
[1.4230275  2.0676174  4.0647597  0.9137681  4.362589   4.47521
 0.65110415 2.1073825  3.827186   5.130698  ]
y_true  -> mean=2.1177, std=3.5020, min=0.0000, max=12.5665
y_pred  -> mean=2.0007, std=1.9580, min=0.0000, max=12.6597
Batch 0 Pearson correlation: 0.5822
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5895
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5959
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5816
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5873
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5824
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5858
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5903
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5891
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5972
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5950
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5907
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5852
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5903
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5997
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5944
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5975
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5979
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6020
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5862
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5853
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5827
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5935
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5860
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5956
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5922
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5803
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5864
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5897
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5850
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5849
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5813
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5709
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5821
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5852
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5802
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5956
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5797
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5945
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5819
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5901
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5948
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6014
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5870
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6004
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5916
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5993
2025-11-14 20:19:10,937 - INFO - Fold 4 Train Epoch 5/200, Batch 50, Loss: 8.0673, Pearson: 0.5917, Spearman: 0.5602
2025-11-14 20:19:19,994 - INFO - Fold 4 Train Epoch 5/200, Batch 60, Loss: 7.7802, Pearson: 0.6006, Spearman: 0.5616
2025-11-14 20:19:30,141 - INFO - Fold 4 Train Epoch 5/200, Batch 70, Loss: 7.9376, Pearson: 0.6010, Spearman: 0.5711
2025-11-14 20:19:40,290 - INFO - Fold 4 Train Epoch 5/200, Batch 80, Loss: 7.8066, Pearson: 0.6078, Spearman: 0.5706
2025-11-14 20:19:43,556 - INFO - Fold 4 Train Epoch 5/200, Train Loss: 7.9330, Pearson Mean: 0.5921, Spearman Mean: 0.5587
2025-11-14 20:19:43,556 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3714, 'spearman_mean_genewise': 0.3379, 'l1_error_mean': 2.0285, 'l2_errors_mean': 7.9338, 'r2_scores_mean': 0.143, 'pearson_std': 0.1057, 'l2_error_q1': 5.206, 'l2_error_q2': 7.5161, 'l2_error_q3': 10.481, 'r2_score_q1': 0.0834, 'r2_score_q2': 0.1287, 'r2_score_q3': 0.1786, 'mape_mean': 62.4579, 'mape_std': 17.081, 'rmse_mean': 2.7643, 'rmse_std': 0.5407}
2025-11-14 20:19:43,927 - INFO - Fold 4 Val Epoch 5/200, Batch 0, Loss: 9.8551, Pearson: 0.5410, Spearman: 0.5516
2025-11-14 20:19:45,640 - INFO - Fold 4 Val Epoch 5/200, Batch 10, Loss: 8.6575, Pearson: 0.5699, Spearman: 0.5782
2025-11-14 20:19:47,367 - INFO - Fold 4 Val Epoch 5/200, Batch 20, Loss: 6.7836, Pearson: 0.4888, Spearman: 0.5013
2025-11-14 20:19:50,274 - INFO - Fold 4 Val Epoch 5/200, Val Loss: 8.4319, Pearson Mean: 0.5288, Spearman Mean: 0.5354
2025-11-14 20:19:50,274 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2842, 'spearman_mean_genewise': 0.2625, 'l1_error_mean': 2.1608, 'l2_errors_mean': 8.4408, 'r2_scores_mean': 0.0671, 'pearson_std': 0.1085, 'l2_error_q1': 5.1198, 'l2_error_q2': 7.8614, 'l2_error_q3': 11.7188, 'r2_score_q1': 0.0241, 'r2_score_q2': 0.0575, 'r2_score_q3': 0.1032, 'mape_mean': 65.0593, 'mape_std': 18.0439, 'rmse_mean': 2.8325, 'rmse_std': 0.6464}
2025-11-14 20:19:50,275 - INFO - Learning rate for epoch 5: 0.0001
2025-11-14 20:19:50,340 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:19:51,353 - INFO - Fold 4 Train Epoch 6/200, Batch 0, Loss: 7.7547, Pearson: 0.6035, Spearman: 0.5611
2025-11-14 20:20:01,557 - INFO - Fold 4 Train Epoch 6/200, Batch 10, Loss: 8.1366, Pearson: 0.5994, Spearman: 0.5728
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6000
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5923
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5917
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5888
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5922
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5892
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5989
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5860
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5912
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5983
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5745
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5875
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5913
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6047
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6069
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5912
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6022
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6060
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5914
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6025
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6010
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5886
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6032
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6084
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6038
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5967
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5971
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5958
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5907
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6009
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6078
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5980
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.5977
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.933783531188965
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 6 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       8.497595 0.       0.
 0.       8.497595]
Sample y_pred values (first sample, first 10 genes):
[0.         0.13427112 0.5589086  0.18142512 1.7658312  0.47139505
 0.48875496 0.5671458  1.7994006  6.1291127 ]
y_true  -> mean=2.1451, std=3.4921, min=0.0000, max=12.6524
y_pred  -> mean=2.1142, std=2.0983, min=0.0000, max=14.0016
Batch 0 Pearson correlation: 0.6035
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5914
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6027
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5948
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5907
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6043
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5910
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5994
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6048
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6016
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6064
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6045
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6028
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6032
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6050
2025-11-14 20:20:11,694 - INFO - Fold 4 Train Epoch 6/200, Batch 20, Loss: 7.8958, Pearson: 0.6066, Spearman: 0.5694
2025-11-14 20:20:21,841 - INFO - Fold 4 Train Epoch 6/200, Batch 30, Loss: 7.8609, Pearson: 0.6003, Spearman: 0.5610
2025-11-14 20:20:32,007 - INFO - Fold 4 Train Epoch 6/200, Batch 40, Loss: 7.7562, Pearson: 0.6079, Spearman: 0.5693
2025-11-14 20:20:40,680 - INFO - Fold 4 Train Epoch 6/200, Batch 50, Loss: 8.2538, Pearson: 0.5924, Spearman: 0.5639
2025-11-14 20:20:48,854 - INFO - Fold 4 Train Epoch 6/200, Batch 60, Loss: 7.6058, Pearson: 0.6003, Spearman: 0.5679
2025-11-14 20:20:58,996 - INFO - Fold 4 Train Epoch 6/200, Batch 70, Loss: 7.6534, Pearson: 0.6004, Spearman: 0.5706
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6009
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5986
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6038
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6019
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6153
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6002
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5962
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5951
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6003
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6019
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5964
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6030
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5993
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5949
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5905
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6079
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6073
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5961
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6067
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6119
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5980
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5924
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6042
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5961
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6004
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6051
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6033
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6003
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5904
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6013
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5947
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6027
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5952
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6012
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6004
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5988
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6045
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6060
2025-11-14 20:21:09,151 - INFO - Fold 4 Train Epoch 6/200, Batch 80, Loss: 7.6726, Pearson: 0.6037, Spearman: 0.5605
2025-11-14 20:21:12,447 - INFO - Fold 4 Train Epoch 6/200, Train Loss: 7.7674, Pearson Mean: 0.6029, Spearman Mean: 0.5667
2025-11-14 20:21:12,447 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3859, 'spearman_mean_genewise': 0.3497, 'l1_error_mean': 2.0001, 'l2_errors_mean': 7.767, 'r2_scores_mean': 0.1575, 'pearson_std': 0.1087, 'l2_error_q1': 5.1955, 'l2_error_q2': 7.3549, 'l2_error_q3': 10.1884, 'r2_score_q1': 0.0905, 'r2_score_q2': 0.1399, 'r2_score_q3': 0.1959, 'mape_mean': 61.1809, 'mape_std': 17.4495, 'rmse_mean': 2.7372, 'rmse_std': 0.5242}
2025-11-14 20:21:12,818 - INFO - Fold 4 Val Epoch 6/200, Batch 0, Loss: 9.7513, Pearson: 0.5468, Spearman: 0.5564
2025-11-14 20:21:14,467 - INFO - Fold 4 Val Epoch 6/200, Batch 10, Loss: 8.7709, Pearson: 0.5685, Spearman: 0.5759
2025-11-14 20:21:16,165 - INFO - Fold 4 Val Epoch 6/200, Batch 20, Loss: 6.4161, Pearson: 0.5272, Spearman: 0.5313
2025-11-14 20:21:18,988 - INFO - Fold 4 Val Epoch 6/200, Val Loss: 8.3117, Pearson Mean: 0.5394, Spearman Mean: 0.5444
2025-11-14 20:21:18,989 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2851, 'spearman_mean_genewise': 0.263, 'l1_error_mean': 2.074, 'l2_errors_mean': 8.326, 'r2_scores_mean': 0.0808, 'pearson_std': 0.1123, 'l2_error_q1': 5.072, 'l2_error_q2': 7.7578, 'l2_error_q3': 11.4523, 'r2_score_q1': 0.0337, 'r2_score_q2': 0.0669, 'r2_score_q3': 0.1145, 'mape_mean': 68.0757, 'mape_std': 18.1298, 'rmse_mean': 2.8125, 'rmse_std': 0.6447}
2025-11-14 20:21:18,989 - INFO - Learning rate for epoch 6: 0.0001
2025-11-14 20:21:19,061 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:21:19,920 - INFO - Fold 4 Train Epoch 7/200, Batch 0, Loss: 7.5811, Pearson: 0.6089, Spearman: 0.5742
2025-11-14 20:21:29,846 - INFO - Fold 4 Train Epoch 7/200, Batch 10, Loss: 7.5253, Pearson: 0.6087, Spearman: 0.5676
2025-11-14 20:21:39,994 - INFO - Fold 4 Train Epoch 7/200, Batch 20, Loss: 7.7399, Pearson: 0.6131, Spearman: 0.5734
2025-11-14 20:21:50,168 - INFO - Fold 4 Train Epoch 7/200, Batch 30, Loss: 8.0337, Pearson: 0.5994, Spearman: 0.5692
2025-11-14 20:22:00,319 - INFO - Fold 4 Train Epoch 7/200, Batch 40, Loss: 7.6995, Pearson: 0.6122, Spearman: 0.5806
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6065
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6046
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6029
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6081
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6015
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.766964435577393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 7 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 7.638048 8.73634 ]
Sample y_pred values (first sample, first 10 genes):
[0.29582024 1.3915527  1.8150406  1.1764046  3.4966393  1.9889048
 0.76072407 2.1676412  2.9442792  6.893055  ]
y_true  -> mean=2.0822, std=3.4677, min=0.0000, max=12.5841
y_pred  -> mean=2.1516, std=2.2085, min=0.0000, max=14.1182
Batch 0 Pearson correlation: 0.6089
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6080
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6040
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6022
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6074
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6045
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6204
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5985
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6020
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5982
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5994
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5975
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6079
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5973
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6165
2025-11-14 20:22:10,040 - INFO - Fold 4 Train Epoch 7/200, Batch 50, Loss: 7.6682, Pearson: 0.6087, Spearman: 0.5745
2025-11-14 20:22:17,042 - INFO - Fold 4 Train Epoch 7/200, Batch 60, Loss: 7.7279, Pearson: 0.6025, Spearman: 0.5724
2025-11-14 20:22:27,209 - INFO - Fold 4 Train Epoch 7/200, Batch 70, Loss: 7.4739, Pearson: 0.6074, Spearman: 0.5720
2025-11-14 20:22:37,374 - INFO - Fold 4 Train Epoch 7/200, Batch 80, Loss: 7.3922, Pearson: 0.6265, Spearman: 0.5815
2025-11-14 20:22:40,670 - INFO - Fold 4 Train Epoch 7/200, Train Loss: 7.6561, Pearson Mean: 0.6103, Spearman Mean: 0.5721
2025-11-14 20:22:40,670 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3966, 'spearman_mean_genewise': 0.3591, 'l1_error_mean': 1.9783, 'l2_errors_mean': 7.6559, 'r2_scores_mean': 0.1673, 'pearson_std': 0.1109, 'l2_error_q1': 5.1337, 'l2_error_q2': 7.2874, 'l2_error_q3': 10.0116, 'r2_score_q1': 0.0977, 'r2_score_q2': 0.1488, 'r2_score_q3': 0.2066, 'mape_mean': 60.6466, 'mape_std': 17.5053, 'rmse_mean': 2.7187, 'rmse_std': 0.5143}
2025-11-14 20:22:41,030 - INFO - Fold 4 Val Epoch 7/200, Batch 0, Loss: 9.7259, Pearson: 0.5486, Spearman: 0.5564
2025-11-14 20:22:42,747 - INFO - Fold 4 Val Epoch 7/200, Batch 10, Loss: 8.7441, Pearson: 0.5633, Spearman: 0.5744
2025-11-14 20:22:44,398 - INFO - Fold 4 Val Epoch 7/200, Batch 20, Loss: 6.2675, Pearson: 0.5435, Spearman: 0.5446
2025-11-14 20:22:47,318 - INFO - Fold 4 Val Epoch 7/200, Val Loss: 8.2387, Pearson Mean: 0.5457, Spearman Mean: 0.5509
2025-11-14 20:22:47,318 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2938, 'spearman_mean_genewise': 0.2688, 'l1_error_mean': 2.0767, 'l2_errors_mean': 8.2525, 'r2_scores_mean': 0.0853, 'pearson_std': 0.1142, 'l2_error_q1': 5.0825, 'l2_error_q2': 7.6697, 'l2_error_q3': 11.3797, 'r2_score_q1': 0.0317, 'r2_score_q2': 0.0672, 'r2_score_q3': 0.1168, 'mape_mean': 65.6085, 'mape_std': 18.2278, 'rmse_mean': 2.8024, 'rmse_std': 0.6319}
2025-11-14 20:22:47,318 - INFO - Learning rate for epoch 7: 0.0001
2025-11-14 20:22:47,388 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:22:48,429 - INFO - Fold 4 Train Epoch 8/200, Batch 0, Loss: 7.6551, Pearson: 0.6015, Spearman: 0.5668
2025-11-14 20:22:58,611 - INFO - Fold 4 Train Epoch 8/200, Batch 10, Loss: 7.3557, Pearson: 0.6269, Spearman: 0.5782
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6107
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6074
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6021
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6025
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6072
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6202
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5961
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6001
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6074
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6030
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6076
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6091
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6056
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.655941009521484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 8 =====================
Sample y_true values (first sample, first 10 genes):
[0.        7.1082454 7.1082454 7.1082454 7.1082454 0.        0.
 0.        8.206312  8.8993225]
Sample y_pred values (first sample, first 10 genes):
[1.2624801 3.4751318 4.5867243 1.4745543 6.1969576 5.7783747 0.9610802
 3.2976081 4.990878  8.397707 ]
y_true  -> mean=2.0108, std=3.4606, min=0.0000, max=12.4494
y_pred  -> mean=2.1051, std=2.1336, min=0.0000, max=15.0010
Batch 0 Pearson correlation: 0.6015
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6017
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6118
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5990
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6079
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6077
2025-11-14 20:23:08,752 - INFO - Fold 4 Train Epoch 8/200, Batch 20, Loss: 7.3836, Pearson: 0.6328, Spearman: 0.5809
2025-11-14 20:23:18,914 - INFO - Fold 4 Train Epoch 8/200, Batch 30, Loss: 7.4375, Pearson: 0.6258, Spearman: 0.5827
2025-11-14 20:23:29,057 - INFO - Fold 4 Train Epoch 8/200, Batch 40, Loss: 7.4698, Pearson: 0.6088, Spearman: 0.5742
2025-11-14 20:23:39,210 - INFO - Fold 4 Train Epoch 8/200, Batch 50, Loss: 7.3168, Pearson: 0.6247, Spearman: 0.5764
2025-11-14 20:23:46,910 - INFO - Fold 4 Train Epoch 8/200, Batch 60, Loss: 7.8292, Pearson: 0.6130, Spearman: 0.5752
2025-11-14 20:23:56,152 - INFO - Fold 4 Train Epoch 8/200, Batch 70, Loss: 7.8966, Pearson: 0.6071, Spearman: 0.5696
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6086
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6078
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6097
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6094
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6148
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6079
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6107
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6021
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6130
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6017
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6153
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6137
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6195
2025-11-14 20:24:06,312 - INFO - Fold 4 Train Epoch 8/200, Batch 80, Loss: 7.4586, Pearson: 0.6303, Spearman: 0.5919
2025-11-14 20:24:09,548 - INFO - Fold 4 Train Epoch 8/200, Train Loss: 7.5672, Pearson Mean: 0.6158, Spearman Mean: 0.5767
2025-11-14 20:24:09,548 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4043, 'spearman_mean_genewise': 0.3662, 'l1_error_mean': 1.9597, 'l2_errors_mean': 7.5665, 'r2_scores_mean': 0.1748, 'pearson_std': 0.1126, 'l2_error_q1': 5.088, 'l2_error_q2': 7.1979, 'l2_error_q3': 9.8527, 'r2_score_q1': 0.1026, 'r2_score_q2': 0.1531, 'r2_score_q3': 0.2143, 'mape_mean': 59.9856, 'mape_std': 17.8676, 'rmse_mean': 2.7038, 'rmse_std': 0.5061}
2025-11-14 20:24:09,939 - INFO - Fold 4 Val Epoch 8/200, Batch 0, Loss: 9.7994, Pearson: 0.5480, Spearman: 0.5557
2025-11-14 20:24:11,614 - INFO - Fold 4 Val Epoch 8/200, Batch 10, Loss: 8.6473, Pearson: 0.5706, Spearman: 0.5787
2025-11-14 20:24:13,336 - INFO - Fold 4 Val Epoch 8/200, Batch 20, Loss: 6.3388, Pearson: 0.5375, Spearman: 0.5395
2025-11-14 20:24:16,197 - INFO - Fold 4 Val Epoch 8/200, Val Loss: 8.1893, Pearson Mean: 0.5504, Spearman Mean: 0.5534
2025-11-14 20:24:16,197 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.297, 'spearman_mean_genewise': 0.272, 'l1_error_mean': 2.0385, 'l2_errors_mean': 8.2047, 'r2_scores_mean': 0.0913, 'pearson_std': 0.1161, 'l2_error_q1': 5.0431, 'l2_error_q2': 7.6446, 'l2_error_q3': 11.2643, 'r2_score_q1': 0.0385, 'r2_score_q2': 0.0724, 'r2_score_q3': 0.1225, 'mape_mean': 67.2733, 'mape_std': 18.5481, 'rmse_mean': 2.7936, 'rmse_std': 0.6329}
2025-11-14 20:24:16,197 - INFO - Learning rate for epoch 8: 0.0001
2025-11-14 20:24:16,262 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:24:17,302 - INFO - Fold 4 Train Epoch 9/200, Batch 0, Loss: 7.3639, Pearson: 0.6241, Spearman: 0.5806
2025-11-14 20:24:27,472 - INFO - Fold 4 Train Epoch 9/200, Batch 10, Loss: 7.3665, Pearson: 0.6230, Spearman: 0.5783
2025-11-14 20:24:37,630 - INFO - Fold 4 Train Epoch 9/200, Batch 20, Loss: 7.6313, Pearson: 0.6105, Spearman: 0.5701
2025-11-14 20:24:47,776 - INFO - Fold 4 Train Epoch 9/200, Batch 30, Loss: 7.5986, Pearson: 0.6115, Spearman: 0.5714
2025-11-14 20:24:57,935 - INFO - Fold 4 Train Epoch 9/200, Batch 40, Loss: 7.3412, Pearson: 0.6354, Spearman: 0.5855
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6254
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6138
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.5665082931518555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 9 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        7.0106907 7.0106907 7.0106907 0.
 0.        0.        9.574807 ]
Sample y_pred values (first sample, first 10 genes):
[0.599738  1.4675922 1.245694  1.4521987 4.6384506 2.8705754 0.7547387
 1.8005297 4.867588  7.9871063]
y_true  -> mean=2.0686, std=3.4730, min=0.0000, max=12.5841
y_pred  -> mean=2.0743, std=2.1906, min=0.0000, max=14.1004
Batch 0 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6069
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6111
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6025
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6038
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6130
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6165
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6118
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6118
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6208
2025-11-14 20:25:08,097 - INFO - Fold 4 Train Epoch 9/200, Batch 50, Loss: 7.6046, Pearson: 0.6240, Spearman: 0.5787
2025-11-14 20:25:16,807 - INFO - Fold 4 Train Epoch 9/200, Batch 60, Loss: 7.5126, Pearson: 0.6312, Spearman: 0.5874
2025-11-14 20:25:25,006 - INFO - Fold 4 Train Epoch 9/200, Batch 70, Loss: 7.4453, Pearson: 0.6188, Spearman: 0.5822
2025-11-14 20:25:35,179 - INFO - Fold 4 Train Epoch 9/200, Batch 80, Loss: 7.4473, Pearson: 0.6234, Spearman: 0.5813
2025-11-14 20:25:38,485 - INFO - Fold 4 Train Epoch 9/200, Train Loss: 7.5117, Pearson Mean: 0.6194, Spearman Mean: 0.5804
2025-11-14 20:25:38,486 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4094, 'spearman_mean_genewise': 0.3708, 'l1_error_mean': 1.9465, 'l2_errors_mean': 7.512, 'r2_scores_mean': 0.1795, 'pearson_std': 0.1142, 'l2_error_q1': 5.0675, 'l2_error_q2': 7.151, 'l2_error_q3': 9.7208, 'r2_score_q1': 0.106, 'r2_score_q2': 0.1585, 'r2_score_q3': 0.2201, 'mape_mean': 59.6242, 'mape_std': 17.9196, 'rmse_mean': 2.6943, 'rmse_std': 0.5026}
2025-11-14 20:25:38,863 - INFO - Fold 4 Val Epoch 9/200, Batch 0, Loss: 9.7133, Pearson: 0.5461, Spearman: 0.5558
2025-11-14 20:25:40,516 - INFO - Fold 4 Val Epoch 9/200, Batch 10, Loss: 8.8422, Pearson: 0.5653, Spearman: 0.5735
2025-11-14 20:25:42,255 - INFO - Fold 4 Val Epoch 9/200, Batch 20, Loss: 6.1081, Pearson: 0.5585, Spearman: 0.5595
2025-11-14 20:25:45,129 - INFO - Fold 4 Val Epoch 9/200, Val Loss: 8.1655, Pearson Mean: 0.5538, Spearman Mean: 0.5571
2025-11-14 20:25:45,130 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.301, 'spearman_mean_genewise': 0.2738, 'l1_error_mean': 1.9972, 'l2_errors_mean': 8.1886, 'r2_scores_mean': 0.0924, 'pearson_std': 0.1196, 'l2_error_q1': 5.0321, 'l2_error_q2': 7.7079, 'l2_error_q3': 11.3249, 'r2_score_q1': 0.0361, 'r2_score_q2': 0.0713, 'r2_score_q3': 0.1227, 'mape_mean': 68.0079, 'mape_std': 18.1786, 'rmse_mean': 2.791, 'rmse_std': 0.6315}
2025-11-14 20:25:45,130 - INFO - Learning rate for epoch 9: 0.0001
2025-11-14 20:25:45,199 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:25:46,132 - INFO - Fold 4 Train Epoch 10/200, Batch 0, Loss: 7.4286, Pearson: 0.6208, Spearman: 0.5875
2025-11-14 20:25:56,068 - INFO - Fold 4 Train Epoch 10/200, Batch 10, Loss: 7.5011, Pearson: 0.6322, Spearman: 0.5972
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6049
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6113
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6025
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6164
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6390
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.512031555175781
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 10 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 8.568676 0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.3238487  0.         0.79517996 1.058851   1.3279417  1.1842699
 0.41502267 2.0393329  1.7093742  4.656607  ]
y_true  -> mean=2.0667, std=3.4762, min=0.0000, max=12.4503
y_pred  -> mean=2.0621, std=2.2048, min=0.0000, max=14.1318
Batch 0 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6137
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6260
2025-11-14 20:26:06,213 - INFO - Fold 4 Train Epoch 10/200, Batch 20, Loss: 7.2438, Pearson: 0.6320, Spearman: 0.5822
2025-11-14 20:26:16,365 - INFO - Fold 4 Train Epoch 10/200, Batch 30, Loss: 7.1950, Pearson: 0.6257, Spearman: 0.5796
2025-11-14 20:26:26,535 - INFO - Fold 4 Train Epoch 10/200, Batch 40, Loss: 7.4175, Pearson: 0.6325, Spearman: 0.5913
2025-11-14 20:26:36,670 - INFO - Fold 4 Train Epoch 10/200, Batch 50, Loss: 7.7907, Pearson: 0.6184, Spearman: 0.5819
2025-11-14 20:26:46,089 - INFO - Fold 4 Train Epoch 10/200, Batch 60, Loss: 7.6303, Pearson: 0.6134, Spearman: 0.5822
2025-11-14 20:26:53,575 - INFO - Fold 4 Train Epoch 10/200, Batch 70, Loss: 7.4798, Pearson: 0.6237, Spearman: 0.5881
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6111
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6165
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6137
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6051
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6075
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6156
2025-11-14 20:27:03,654 - INFO - Fold 4 Train Epoch 10/200, Batch 80, Loss: 7.4382, Pearson: 0.6171, Spearman: 0.5812
2025-11-14 20:27:06,935 - INFO - Fold 4 Train Epoch 10/200, Train Loss: 7.4541, Pearson Mean: 0.6231, Spearman Mean: 0.5839
2025-11-14 20:27:06,935 - INFO - Training Metrics: {'pearson_mean_genewise': 0.415, 'spearman_mean_genewise': 0.3759, 'l1_error_mean': 1.9314, 'l2_errors_mean': 7.4538, 'r2_scores_mean': 0.1846, 'pearson_std': 0.1155, 'l2_error_q1': 5.0616, 'l2_error_q2': 7.0714, 'l2_error_q3': 9.586, 'r2_score_q1': 0.1094, 'r2_score_q2': 0.1618, 'r2_score_q3': 0.2265, 'mape_mean': 59.4843, 'mape_std': 17.9182, 'rmse_mean': 2.6843, 'rmse_std': 0.4984}
2025-11-14 20:27:07,304 - INFO - Fold 4 Val Epoch 10/200, Batch 0, Loss: 9.7312, Pearson: 0.5450, Spearman: 0.5541
2025-11-14 20:27:08,984 - INFO - Fold 4 Val Epoch 10/200, Batch 10, Loss: 8.6766, Pearson: 0.5705, Spearman: 0.5772
2025-11-14 20:27:10,627 - INFO - Fold 4 Val Epoch 10/200, Batch 20, Loss: 6.2023, Pearson: 0.5523, Spearman: 0.5542
2025-11-14 20:27:13,452 - INFO - Fold 4 Val Epoch 10/200, Val Loss: 8.1456, Pearson Mean: 0.5531, Spearman Mean: 0.5561
2025-11-14 20:27:13,453 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2997, 'spearman_mean_genewise': 0.2755, 'l1_error_mean': 2.0463, 'l2_errors_mean': 8.1652, 'r2_scores_mean': 0.0935, 'pearson_std': 0.1203, 'l2_error_q1': 5.0625, 'l2_error_q2': 7.6512, 'l2_error_q3': 11.2461, 'r2_score_q1': 0.0366, 'r2_score_q2': 0.0728, 'r2_score_q3': 0.1257, 'mape_mean': 66.1216, 'mape_std': 18.7053, 'rmse_mean': 2.788, 'rmse_std': 0.6263}
2025-11-14 20:27:13,453 - INFO - Learning rate for epoch 10: 0.0001
2025-11-14 20:27:13,531 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:27:14,375 - INFO - Fold 4 Train Epoch 11/200, Batch 0, Loss: 7.4597, Pearson: 0.6239, Spearman: 0.5869
2025-11-14 20:27:24,326 - INFO - Fold 4 Train Epoch 11/200, Batch 10, Loss: 7.3487, Pearson: 0.6297, Spearman: 0.5906
2025-11-14 20:27:34,466 - INFO - Fold 4 Train Epoch 11/200, Batch 20, Loss: 7.2925, Pearson: 0.6346, Spearman: 0.5852
2025-11-14 20:27:44,632 - INFO - Fold 4 Train Epoch 11/200, Batch 30, Loss: 7.7095, Pearson: 0.6235, Spearman: 0.5869
2025-11-14 20:27:54,785 - INFO - Fold 4 Train Epoch 11/200, Batch 40, Loss: 7.6039, Pearson: 0.6067, Spearman: 0.5760
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6339
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6124
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.453813552856445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 11 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        6.809919  0.        0.
 0.        0.        7.9077954]
Sample y_pred values (first sample, first 10 genes):
[0.06807545 0.896407   1.3929722  0.6506384  4.04688    1.5750204
 0.84255433 1.2092779  3.8164802  8.237289  ]
y_true  -> mean=2.1157, std=3.4941, min=0.0000, max=12.9888
y_pred  -> mean=2.0671, std=2.1665, min=0.0000, max=14.2144
Batch 0 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6067
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6210
2025-11-14 20:28:04,965 - INFO - Fold 4 Train Epoch 11/200, Batch 50, Loss: 7.4395, Pearson: 0.6307, Spearman: 0.5879
2025-11-14 20:28:15,109 - INFO - Fold 4 Train Epoch 11/200, Batch 60, Loss: 7.2683, Pearson: 0.6301, Spearman: 0.5884
2025-11-14 20:28:22,818 - INFO - Fold 4 Train Epoch 11/200, Batch 70, Loss: 7.2395, Pearson: 0.6275, Spearman: 0.5882
2025-11-14 20:28:32,120 - INFO - Fold 4 Train Epoch 11/200, Batch 80, Loss: 7.1632, Pearson: 0.6459, Spearman: 0.5972
2025-11-14 20:28:35,419 - INFO - Fold 4 Train Epoch 11/200, Train Loss: 7.4068, Pearson Mean: 0.6260, Spearman Mean: 0.5868
2025-11-14 20:28:35,419 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4198, 'spearman_mean_genewise': 0.38, 'l1_error_mean': 1.9221, 'l2_errors_mean': 7.4061, 'r2_scores_mean': 0.189, 'pearson_std': 0.116, 'l2_error_q1': 5.0316, 'l2_error_q2': 7.0453, 'l2_error_q3': 9.4952, 'r2_score_q1': 0.1125, 'r2_score_q2': 0.165, 'r2_score_q3': 0.2317, 'mape_mean': 59.1033, 'mape_std': 18.072, 'rmse_mean': 2.6759, 'rmse_std': 0.4954}
2025-11-14 20:28:35,804 - INFO - Fold 4 Val Epoch 11/200, Batch 0, Loss: 9.6865, Pearson: 0.5489, Spearman: 0.5567
2025-11-14 20:28:37,466 - INFO - Fold 4 Val Epoch 11/200, Batch 10, Loss: 8.6923, Pearson: 0.5680, Spearman: 0.5739
2025-11-14 20:28:39,194 - INFO - Fold 4 Val Epoch 11/200, Batch 20, Loss: 6.1351, Pearson: 0.5580, Spearman: 0.5576
2025-11-14 20:28:42,078 - INFO - Fold 4 Val Epoch 11/200, Val Loss: 8.1356, Pearson Mean: 0.5566, Spearman Mean: 0.5579
2025-11-14 20:28:42,079 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2996, 'spearman_mean_genewise': 0.2747, 'l1_error_mean': 2.0152, 'l2_errors_mean': 8.1591, 'r2_scores_mean': 0.0948, 'pearson_std': 0.1208, 'l2_error_q1': 5.0282, 'l2_error_q2': 7.6442, 'l2_error_q3': 11.2344, 'r2_score_q1': 0.0385, 'r2_score_q2': 0.0717, 'r2_score_q3': 0.1251, 'mape_mean': 67.346, 'mape_std': 19.0719, 'rmse_mean': 2.7865, 'rmse_std': 0.6283}
2025-11-14 20:28:42,079 - INFO - Learning rate for epoch 11: 0.0001
2025-11-14 20:28:42,079 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 20:28:42,924 - INFO - Fold 4 Train Epoch 12/200, Batch 0, Loss: 7.2920, Pearson: 0.6275, Spearman: 0.5956
2025-11-14 20:28:52,875 - INFO - Fold 4 Train Epoch 12/200, Batch 10, Loss: 7.3205, Pearson: 0.6226, Spearman: 0.5902
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6129
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6204
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6198
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6267
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.406092166900635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 12 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        6.9340477 0.        7.6267076 8.542706  0.
 0.        0.        0.       ]
Sample y_pred values (first sample, first 10 genes):
[2.4233751 3.1159742 3.885017  1.8169948 4.4374533 6.563153  1.0542728
 2.5895622 2.8505301 4.7352076]
y_true  -> mean=2.0476, std=3.4679, min=0.0000, max=12.6482
y_pred  -> mean=2.0861, std=2.1765, min=0.0000, max=13.9631
Batch 0 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6261
2025-11-14 20:29:03,016 - INFO - Fold 4 Train Epoch 12/200, Batch 20, Loss: 7.3088, Pearson: 0.6282, Spearman: 0.5813
2025-11-14 20:29:13,181 - INFO - Fold 4 Train Epoch 12/200, Batch 30, Loss: 7.5261, Pearson: 0.6209, Spearman: 0.5827
2025-11-14 20:29:23,330 - INFO - Fold 4 Train Epoch 12/200, Batch 40, Loss: 7.7794, Pearson: 0.6314, Spearman: 0.5936
2025-11-14 20:29:33,496 - INFO - Fold 4 Train Epoch 12/200, Batch 50, Loss: 7.3970, Pearson: 0.6220, Spearman: 0.5913
2025-11-14 20:29:43,666 - INFO - Fold 4 Train Epoch 12/200, Batch 60, Loss: 7.5206, Pearson: 0.6308, Spearman: 0.5906
2025-11-14 20:29:52,603 - INFO - Fold 4 Train Epoch 12/200, Batch 70, Loss: 7.3078, Pearson: 0.6353, Spearman: 0.5976
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6174
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6236
2025-11-14 20:30:00,550 - INFO - Fold 4 Train Epoch 12/200, Batch 80, Loss: 7.4503, Pearson: 0.6288, Spearman: 0.5920
2025-11-14 20:30:03,812 - INFO - Fold 4 Train Epoch 12/200, Train Loss: 7.3501, Pearson Mean: 0.6296, Spearman Mean: 0.5902
2025-11-14 20:30:03,812 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4253, 'spearman_mean_genewise': 0.3849, 'l1_error_mean': 1.9127, 'l2_errors_mean': 7.3509, 'r2_scores_mean': 0.194, 'pearson_std': 0.1165, 'l2_error_q1': 5.0033, 'l2_error_q2': 7.0119, 'l2_error_q3': 9.4059, 'r2_score_q1': 0.1162, 'r2_score_q2': 0.169, 'r2_score_q3': 0.2385, 'mape_mean': 58.7009, 'mape_std': 18.1321, 'rmse_mean': 2.6663, 'rmse_std': 0.4917}
2025-11-14 20:30:04,192 - INFO - Fold 4 Val Epoch 12/200, Batch 0, Loss: 9.6879, Pearson: 0.5481, Spearman: 0.5559
2025-11-14 20:30:05,904 - INFO - Fold 4 Val Epoch 12/200, Batch 10, Loss: 8.6470, Pearson: 0.5723, Spearman: 0.5784
2025-11-14 20:30:07,607 - INFO - Fold 4 Val Epoch 12/200, Batch 20, Loss: 6.1216, Pearson: 0.5580, Spearman: 0.5571
2025-11-14 20:30:10,475 - INFO - Fold 4 Val Epoch 12/200, Val Loss: 8.1099, Pearson Mean: 0.5583, Spearman Mean: 0.5591
2025-11-14 20:30:10,476 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3034, 'spearman_mean_genewise': 0.2772, 'l1_error_mean': 1.9971, 'l2_errors_mean': 8.1322, 'r2_scores_mean': 0.0972, 'pearson_std': 0.1215, 'l2_error_q1': 5.0107, 'l2_error_q2': 7.6331, 'l2_error_q3': 11.1822, 'r2_score_q1': 0.0391, 'r2_score_q2': 0.0746, 'r2_score_q3': 0.128, 'mape_mean': 67.2563, 'mape_std': 19.2109, 'rmse_mean': 2.782, 'rmse_std': 0.6266}
2025-11-14 20:30:10,476 - INFO - Learning rate for epoch 12: 0.0001
2025-11-14 20:30:10,541 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:30:11,560 - INFO - Fold 4 Train Epoch 13/200, Batch 0, Loss: 7.3384, Pearson: 0.6099, Spearman: 0.5749
2025-11-14 20:30:21,752 - INFO - Fold 4 Train Epoch 13/200, Batch 10, Loss: 7.5112, Pearson: 0.6304, Spearman: 0.5957
2025-11-14 20:30:31,928 - INFO - Fold 4 Train Epoch 13/200, Batch 20, Loss: 7.3233, Pearson: 0.6304, Spearman: 0.5915
2025-11-14 20:30:42,086 - INFO - Fold 4 Train Epoch 13/200, Batch 30, Loss: 7.4551, Pearson: 0.6234, Spearman: 0.5856
2025-11-14 20:30:52,248 - INFO - Fold 4 Train Epoch 13/200, Batch 40, Loss: 7.4119, Pearson: 0.6199, Spearman: 0.5824
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6422
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6413
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.3508477210998535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 13 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 6.60826   7.7059727 8.909631 ]
Sample y_pred values (first sample, first 10 genes):
[1.9770753 3.888563  3.620861  2.0311356 5.1944933 4.7262926 1.0637555
 3.8318486 3.2264938 9.319278 ]
y_true  -> mean=1.8781, std=3.4085, min=0.0000, max=12.6263
y_pred  -> mean=2.0454, std=2.1973, min=0.0000, max=14.0749
Batch 0 Pearson correlation: 0.6099
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6119
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6298
2025-11-14 20:31:02,586 - INFO - Fold 4 Train Epoch 13/200, Batch 50, Loss: 7.2047, Pearson: 0.6458, Spearman: 0.6029
2025-11-14 20:31:12,772 - INFO - Fold 4 Train Epoch 13/200, Batch 60, Loss: 7.2556, Pearson: 0.6365, Spearman: 0.5934
2025-11-14 20:31:22,534 - INFO - Fold 4 Train Epoch 13/200, Batch 70, Loss: 7.4056, Pearson: 0.6133, Spearman: 0.5800
2025-11-14 20:31:29,597 - INFO - Fold 4 Train Epoch 13/200, Batch 80, Loss: 7.3543, Pearson: 0.6272, Spearman: 0.5905
2025-11-14 20:31:32,865 - INFO - Fold 4 Train Epoch 13/200, Train Loss: 7.3167, Pearson Mean: 0.6319, Spearman Mean: 0.5926
2025-11-14 20:31:32,865 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4291, 'spearman_mean_genewise': 0.3882, 'l1_error_mean': 1.9021, 'l2_errors_mean': 7.3167, 'r2_scores_mean': 0.1972, 'pearson_std': 0.1166, 'l2_error_q1': 5.0016, 'l2_error_q2': 6.9659, 'l2_error_q3': 9.3441, 'r2_score_q1': 0.1191, 'r2_score_q2': 0.1717, 'r2_score_q3': 0.2414, 'mape_mean': 58.5785, 'mape_std': 18.1012, 'rmse_mean': 2.6602, 'rmse_std': 0.4897}
2025-11-14 20:31:33,233 - INFO - Fold 4 Val Epoch 13/200, Batch 0, Loss: 9.7582, Pearson: 0.5475, Spearman: 0.5549
2025-11-14 20:31:34,867 - INFO - Fold 4 Val Epoch 13/200, Batch 10, Loss: 8.6386, Pearson: 0.5722, Spearman: 0.5776
2025-11-14 20:31:36,506 - INFO - Fold 4 Val Epoch 13/200, Batch 20, Loss: 6.0853, Pearson: 0.5644, Spearman: 0.5627
2025-11-14 20:31:39,299 - INFO - Fold 4 Val Epoch 13/200, Val Loss: 8.0925, Pearson Mean: 0.5601, Spearman Mean: 0.5608
2025-11-14 20:31:39,299 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2995, 'spearman_mean_genewise': 0.2752, 'l1_error_mean': 2.0233, 'l2_errors_mean': 8.1143, 'r2_scores_mean': 0.0983, 'pearson_std': 0.1247, 'l2_error_q1': 4.984, 'l2_error_q2': 7.6266, 'l2_error_q3': 11.1848, 'r2_score_q1': 0.0375, 'r2_score_q2': 0.0734, 'r2_score_q3': 0.1272, 'mape_mean': 67.0752, 'mape_std': 19.6901, 'rmse_mean': 2.7794, 'rmse_std': 0.6241}
2025-11-14 20:31:39,299 - INFO - Learning rate for epoch 13: 0.0001
2025-11-14 20:31:39,300 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 20:31:40,178 - INFO - Fold 4 Train Epoch 14/200, Batch 0, Loss: 7.1709, Pearson: 0.6375, Spearman: 0.5926
2025-11-14 20:31:50,364 - INFO - Fold 4 Train Epoch 14/200, Batch 10, Loss: 7.2059, Pearson: 0.6422, Spearman: 0.5967
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6355
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6276
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.316676616668701
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 14 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.2641788 0.        6.166966  6.166966  0.
 6.166966  6.8590636 8.562952 ]
Sample y_pred values (first sample, first 10 genes):
[0.7797241  3.6812737  3.3924074  1.2667229  5.166663   4.1995068
 0.91520023 2.6532352  3.2928946  8.956331  ]
y_true  -> mean=2.1156, std=3.4751, min=0.0000, max=12.4522
y_pred  -> mean=2.0711, std=2.2253, min=0.0000, max=14.5590
Batch 0 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6396
2025-11-14 20:32:00,512 - INFO - Fold 4 Train Epoch 14/200, Batch 20, Loss: 7.2281, Pearson: 0.6185, Spearman: 0.5799
2025-11-14 20:32:10,681 - INFO - Fold 4 Train Epoch 14/200, Batch 30, Loss: 7.1606, Pearson: 0.6314, Spearman: 0.5941
2025-11-14 20:32:20,858 - INFO - Fold 4 Train Epoch 14/200, Batch 40, Loss: 7.2171, Pearson: 0.6265, Spearman: 0.5940
2025-11-14 20:32:31,013 - INFO - Fold 4 Train Epoch 14/200, Batch 50, Loss: 7.1591, Pearson: 0.6365, Spearman: 0.5965
2025-11-14 20:32:41,194 - INFO - Fold 4 Train Epoch 14/200, Batch 60, Loss: 7.1741, Pearson: 0.6354, Spearman: 0.5941
2025-11-14 20:32:51,355 - INFO - Fold 4 Train Epoch 14/200, Batch 70, Loss: 7.2729, Pearson: 0.6280, Spearman: 0.5914
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6357
2025-11-14 20:32:59,445 - INFO - Fold 4 Train Epoch 14/200, Batch 80, Loss: 7.2026, Pearson: 0.6356, Spearman: 0.5993
2025-11-14 20:33:02,001 - INFO - Fold 4 Train Epoch 14/200, Train Loss: 7.2880, Pearson Mean: 0.6338, Spearman Mean: 0.5947
2025-11-14 20:33:02,001 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4324, 'spearman_mean_genewise': 0.3911, 'l1_error_mean': 1.8956, 'l2_errors_mean': 7.2877, 'r2_scores_mean': 0.2001, 'pearson_std': 0.1166, 'l2_error_q1': 4.9868, 'l2_error_q2': 6.9272, 'l2_error_q3': 9.3311, 'r2_score_q1': 0.1211, 'r2_score_q2': 0.1749, 'r2_score_q3': 0.2448, 'mape_mean': 58.4997, 'mape_std': 18.0843, 'rmse_mean': 2.6551, 'rmse_std': 0.4882}
2025-11-14 20:33:02,282 - INFO - Fold 4 Val Epoch 14/200, Batch 0, Loss: 9.7797, Pearson: 0.5432, Spearman: 0.5504
2025-11-14 20:33:03,996 - INFO - Fold 4 Val Epoch 14/200, Batch 10, Loss: 8.5863, Pearson: 0.5745, Spearman: 0.5799
2025-11-14 20:33:05,657 - INFO - Fold 4 Val Epoch 14/200, Batch 20, Loss: 6.1175, Pearson: 0.5634, Spearman: 0.5615
2025-11-14 20:33:08,578 - INFO - Fold 4 Val Epoch 14/200, Val Loss: 8.1022, Pearson Mean: 0.5589, Spearman Mean: 0.5593
2025-11-14 20:33:08,578 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3009, 'spearman_mean_genewise': 0.2772, 'l1_error_mean': 2.0423, 'l2_errors_mean': 8.1246, 'r2_scores_mean': 0.0973, 'pearson_std': 0.1237, 'l2_error_q1': 5.0343, 'l2_error_q2': 7.5876, 'l2_error_q3': 11.22, 'r2_score_q1': 0.037, 'r2_score_q2': 0.0727, 'r2_score_q3': 0.1269, 'mape_mean': 66.2067, 'mape_std': 19.4133, 'rmse_mean': 2.781, 'rmse_std': 0.625}
2025-11-14 20:33:08,579 - INFO - Learning rate for epoch 14: 0.0001
2025-11-14 20:33:08,579 - INFO - No improvement in spearman genewise. Patience: 2/30
2025-11-14 20:33:09,439 - INFO - Fold 4 Train Epoch 15/200, Batch 0, Loss: 7.2752, Pearson: 0.6394, Spearman: 0.6001
2025-11-14 20:33:19,399 - INFO - Fold 4 Train Epoch 15/200, Batch 10, Loss: 7.4401, Pearson: 0.6380, Spearman: 0.5999
2025-11-14 20:33:29,579 - INFO - Fold 4 Train Epoch 15/200, Batch 20, Loss: 7.3966, Pearson: 0.6297, Spearman: 0.5950
2025-11-14 20:33:39,757 - INFO - Fold 4 Train Epoch 15/200, Batch 30, Loss: 7.0458, Pearson: 0.6437, Spearman: 0.5973
2025-11-14 20:33:49,915 - INFO - Fold 4 Train Epoch 15/200, Batch 40, Loss: 7.2691, Pearson: 0.6349, Spearman: 0.5967
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6266
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6286
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.287670612335205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 15 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       7.787647 7.787647 0.       0.       7.787647
 0.       8.480587]
Sample y_pred values (first sample, first 10 genes):
[0.670961   1.9658262  3.2720077  0.49135163 4.6887465  2.8800235
 0.45664138 2.1691341  4.5731044  7.585863  ]
y_true  -> mean=2.1607, std=3.5073, min=0.0000, max=12.5698
y_pred  -> mean=2.1065, std=2.2280, min=0.0000, max=14.0760
Batch 0 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6295
2025-11-14 20:34:00,071 - INFO - Fold 4 Train Epoch 15/200, Batch 50, Loss: 7.1746, Pearson: 0.6404, Spearman: 0.5967
2025-11-14 20:34:10,224 - INFO - Fold 4 Train Epoch 15/200, Batch 60, Loss: 7.2782, Pearson: 0.6385, Spearman: 0.5950
2025-11-14 20:34:20,415 - INFO - Fold 4 Train Epoch 15/200, Batch 70, Loss: 7.1812, Pearson: 0.6296, Spearman: 0.5894
2025-11-14 20:34:29,606 - INFO - Fold 4 Train Epoch 15/200, Batch 80, Loss: 7.3284, Pearson: 0.6380, Spearman: 0.6007
2025-11-14 20:34:32,515 - INFO - Fold 4 Train Epoch 15/200, Train Loss: 7.2971, Pearson Mean: 0.6333, Spearman Mean: 0.5948
2025-11-14 20:34:32,515 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4318, 'spearman_mean_genewise': 0.3906, 'l1_error_mean': 1.8952, 'l2_errors_mean': 7.2972, 'r2_scores_mean': 0.1994, 'pearson_std': 0.116, 'l2_error_q1': 4.9784, 'l2_error_q2': 6.9438, 'l2_error_q3': 9.328, 'r2_score_q1': 0.1208, 'r2_score_q2': 0.1747, 'r2_score_q3': 0.2444, 'mape_mean': 58.5313, 'mape_std': 18.0682, 'rmse_mean': 2.6566, 'rmse_std': 0.4898}
2025-11-14 20:34:32,795 - INFO - Fold 4 Val Epoch 15/200, Batch 0, Loss: 9.8200, Pearson: 0.5408, Spearman: 0.5493
2025-11-14 20:34:33,981 - INFO - Fold 4 Val Epoch 15/200, Batch 10, Loss: 8.6362, Pearson: 0.5724, Spearman: 0.5787
2025-11-14 20:34:35,153 - INFO - Fold 4 Val Epoch 15/200, Batch 20, Loss: 6.0605, Pearson: 0.5649, Spearman: 0.5645
2025-11-14 20:34:37,814 - INFO - Fold 4 Val Epoch 15/200, Val Loss: 8.1144, Pearson Mean: 0.5572, Spearman Mean: 0.5589
2025-11-14 20:34:37,815 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3025, 'spearman_mean_genewise': 0.2781, 'l1_error_mean': 2.0082, 'l2_errors_mean': 8.1375, 'r2_scores_mean': 0.095, 'pearson_std': 0.1242, 'l2_error_q1': 5.0457, 'l2_error_q2': 7.6424, 'l2_error_q3': 11.2257, 'r2_score_q1': 0.0338, 'r2_score_q2': 0.0705, 'r2_score_q3': 0.1269, 'mape_mean': 66.3387, 'mape_std': 18.9339, 'rmse_mean': 2.7836, 'rmse_std': 0.6235}
2025-11-14 20:34:37,815 - INFO - Learning rate for epoch 15: 0.0001
2025-11-14 20:34:37,876 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_4/best_model.pth
2025-11-14 20:34:38,605 - INFO - Fold 4 Train Epoch 16/200, Batch 0, Loss: 7.4774, Pearson: 0.6394, Spearman: 0.5984
2025-11-14 20:34:44,742 - INFO - Fold 4 Train Epoch 16/200, Batch 10, Loss: 7.1069, Pearson: 0.6390, Spearman: 0.6048
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6367
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6385
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.2972517013549805
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 16 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        7.6074176 0.        0.
 0.        0.        8.705698 ]
Sample y_pred values (first sample, first 10 genes):
[0.48575935 0.         1.3956943  0.5799293  3.6983252  1.1508832
 0.89983785 1.7254605  3.0914288  7.743989  ]
y_true  -> mean=2.2931, std=3.5473, min=0.0000, max=12.6362
y_pred  -> mean=2.0987, std=2.2394, min=0.0000, max=14.1376
Batch 0 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6396
2025-11-14 20:34:50,871 - INFO - Fold 4 Train Epoch 16/200, Batch 20, Loss: 7.2520, Pearson: 0.6451, Spearman: 0.5999
2025-11-14 20:34:59,931 - INFO - Fold 4 Train Epoch 16/200, Batch 30, Loss: 7.4553, Pearson: 0.6285, Spearman: 0.6013
2025-11-14 20:35:10,013 - INFO - Fold 4 Train Epoch 16/200, Batch 40, Loss: 7.4723, Pearson: 0.6317, Spearman: 0.5953
2025-11-14 20:35:20,129 - INFO - Fold 4 Train Epoch 16/200, Batch 50, Loss: 7.2498, Pearson: 0.6327, Spearman: 0.5945
2025-11-14 20:35:30,248 - INFO - Fold 4 Train Epoch 16/200, Batch 60, Loss: 7.1185, Pearson: 0.6343, Spearman: 0.5918
2025-11-14 20:35:40,363 - INFO - Fold 4 Train Epoch 16/200, Batch 70, Loss: 7.2597, Pearson: 0.6323, Spearman: 0.5955
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6257
2025-11-14 20:35:50,304 - INFO - Fold 4 Train Epoch 16/200, Batch 80, Loss: 7.0289, Pearson: 0.6383, Spearman: 0.5944
2025-11-14 20:35:53,517 - INFO - Fold 4 Train Epoch 16/200, Train Loss: 7.2508, Pearson Mean: 0.6361, Spearman Mean: 0.5977
2025-11-14 20:35:53,517 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4368, 'spearman_mean_genewise': 0.3948, 'l1_error_mean': 1.8883, 'l2_errors_mean': 7.2505, 'r2_scores_mean': 0.2038, 'pearson_std': 0.1159, 'l2_error_q1': 4.9511, 'l2_error_q2': 6.9069, 'l2_error_q3': 9.268, 'r2_score_q1': 0.1253, 'r2_score_q2': 0.1779, 'r2_score_q3': 0.248, 'mape_mean': 58.1763, 'mape_std': 18.081, 'rmse_mean': 2.6484, 'rmse_std': 0.4865}
2025-11-14 20:35:53,804 - INFO - Fold 4 Val Epoch 16/200, Batch 0, Loss: 9.7218, Pearson: 0.5459, Spearman: 0.5546
2025-11-14 20:35:55,543 - INFO - Fold 4 Val Epoch 16/200, Batch 10, Loss: 8.6798, Pearson: 0.5687, Spearman: 0.5734
2025-11-14 20:35:57,311 - INFO - Fold 4 Val Epoch 16/200, Batch 20, Loss: 6.1384, Pearson: 0.5641, Spearman: 0.5609
2025-11-14 20:36:00,115 - INFO - Fold 4 Val Epoch 16/200, Val Loss: 8.1043, Pearson Mean: 0.5587, Spearman Mean: 0.5594
2025-11-14 20:36:00,116 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2991, 'spearman_mean_genewise': 0.2743, 'l1_error_mean': 2.0337, 'l2_errors_mean': 8.127, 'r2_scores_mean': 0.0966, 'pearson_std': 0.1245, 'l2_error_q1': 5.028, 'l2_error_q2': 7.6424, 'l2_error_q3': 11.1906, 'r2_score_q1': 0.0363, 'r2_score_q2': 0.0722, 'r2_score_q3': 0.1269, 'mape_mean': 66.1899, 'mape_std': 19.626, 'rmse_mean': 2.7817, 'rmse_std': 0.624}
2025-11-14 20:36:00,116 - INFO - Learning rate for epoch 16: 0.0001
2025-11-14 20:36:00,116 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 20:36:01,060 - INFO - Fold 4 Train Epoch 17/200, Batch 0, Loss: 7.0575, Pearson: 0.6383, Spearman: 0.6001
2025-11-14 20:36:11,212 - INFO - Fold 4 Train Epoch 17/200, Batch 10, Loss: 7.1879, Pearson: 0.6389, Spearman: 0.6053
2025-11-14 20:36:18,380 - INFO - Fold 4 Train Epoch 17/200, Batch 20, Loss: 7.2124, Pearson: 0.6414, Spearman: 0.6033
2025-11-14 20:36:27,896 - INFO - Fold 4 Train Epoch 17/200, Batch 30, Loss: 7.1594, Pearson: 0.6402, Spearman: 0.5931
2025-11-14 20:36:38,023 - INFO - Fold 4 Train Epoch 17/200, Batch 40, Loss: 7.6042, Pearson: 0.6370, Spearman: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6301
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6315
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.250541687011719
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 17 =====================
Sample y_true values (first sample, first 10 genes):
[0.        6.9558005 7.648471  0.        7.648471  6.9558005 0.
 0.        7.648471  7.648471 ]
Sample y_pred values (first sample, first 10 genes):
[1.1154671 2.100951  5.4067874 1.1041975 6.507366  5.5135612 1.2241367
 3.0100145 5.164585  6.005513 ]
y_true  -> mean=2.0016, std=3.4505, min=0.0000, max=12.6904
y_pred  -> mean=2.0519, std=2.2101, min=0.0000, max=14.8165
Batch 0 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6325
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6328
2025-11-14 20:36:48,175 - INFO - Fold 4 Train Epoch 17/200, Batch 50, Loss: 7.4368, Pearson: 0.6471, Spearman: 0.6073
2025-11-14 20:36:58,354 - INFO - Fold 4 Train Epoch 17/200, Batch 60, Loss: 7.2922, Pearson: 0.6384, Spearman: 0.5985
2025-11-14 20:37:08,529 - INFO - Fold 4 Train Epoch 17/200, Batch 70, Loss: 7.3105, Pearson: 0.6404, Spearman: 0.5950
2025-11-14 20:37:18,659 - INFO - Fold 4 Train Epoch 17/200, Batch 80, Loss: 7.3175, Pearson: 0.6356, Spearman: 0.6033
2025-11-14 20:37:21,911 - INFO - Fold 4 Train Epoch 17/200, Train Loss: 7.2024, Pearson Mean: 0.6393, Spearman Mean: 0.6009
2025-11-14 20:37:21,911 - INFO - Training Metrics: {'pearson_mean_genewise': 0.442, 'spearman_mean_genewise': 0.3992, 'l1_error_mean': 1.8774, 'l2_errors_mean': 7.2023, 'r2_scores_mean': 0.2084, 'pearson_std': 0.1161, 'l2_error_q1': 4.9378, 'l2_error_q2': 6.832, 'l2_error_q3': 9.2132, 'r2_score_q1': 0.1291, 'r2_score_q2': 0.1817, 'r2_score_q3': 0.2549, 'mape_mean': 58.0431, 'mape_std': 18.0373, 'rmse_mean': 2.6397, 'rmse_std': 0.4839}
2025-11-14 20:37:22,284 - INFO - Fold 4 Val Epoch 17/200, Batch 0, Loss: 9.8957, Pearson: 0.5364, Spearman: 0.5442
2025-11-14 20:37:23,964 - INFO - Fold 4 Val Epoch 17/200, Batch 10, Loss: 8.6499, Pearson: 0.5715, Spearman: 0.5759
2025-11-14 20:37:25,698 - INFO - Fold 4 Val Epoch 17/200, Batch 20, Loss: 6.0840, Pearson: 0.5651, Spearman: 0.5623
2025-11-14 20:37:28,567 - INFO - Fold 4 Val Epoch 17/200, Val Loss: 8.1270, Pearson Mean: 0.5571, Spearman Mean: 0.5576
2025-11-14 20:37:28,567 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2987, 'spearman_mean_genewise': 0.2749, 'l1_error_mean': 2.0136, 'l2_errors_mean': 8.1498, 'r2_scores_mean': 0.0948, 'pearson_std': 0.1248, 'l2_error_q1': 5.0272, 'l2_error_q2': 7.6259, 'l2_error_q3': 11.2394, 'r2_score_q1': 0.0346, 'r2_score_q2': 0.0697, 'r2_score_q3': 0.1245, 'mape_mean': 67.16, 'mape_std': 19.1973, 'rmse_mean': 2.7851, 'rmse_std': 0.6269}
2025-11-14 20:37:28,567 - INFO - Learning rate for epoch 17: 0.0001
2025-11-14 20:37:28,567 - INFO - No improvement in spearman genewise. Patience: 2/30
2025-11-14 20:37:29,617 - INFO - Fold 4 Train Epoch 18/200, Batch 0, Loss: 7.2423, Pearson: 0.6318, Spearman: 0.5954
2025-11-14 20:37:39,426 - INFO - Fold 4 Train Epoch 18/200, Batch 10, Loss: 6.9932, Pearson: 0.6501, Spearman: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6420
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6418
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.202309608459473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 18 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       8.422103 0.       0.       0.       0.       8.422103
 0.       8.422103]
Sample y_pred values (first sample, first 10 genes):
[0.18811406 0.81752783 1.6525903  0.         1.7489793  1.2393621
 0.         0.48239523 1.6419739  6.5055933 ]
y_true  -> mean=2.0565, std=3.4720, min=0.0000, max=12.8889
y_pred  -> mean=2.0618, std=2.1972, min=0.0000, max=12.7599
Batch 0 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6393
2025-11-14 20:37:46,437 - INFO - Fold 4 Train Epoch 18/200, Batch 20, Loss: 6.9814, Pearson: 0.6397, Spearman: 0.5980
2025-11-14 20:37:56,254 - INFO - Fold 4 Train Epoch 18/200, Batch 30, Loss: 7.3233, Pearson: 0.6391, Spearman: 0.6024
2025-11-14 20:38:06,379 - INFO - Fold 4 Train Epoch 18/200, Batch 40, Loss: 7.1371, Pearson: 0.6402, Spearman: 0.5905
2025-11-14 20:38:16,540 - INFO - Fold 4 Train Epoch 18/200, Batch 50, Loss: 7.1125, Pearson: 0.6410, Spearman: 0.5987
2025-11-14 20:38:26,672 - INFO - Fold 4 Train Epoch 18/200, Batch 60, Loss: 7.3084, Pearson: 0.6342, Spearman: 0.6016
2025-11-14 20:38:36,819 - INFO - Fold 4 Train Epoch 18/200, Batch 70, Loss: 7.3180, Pearson: 0.6445, Spearman: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6506
2025-11-14 20:38:46,980 - INFO - Fold 4 Train Epoch 18/200, Batch 80, Loss: 7.1042, Pearson: 0.6434, Spearman: 0.6021
2025-11-14 20:38:50,227 - INFO - Fold 4 Train Epoch 18/200, Train Loss: 7.1726, Pearson Mean: 0.6411, Spearman Mean: 0.6029
2025-11-14 20:38:50,227 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4456, 'spearman_mean_genewise': 0.4024, 'l1_error_mean': 1.8714, 'l2_errors_mean': 7.1729, 'r2_scores_mean': 0.2114, 'pearson_std': 0.1153, 'l2_error_q1': 4.9135, 'l2_error_q2': 6.8136, 'l2_error_q3': 9.1258, 'r2_score_q1': 0.1319, 'r2_score_q2': 0.185, 'r2_score_q3': 0.2577, 'mape_mean': 57.901, 'mape_std': 18.0529, 'rmse_mean': 2.6344, 'rmse_std': 0.4824}
2025-11-14 20:38:50,588 - INFO - Fold 4 Val Epoch 18/200, Batch 0, Loss: 9.7583, Pearson: 0.5437, Spearman: 0.5511
2025-11-14 20:38:52,285 - INFO - Fold 4 Val Epoch 18/200, Batch 10, Loss: 8.7072, Pearson: 0.5668, Spearman: 0.5719
2025-11-14 20:38:53,977 - INFO - Fold 4 Val Epoch 18/200, Batch 20, Loss: 6.0685, Pearson: 0.5662, Spearman: 0.5634
2025-11-14 20:38:56,816 - INFO - Fold 4 Val Epoch 18/200, Val Loss: 8.1231, Pearson Mean: 0.5563, Spearman Mean: 0.5573
2025-11-14 20:38:56,817 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2989, 'spearman_mean_genewise': 0.2743, 'l1_error_mean': 2.0274, 'l2_errors_mean': 8.1461, 'r2_scores_mean': 0.0945, 'pearson_std': 0.1237, 'l2_error_q1': 5.0277, 'l2_error_q2': 7.6843, 'l2_error_q3': 11.2186, 'r2_score_q1': 0.034, 'r2_score_q2': 0.0713, 'r2_score_q3': 0.1246, 'mape_mean': 66.3807, 'mape_std': 19.1702, 'rmse_mean': 2.785, 'rmse_std': 0.6245}
2025-11-14 20:38:56,817 - INFO - Learning rate for epoch 18: 0.0001
2025-11-14 20:38:56,817 - INFO - No improvement in spearman genewise. Patience: 3/30
2025-11-14 20:38:57,890 - INFO - Fold 4 Train Epoch 19/200, Batch 0, Loss: 7.2230, Pearson: 0.6523, Spearman: 0.6075
2025-11-14 20:39:07,297 - INFO - Fold 4 Train Epoch 19/200, Batch 10, Loss: 7.0020, Pearson: 0.6465, Spearman: 0.6006
2025-11-14 20:39:14,441 - INFO - Fold 4 Train Epoch 19/200, Batch 20, Loss: 7.0936, Pearson: 0.6401, Spearman: 0.6083
2025-11-14 20:39:24,598 - INFO - Fold 4 Train Epoch 19/200, Batch 30, Loss: 6.8948, Pearson: 0.6491, Spearman: 0.6097
2025-11-14 20:39:34,771 - INFO - Fold 4 Train Epoch 19/200, Batch 40, Loss: 7.1593, Pearson: 0.6331, Spearman: 0.5983
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6407
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6339
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.172885417938232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 19 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.7544827 8.852809  0.        0.        0.
 0.        7.7544827 0.       ]
Sample y_pred values (first sample, first 10 genes):
[1.2331601 0.        1.0291613 1.9465376 2.6664693 1.6981397 0.9948188
 2.8156075 3.8433924 5.9866643]
y_true  -> mean=2.2994, std=3.5376, min=0.0000, max=12.7741
y_pred  -> mean=2.1215, std=2.2766, min=0.0000, max=12.9823
Batch 0 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6375
2025-11-14 20:39:44,886 - INFO - Fold 4 Train Epoch 19/200, Batch 50, Loss: 7.0089, Pearson: 0.6573, Spearman: 0.6115
2025-11-14 20:39:55,046 - INFO - Fold 4 Train Epoch 19/200, Batch 60, Loss: 7.1452, Pearson: 0.6376, Spearman: 0.6075
2025-11-14 20:40:05,153 - INFO - Fold 4 Train Epoch 19/200, Batch 70, Loss: 7.2603, Pearson: 0.6404, Spearman: 0.6047
2025-11-14 20:40:15,278 - INFO - Fold 4 Train Epoch 19/200, Batch 80, Loss: 6.9928, Pearson: 0.6439, Spearman: 0.6066
2025-11-14 20:40:18,516 - INFO - Fold 4 Train Epoch 19/200, Train Loss: 7.1430, Pearson Mean: 0.6431, Spearman Mean: 0.6051
2025-11-14 20:40:18,516 - INFO - Training Metrics: {'pearson_mean_genewise': 0.449, 'spearman_mean_genewise': 0.4053, 'l1_error_mean': 1.8692, 'l2_errors_mean': 7.143, 'r2_scores_mean': 0.2145, 'pearson_std': 0.1148, 'l2_error_q1': 4.8953, 'l2_error_q2': 6.7916, 'l2_error_q3': 9.1142, 'r2_score_q1': 0.1349, 'r2_score_q2': 0.188, 'r2_score_q3': 0.2616, 'mape_mean': 57.6955, 'mape_std': 18.0258, 'rmse_mean': 2.629, 'rmse_std': 0.4809}
2025-11-14 20:40:18,878 - INFO - Fold 4 Val Epoch 19/200, Batch 0, Loss: 9.7753, Pearson: 0.5448, Spearman: 0.5525
2025-11-14 20:40:20,529 - INFO - Fold 4 Val Epoch 19/200, Batch 10, Loss: 8.6638, Pearson: 0.5702, Spearman: 0.5743
2025-11-14 20:40:22,233 - INFO - Fold 4 Val Epoch 19/200, Batch 20, Loss: 6.0767, Pearson: 0.5655, Spearman: 0.5627
2025-11-14 20:40:25,134 - INFO - Fold 4 Val Epoch 19/200, Val Loss: 8.1236, Pearson Mean: 0.5569, Spearman Mean: 0.5570
2025-11-14 20:40:25,134 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2972, 'spearman_mean_genewise': 0.2745, 'l1_error_mean': 2.0102, 'l2_errors_mean': 8.1472, 'r2_scores_mean': 0.0943, 'pearson_std': 0.1245, 'l2_error_q1': 5.047, 'l2_error_q2': 7.6744, 'l2_error_q3': 11.1947, 'r2_score_q1': 0.0344, 'r2_score_q2': 0.0708, 'r2_score_q3': 0.1261, 'mape_mean': 67.2617, 'mape_std': 19.5155, 'rmse_mean': 2.7853, 'rmse_std': 0.6239}
2025-11-14 20:40:25,134 - INFO - Learning rate for epoch 19: 1e-05
2025-11-14 20:40:25,134 - INFO - No improvement in spearman genewise. Patience: 4/30
2025-11-14 20:40:26,182 - INFO - Fold 4 Train Epoch 20/200, Batch 0, Loss: 7.1569, Pearson: 0.6446, Spearman: 0.6068
2025-11-14 20:40:35,424 - INFO - Fold 4 Train Epoch 20/200, Batch 10, Loss: 7.1315, Pearson: 0.6581, Spearman: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6329
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6493
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.142964839935303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 20 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.390787]
Sample y_pred values (first sample, first 10 genes):
[0.2539884  1.3480244  1.1707127  1.1699326  3.544651   2.0138428
 0.41557813 1.671804   2.3943238  7.442075  ]
y_true  -> mean=2.1505, std=3.4990, min=0.0000, max=12.6672
y_pred  -> mean=2.1143, std=2.2415, min=0.0000, max=13.4025
Batch 0 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6465
2025-11-14 20:40:42,888 - INFO - Fold 4 Train Epoch 20/200, Batch 20, Loss: 7.1827, Pearson: 0.6426, Spearman: 0.6027
2025-11-14 20:40:52,923 - INFO - Fold 4 Train Epoch 20/200, Batch 30, Loss: 7.1374, Pearson: 0.6375, Spearman: 0.6017
2025-11-14 20:41:03,079 - INFO - Fold 4 Train Epoch 20/200, Batch 40, Loss: 6.9725, Pearson: 0.6495, Spearman: 0.6122
2025-11-14 20:41:13,233 - INFO - Fold 4 Train Epoch 20/200, Batch 50, Loss: 6.9863, Pearson: 0.6461, Spearman: 0.6114
2025-11-14 20:41:23,376 - INFO - Fold 4 Train Epoch 20/200, Batch 60, Loss: 6.8580, Pearson: 0.6501, Spearman: 0.6128
2025-11-14 20:41:33,517 - INFO - Fold 4 Train Epoch 20/200, Batch 70, Loss: 7.0311, Pearson: 0.6481, Spearman: 0.6097
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6467
2025-11-14 20:41:43,661 - INFO - Fold 4 Train Epoch 20/200, Batch 80, Loss: 7.1800, Pearson: 0.6470, Spearman: 0.6074
2025-11-14 20:41:46,902 - INFO - Fold 4 Train Epoch 20/200, Train Loss: 7.0900, Pearson Mean: 0.6463, Spearman Mean: 0.6084
2025-11-14 20:41:46,902 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4554, 'spearman_mean_genewise': 0.4107, 'l1_error_mean': 1.8634, 'l2_errors_mean': 7.0902, 'r2_scores_mean': 0.2199, 'pearson_std': 0.1137, 'l2_error_q1': 4.8603, 'l2_error_q2': 6.7506, 'l2_error_q3': 9.0411, 'r2_score_q1': 0.1397, 'r2_score_q2': 0.1929, 'r2_score_q3': 0.2676, 'mape_mean': 57.2393, 'mape_std': 18.0677, 'rmse_mean': 2.6194, 'rmse_std': 0.4785}
2025-11-14 20:41:47,284 - INFO - Fold 4 Val Epoch 20/200, Batch 0, Loss: 9.7597, Pearson: 0.5439, Spearman: 0.5508
2025-11-14 20:41:48,932 - INFO - Fold 4 Val Epoch 20/200, Batch 10, Loss: 8.6739, Pearson: 0.5692, Spearman: 0.5737
2025-11-14 20:41:50,632 - INFO - Fold 4 Val Epoch 20/200, Batch 20, Loss: 6.1060, Pearson: 0.5631, Spearman: 0.5610
2025-11-14 20:41:53,511 - INFO - Fold 4 Val Epoch 20/200, Val Loss: 8.1301, Pearson Mean: 0.5557, Spearman Mean: 0.5562
2025-11-14 20:41:53,511 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2986, 'spearman_mean_genewise': 0.2748, 'l1_error_mean': 2.0304, 'l2_errors_mean': 8.1526, 'r2_scores_mean': 0.0933, 'pearson_std': 0.1238, 'l2_error_q1': 5.051, 'l2_error_q2': 7.6588, 'l2_error_q3': 11.2079, 'r2_score_q1': 0.0321, 'r2_score_q2': 0.0708, 'r2_score_q3': 0.1252, 'mape_mean': 65.9575, 'mape_std': 19.2965, 'rmse_mean': 2.7865, 'rmse_std': 0.6229}
2025-11-14 20:41:53,512 - INFO - Learning rate for epoch 20: 1e-05
2025-11-14 20:41:53,512 - INFO - No improvement in spearman genewise. Patience: 5/30
2025-11-14 20:41:54,508 - INFO - Fold 4 Train Epoch 21/200, Batch 0, Loss: 6.9851, Pearson: 0.6467, Spearman: 0.6115
2025-11-14 20:42:03,514 - INFO - Fold 4 Train Epoch 21/200, Batch 10, Loss: 7.1384, Pearson: 0.6407, Spearman: 0.6055
2025-11-14 20:42:11,129 - INFO - Fold 4 Train Epoch 21/200, Batch 20, Loss: 7.2580, Pearson: 0.6395, Spearman: 0.6050
2025-11-14 20:42:21,185 - INFO - Fold 4 Train Epoch 21/200, Batch 30, Loss: 7.0417, Pearson: 0.6551, Spearman: 0.6139
2025-11-14 20:42:31,323 - INFO - Fold 4 Train Epoch 21/200, Batch 40, Loss: 7.1663, Pearson: 0.6404, Spearman: 0.6092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6428
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6412
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.090153694152832
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 21 =====================
Sample y_true values (first sample, first 10 genes):
[ 0.        0.        8.537592  0.        0.        0.        0.
  0.        0.       10.146873]
Sample y_pred values (first sample, first 10 genes):
[0.         0.6372334  1.2082613  0.08310002 2.1736183  0.732704
 0.21467437 0.6778435  2.63581    7.3866215 ]
y_true  -> mean=2.0527, std=3.4647, min=0.0000, max=12.4917
y_pred  -> mean=2.0971, std=2.2425, min=0.0000, max=12.7871
Batch 0 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6496
2025-11-14 20:42:41,412 - INFO - Fold 4 Train Epoch 21/200, Batch 50, Loss: 7.1336, Pearson: 0.6426, Spearman: 0.6103
2025-11-14 20:42:51,543 - INFO - Fold 4 Train Epoch 21/200, Batch 60, Loss: 6.9235, Pearson: 0.6566, Spearman: 0.6095
2025-11-14 20:43:01,703 - INFO - Fold 4 Train Epoch 21/200, Batch 70, Loss: 7.1831, Pearson: 0.6365, Spearman: 0.6011
2025-11-14 20:43:11,866 - INFO - Fold 4 Train Epoch 21/200, Batch 80, Loss: 7.0903, Pearson: 0.6479, Spearman: 0.6101
2025-11-14 20:43:15,100 - INFO - Fold 4 Train Epoch 21/200, Train Loss: 7.0731, Pearson Mean: 0.6474, Spearman Mean: 0.6094
2025-11-14 20:43:15,100 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4573, 'spearman_mean_genewise': 0.4123, 'l1_error_mean': 1.8592, 'l2_errors_mean': 7.073, 'r2_scores_mean': 0.2216, 'pearson_std': 0.1135, 'l2_error_q1': 4.8618, 'l2_error_q2': 6.7215, 'l2_error_q3': 9.0135, 'r2_score_q1': 0.141, 'r2_score_q2': 0.1944, 'r2_score_q3': 0.2689, 'mape_mean': 57.3443, 'mape_std': 18.0359, 'rmse_mean': 2.6163, 'rmse_std': 0.4773}
2025-11-14 20:43:15,486 - INFO - Fold 4 Val Epoch 21/200, Batch 0, Loss: 9.7542, Pearson: 0.5440, Spearman: 0.5509
2025-11-14 20:43:17,179 - INFO - Fold 4 Val Epoch 21/200, Batch 10, Loss: 8.6838, Pearson: 0.5685, Spearman: 0.5730
2025-11-14 20:43:18,866 - INFO - Fold 4 Val Epoch 21/200, Batch 20, Loss: 6.0823, Pearson: 0.5653, Spearman: 0.5628
2025-11-14 20:43:21,750 - INFO - Fold 4 Val Epoch 21/200, Val Loss: 8.1169, Pearson Mean: 0.5570, Spearman Mean: 0.5572
2025-11-14 20:43:21,751 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.298, 'spearman_mean_genewise': 0.2745, 'l1_error_mean': 2.0267, 'l2_errors_mean': 8.1401, 'r2_scores_mean': 0.0946, 'pearson_std': 0.1251, 'l2_error_q1': 5.0291, 'l2_error_q2': 7.6816, 'l2_error_q3': 11.1734, 'r2_score_q1': 0.0338, 'r2_score_q2': 0.0723, 'r2_score_q3': 0.126, 'mape_mean': 66.337, 'mape_std': 19.4755, 'rmse_mean': 2.7843, 'rmse_std': 0.6227}
2025-11-14 20:43:21,751 - INFO - Learning rate for epoch 21: 1e-05
2025-11-14 20:43:21,751 - INFO - No improvement in spearman genewise. Patience: 6/30
2025-11-14 20:43:22,843 - INFO - Fold 4 Train Epoch 22/200, Batch 0, Loss: 6.9404, Pearson: 0.6566, Spearman: 0.6073
2025-11-14 20:43:31,785 - INFO - Fold 4 Train Epoch 22/200, Batch 10, Loss: 7.0135, Pearson: 0.6352, Spearman: 0.5983
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6493
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6491
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.072956085205078
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 22 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       9.796185]
Sample y_pred values (first sample, first 10 genes):
[0.21407391 0.         0.70110834 0.32000068 1.4350356  0.53286725
 0.26549968 0.09789839 0.9049941  6.097316  ]
y_true  -> mean=2.1631, std=3.4907, min=0.0000, max=12.8135
y_pred  -> mean=2.0756, std=2.2708, min=0.0000, max=13.5355
Batch 0 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6484
2025-11-14 20:43:39,536 - INFO - Fold 4 Train Epoch 22/200, Batch 20, Loss: 7.0193, Pearson: 0.6445, Spearman: 0.6130
2025-11-14 20:43:49,631 - INFO - Fold 4 Train Epoch 22/200, Batch 30, Loss: 7.0167, Pearson: 0.6498, Spearman: 0.6126
2025-11-14 20:43:59,775 - INFO - Fold 4 Train Epoch 22/200, Batch 40, Loss: 7.0762, Pearson: 0.6495, Spearman: 0.6150
2025-11-14 20:44:09,937 - INFO - Fold 4 Train Epoch 22/200, Batch 50, Loss: 7.1645, Pearson: 0.6486, Spearman: 0.6115
2025-11-14 20:44:20,103 - INFO - Fold 4 Train Epoch 22/200, Batch 60, Loss: 6.9949, Pearson: 0.6501, Spearman: 0.6052
2025-11-14 20:44:30,245 - INFO - Fold 4 Train Epoch 22/200, Batch 70, Loss: 7.1979, Pearson: 0.6454, Spearman: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6485
2025-11-14 20:44:40,410 - INFO - Fold 4 Train Epoch 22/200, Batch 80, Loss: 7.2358, Pearson: 0.6392, Spearman: 0.6038
2025-11-14 20:44:43,680 - INFO - Fold 4 Train Epoch 22/200, Train Loss: 7.0645, Pearson Mean: 0.6480, Spearman Mean: 0.6101
2025-11-14 20:44:43,681 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4582, 'spearman_mean_genewise': 0.413, 'l1_error_mean': 1.8561, 'l2_errors_mean': 7.0646, 'r2_scores_mean': 0.2224, 'pearson_std': 0.1135, 'l2_error_q1': 4.8496, 'l2_error_q2': 6.7137, 'l2_error_q3': 9.0001, 'r2_score_q1': 0.1427, 'r2_score_q2': 0.1951, 'r2_score_q3': 0.2703, 'mape_mean': 57.3849, 'mape_std': 18.0102, 'rmse_mean': 2.6147, 'rmse_std': 0.4772}
2025-11-14 20:44:44,050 - INFO - Fold 4 Val Epoch 22/200, Batch 0, Loss: 9.7660, Pearson: 0.5432, Spearman: 0.5499
2025-11-14 20:44:45,762 - INFO - Fold 4 Val Epoch 22/200, Batch 10, Loss: 8.7013, Pearson: 0.5678, Spearman: 0.5724
2025-11-14 20:44:47,438 - INFO - Fold 4 Val Epoch 22/200, Batch 20, Loss: 6.0650, Pearson: 0.5657, Spearman: 0.5634
2025-11-14 20:44:50,319 - INFO - Fold 4 Val Epoch 22/200, Val Loss: 8.1233, Pearson Mean: 0.5565, Spearman Mean: 0.5569
2025-11-14 20:44:50,320 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2982, 'spearman_mean_genewise': 0.2742, 'l1_error_mean': 2.0218, 'l2_errors_mean': 8.146, 'r2_scores_mean': 0.094, 'pearson_std': 0.1248, 'l2_error_q1': 5.0417, 'l2_error_q2': 7.6733, 'l2_error_q3': 11.2022, 'r2_score_q1': 0.0332, 'r2_score_q2': 0.0708, 'r2_score_q3': 0.1254, 'mape_mean': 66.4999, 'mape_std': 19.2139, 'rmse_mean': 2.7853, 'rmse_std': 0.6231}
2025-11-14 20:44:50,320 - INFO - Learning rate for epoch 22: 1e-05
2025-11-14 20:44:50,320 - INFO - No improvement in spearman genewise. Patience: 7/30
2025-11-14 20:44:51,233 - INFO - Fold 4 Train Epoch 23/200, Batch 0, Loss: 7.0249, Pearson: 0.6473, Spearman: 0.6127
2025-11-14 20:44:59,326 - INFO - Fold 4 Train Epoch 23/200, Batch 10, Loss: 6.9411, Pearson: 0.6641, Spearman: 0.6172
2025-11-14 20:45:07,152 - INFO - Fold 4 Train Epoch 23/200, Batch 20, Loss: 7.0729, Pearson: 0.6517, Spearman: 0.6213
2025-11-14 20:45:17,300 - INFO - Fold 4 Train Epoch 23/200, Batch 30, Loss: 7.1778, Pearson: 0.6440, Spearman: 0.6094
2025-11-14 20:45:27,441 - INFO - Fold 4 Train Epoch 23/200, Batch 40, Loss: 7.1957, Pearson: 0.6470, Spearman: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6515
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6450
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.064629077911377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 23 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       9.110605 0.       0.       0.       0.       8.417569
 0.       0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.         1.2469364  1.4212663  0.         1.2176974  0.39908525
 0.27771795 0.75604904 0.9860966  7.431175  ]
y_true  -> mean=2.0808, std=3.4769, min=0.0000, max=12.5041
y_pred  -> mean=2.0801, std=2.2164, min=0.0000, max=12.9487
Batch 0 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6521
2025-11-14 20:45:37,592 - INFO - Fold 4 Train Epoch 23/200, Batch 50, Loss: 7.2489, Pearson: 0.6519, Spearman: 0.6109
2025-11-14 20:45:47,736 - INFO - Fold 4 Train Epoch 23/200, Batch 60, Loss: 7.2281, Pearson: 0.6451, Spearman: 0.6077
2025-11-14 20:45:57,916 - INFO - Fold 4 Train Epoch 23/200, Batch 70, Loss: 7.0118, Pearson: 0.6501, Spearman: 0.6114
2025-11-14 20:46:08,045 - INFO - Fold 4 Train Epoch 23/200, Batch 80, Loss: 6.9844, Pearson: 0.6515, Spearman: 0.6185
2025-11-14 20:46:11,300 - INFO - Fold 4 Train Epoch 23/200, Train Loss: 7.0580, Pearson Mean: 0.6485, Spearman Mean: 0.6105
2025-11-14 20:46:11,300 - INFO - Training Metrics: {'pearson_mean_genewise': 0.459, 'spearman_mean_genewise': 0.4137, 'l1_error_mean': 1.8554, 'l2_errors_mean': 7.058, 'r2_scores_mean': 0.2231, 'pearson_std': 0.1134, 'l2_error_q1': 4.8398, 'l2_error_q2': 6.705, 'l2_error_q3': 9.0003, 'r2_score_q1': 0.1422, 'r2_score_q2': 0.1959, 'r2_score_q3': 0.2713, 'mape_mean': 57.3682, 'mape_std': 18.0075, 'rmse_mean': 2.6135, 'rmse_std': 0.477}
2025-11-14 20:46:11,662 - INFO - Fold 4 Val Epoch 23/200, Batch 0, Loss: 9.7484, Pearson: 0.5445, Spearman: 0.5513
2025-11-14 20:46:13,306 - INFO - Fold 4 Val Epoch 23/200, Batch 10, Loss: 8.7078, Pearson: 0.5672, Spearman: 0.5719
2025-11-14 20:46:14,915 - INFO - Fold 4 Val Epoch 23/200, Batch 20, Loss: 6.0513, Pearson: 0.5671, Spearman: 0.5645
2025-11-14 20:46:17,746 - INFO - Fold 4 Val Epoch 23/200, Val Loss: 8.1150, Pearson Mean: 0.5576, Spearman Mean: 0.5578
2025-11-14 20:46:17,747 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2983, 'spearman_mean_genewise': 0.2741, 'l1_error_mean': 2.0169, 'l2_errors_mean': 8.1378, 'r2_scores_mean': 0.0949, 'pearson_std': 0.1251, 'l2_error_q1': 5.0376, 'l2_error_q2': 7.6963, 'l2_error_q3': 11.2003, 'r2_score_q1': 0.0339, 'r2_score_q2': 0.0722, 'r2_score_q3': 0.1253, 'mape_mean': 66.6467, 'mape_std': 19.3842, 'rmse_mean': 2.7839, 'rmse_std': 0.6227}
2025-11-14 20:46:17,747 - INFO - Learning rate for epoch 23: 1e-05
2025-11-14 20:46:17,747 - INFO - No improvement in spearman genewise. Patience: 8/30
2025-11-14 20:46:18,612 - INFO - Fold 4 Train Epoch 24/200, Batch 0, Loss: 7.0571, Pearson: 0.6514, Spearman: 0.6115
2025-11-14 20:46:27,121 - INFO - Fold 4 Train Epoch 24/200, Batch 10, Loss: 7.0449, Pearson: 0.6532, Spearman: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6482
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6490
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.0579681396484375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 24 =====================
Sample y_true values (first sample, first 10 genes):
[0.        6.5550075 6.5550075 0.        0.        7.940234  0.
 0.        7.2474427 8.6332035]
Sample y_pred values (first sample, first 10 genes):
[0.         2.2575178  2.8247724  0.90361667 4.7805777  3.63814
 1.1615806  1.7985477  4.073676   8.956358  ]
y_true  -> mean=2.1572, std=3.4995, min=0.0000, max=12.6369
y_pred  -> mean=2.0884, std=2.2274, min=0.0000, max=13.9086
Batch 0 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6596
2025-11-14 20:46:34,931 - INFO - Fold 4 Train Epoch 24/200, Batch 20, Loss: 6.9106, Pearson: 0.6433, Spearman: 0.6046
2025-11-14 20:46:45,056 - INFO - Fold 4 Train Epoch 24/200, Batch 30, Loss: 7.0166, Pearson: 0.6553, Spearman: 0.6113
2025-11-14 20:46:55,210 - INFO - Fold 4 Train Epoch 24/200, Batch 40, Loss: 7.0485, Pearson: 0.6559, Spearman: 0.6131
2025-11-14 20:47:05,352 - INFO - Fold 4 Train Epoch 24/200, Batch 50, Loss: 7.1572, Pearson: 0.6512, Spearman: 0.6158
2025-11-14 20:47:15,493 - INFO - Fold 4 Train Epoch 24/200, Batch 60, Loss: 7.3286, Pearson: 0.6454, Spearman: 0.6127
2025-11-14 20:47:25,646 - INFO - Fold 4 Train Epoch 24/200, Batch 70, Loss: 6.9685, Pearson: 0.6580, Spearman: 0.6173
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6517
2025-11-14 20:47:35,741 - INFO - Fold 4 Train Epoch 24/200, Batch 80, Loss: 6.9642, Pearson: 0.6555, Spearman: 0.6115
2025-11-14 20:47:38,984 - INFO - Fold 4 Train Epoch 24/200, Train Loss: 7.0461, Pearson Mean: 0.6492, Spearman Mean: 0.6112
2025-11-14 20:47:38,984 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4601, 'spearman_mean_genewise': 0.4146, 'l1_error_mean': 1.8544, 'l2_errors_mean': 7.046, 'r2_scores_mean': 0.2242, 'pearson_std': 0.1135, 'l2_error_q1': 4.8394, 'l2_error_q2': 6.7026, 'l2_error_q3': 8.9731, 'r2_score_q1': 0.1435, 'r2_score_q2': 0.1972, 'r2_score_q3': 0.2719, 'mape_mean': 57.2476, 'mape_std': 17.9981, 'rmse_mean': 2.6114, 'rmse_std': 0.4761}
2025-11-14 20:47:39,388 - INFO - Fold 4 Val Epoch 24/200, Batch 0, Loss: 9.7606, Pearson: 0.5437, Spearman: 0.5504
2025-11-14 20:47:41,040 - INFO - Fold 4 Val Epoch 24/200, Batch 10, Loss: 8.6986, Pearson: 0.5679, Spearman: 0.5726
2025-11-14 20:47:42,781 - INFO - Fold 4 Val Epoch 24/200, Batch 20, Loss: 6.0724, Pearson: 0.5647, Spearman: 0.5623
2025-11-14 20:47:45,659 - INFO - Fold 4 Val Epoch 24/200, Val Loss: 8.1258, Pearson Mean: 0.5568, Spearman Mean: 0.5569
2025-11-14 20:47:45,660 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2975, 'spearman_mean_genewise': 0.2735, 'l1_error_mean': 2.0144, 'l2_errors_mean': 8.1483, 'r2_scores_mean': 0.0938, 'pearson_std': 0.125, 'l2_error_q1': 5.0391, 'l2_error_q2': 7.6746, 'l2_error_q3': 11.2083, 'r2_score_q1': 0.0338, 'r2_score_q2': 0.0709, 'r2_score_q3': 0.1253, 'mape_mean': 66.8625, 'mape_std': 19.4004, 'rmse_mean': 2.7857, 'rmse_std': 0.623}
2025-11-14 20:47:45,660 - INFO - Learning rate for epoch 24: 1e-05
2025-11-14 20:47:45,660 - INFO - No improvement in spearman genewise. Patience: 9/30
2025-11-14 20:47:46,601 - INFO - Fold 4 Train Epoch 25/200, Batch 0, Loss: 7.0390, Pearson: 0.6362, Spearman: 0.6027
2025-11-14 20:47:55,177 - INFO - Fold 4 Train Epoch 25/200, Batch 10, Loss: 7.0892, Pearson: 0.6432, Spearman: 0.6115
2025-11-14 20:48:03,159 - INFO - Fold 4 Train Epoch 25/200, Batch 20, Loss: 6.9343, Pearson: 0.6529, Spearman: 0.6138
2025-11-14 20:48:13,320 - INFO - Fold 4 Train Epoch 25/200, Batch 30, Loss: 7.1104, Pearson: 0.6494, Spearman: 0.6105
2025-11-14 20:48:23,527 - INFO - Fold 4 Train Epoch 25/200, Batch 40, Loss: 6.8507, Pearson: 0.6627, Spearman: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6529
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6600
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.04595947265625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 25 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.886585]
Sample y_pred values (first sample, first 10 genes):
[0.07290465 0.         0.7327504  0.         2.7605875  0.6587422
 0.38948655 0.6774071  3.6500168  8.029398  ]
y_true  -> mean=1.9863, std=3.4361, min=0.0000, max=12.4617
y_pred  -> mean=2.0871, std=2.2249, min=0.0000, max=13.5383
Batch 0 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6527
2025-11-14 20:48:33,666 - INFO - Fold 4 Train Epoch 25/200, Batch 50, Loss: 7.1370, Pearson: 0.6443, Spearman: 0.6050
2025-11-14 20:48:43,794 - INFO - Fold 4 Train Epoch 25/200, Batch 60, Loss: 7.1896, Pearson: 0.6491, Spearman: 0.6087
2025-11-14 20:48:53,954 - INFO - Fold 4 Train Epoch 25/200, Batch 70, Loss: 6.9673, Pearson: 0.6506, Spearman: 0.6070
2025-11-14 20:49:04,111 - INFO - Fold 4 Train Epoch 25/200, Batch 80, Loss: 7.1289, Pearson: 0.6522, Spearman: 0.6153
2025-11-14 20:49:07,413 - INFO - Fold 4 Train Epoch 25/200, Train Loss: 7.0469, Pearson Mean: 0.6492, Spearman Mean: 0.6113
2025-11-14 20:49:07,413 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4602, 'spearman_mean_genewise': 0.4147, 'l1_error_mean': 1.856, 'l2_errors_mean': 7.047, 'r2_scores_mean': 0.2242, 'pearson_std': 0.1133, 'l2_error_q1': 4.8358, 'l2_error_q2': 6.7021, 'l2_error_q3': 8.9872, 'r2_score_q1': 0.144, 'r2_score_q2': 0.1973, 'r2_score_q3': 0.2727, 'mape_mean': 57.1958, 'mape_std': 18.0035, 'rmse_mean': 2.6115, 'rmse_std': 0.4765}
2025-11-14 20:49:07,742 - INFO - Fold 4 Val Epoch 25/200, Batch 0, Loss: 9.7607, Pearson: 0.5439, Spearman: 0.5506
2025-11-14 20:49:09,473 - INFO - Fold 4 Val Epoch 25/200, Batch 10, Loss: 8.6811, Pearson: 0.5684, Spearman: 0.5731
2025-11-14 20:49:11,217 - INFO - Fold 4 Val Epoch 25/200, Batch 20, Loss: 6.0979, Pearson: 0.5633, Spearman: 0.5613
2025-11-14 20:49:14,150 - INFO - Fold 4 Val Epoch 25/200, Val Loss: 8.1125, Pearson Mean: 0.5575, Spearman Mean: 0.5578
2025-11-14 20:49:14,150 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2985, 'spearman_mean_genewise': 0.2739, 'l1_error_mean': 2.0356, 'l2_errors_mean': 8.1341, 'r2_scores_mean': 0.095, 'pearson_std': 0.1253, 'l2_error_q1': 5.0434, 'l2_error_q2': 7.6626, 'l2_error_q3': 11.2222, 'r2_score_q1': 0.0332, 'r2_score_q2': 0.0703, 'r2_score_q3': 0.1251, 'mape_mean': 65.9963, 'mape_std': 19.4264, 'rmse_mean': 2.7834, 'rmse_std': 0.622}
2025-11-14 20:49:14,150 - INFO - Learning rate for epoch 25: 1.0000000000000002e-06
2025-11-14 20:49:14,150 - INFO - No improvement in spearman genewise. Patience: 10/30
2025-11-14 20:49:15,126 - INFO - Fold 4 Train Epoch 26/200, Batch 0, Loss: 6.9955, Pearson: 0.6572, Spearman: 0.6168
2025-11-14 20:49:23,558 - INFO - Fold 4 Train Epoch 26/200, Batch 10, Loss: 6.9367, Pearson: 0.6534, Spearman: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6441
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6499
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.047019004821777
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 26 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       8.255089 0.       0.       0.
 0.       8.765811]
Sample y_pred values (first sample, first 10 genes):
[0.350979   2.7529347  1.5331633  1.4005382  4.341863   2.8976693
 0.11929274 1.8467124  3.0317483  8.651033  ]
y_true  -> mean=2.1947, std=3.5061, min=0.0000, max=12.4796
y_pred  -> mean=2.0975, std=2.2504, min=0.0000, max=13.8209
Batch 0 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6448
2025-11-14 20:49:31,928 - INFO - Fold 4 Train Epoch 26/200, Batch 20, Loss: 6.8174, Pearson: 0.6481, Spearman: 0.6161
2025-11-14 20:49:42,089 - INFO - Fold 4 Train Epoch 26/200, Batch 30, Loss: 6.9236, Pearson: 0.6494, Spearman: 0.6140
2025-11-14 20:49:52,241 - INFO - Fold 4 Train Epoch 26/200, Batch 40, Loss: 7.0118, Pearson: 0.6568, Spearman: 0.6150
2025-11-14 20:50:02,413 - INFO - Fold 4 Train Epoch 26/200, Batch 50, Loss: 6.9505, Pearson: 0.6600, Spearman: 0.6176
2025-11-14 20:50:12,580 - INFO - Fold 4 Train Epoch 26/200, Batch 60, Loss: 7.0061, Pearson: 0.6455, Spearman: 0.6093
2025-11-14 20:50:22,758 - INFO - Fold 4 Train Epoch 26/200, Batch 70, Loss: 7.1331, Pearson: 0.6460, Spearman: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6420
2025-11-14 20:50:32,916 - INFO - Fold 4 Train Epoch 26/200, Batch 80, Loss: 6.9178, Pearson: 0.6536, Spearman: 0.6098
2025-11-14 20:50:36,124 - INFO - Fold 4 Train Epoch 26/200, Train Loss: 7.0364, Pearson Mean: 0.6498, Spearman Mean: 0.6120
2025-11-14 20:50:36,124 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4613, 'spearman_mean_genewise': 0.4156, 'l1_error_mean': 1.8537, 'l2_errors_mean': 7.0364, 'r2_scores_mean': 0.2252, 'pearson_std': 0.1133, 'l2_error_q1': 4.8371, 'l2_error_q2': 6.6819, 'l2_error_q3': 8.9693, 'r2_score_q1': 0.1447, 'r2_score_q2': 0.1979, 'r2_score_q3': 0.2727, 'mape_mean': 57.2393, 'mape_std': 18.0173, 'rmse_mean': 2.6096, 'rmse_std': 0.4758}
2025-11-14 20:50:36,511 - INFO - Fold 4 Val Epoch 26/200, Batch 0, Loss: 9.7657, Pearson: 0.5435, Spearman: 0.5502
2025-11-14 20:50:38,209 - INFO - Fold 4 Val Epoch 26/200, Batch 10, Loss: 8.7054, Pearson: 0.5670, Spearman: 0.5717
2025-11-14 20:50:39,922 - INFO - Fold 4 Val Epoch 26/200, Batch 20, Loss: 6.0854, Pearson: 0.5649, Spearman: 0.5622
2025-11-14 20:50:42,772 - INFO - Fold 4 Val Epoch 26/200, Val Loss: 8.1114, Pearson Mean: 0.5578, Spearman Mean: 0.5579
2025-11-14 20:50:42,773 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2979, 'spearman_mean_genewise': 0.2735, 'l1_error_mean': 2.0301, 'l2_errors_mean': 8.1337, 'r2_scores_mean': 0.0952, 'pearson_std': 0.1257, 'l2_error_q1': 5.0424, 'l2_error_q2': 7.6755, 'l2_error_q3': 11.2134, 'r2_score_q1': 0.0337, 'r2_score_q2': 0.0711, 'r2_score_q3': 0.1242, 'mape_mean': 66.2521, 'mape_std': 19.5335, 'rmse_mean': 2.7832, 'rmse_std': 0.6223}
2025-11-14 20:50:42,773 - INFO - Learning rate for epoch 26: 1.0000000000000002e-06
2025-11-14 20:50:42,773 - INFO - No improvement in spearman genewise. Patience: 11/30
2025-11-14 20:50:43,699 - INFO - Fold 4 Train Epoch 27/200, Batch 0, Loss: 6.9348, Pearson: 0.6550, Spearman: 0.6116
2025-11-14 20:50:51,906 - INFO - Fold 4 Train Epoch 27/200, Batch 10, Loss: 6.9147, Pearson: 0.6619, Spearman: 0.6129
2025-11-14 20:51:00,446 - INFO - Fold 4 Train Epoch 27/200, Batch 20, Loss: 6.9035, Pearson: 0.6557, Spearman: 0.6133
2025-11-14 20:51:10,624 - INFO - Fold 4 Train Epoch 27/200, Batch 30, Loss: 6.8873, Pearson: 0.6572, Spearman: 0.6081
2025-11-14 20:51:20,764 - INFO - Fold 4 Train Epoch 27/200, Batch 40, Loss: 6.9779, Pearson: 0.6458, Spearman: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6526
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6556
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.0363922119140625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 27 =====================
Sample y_true values (first sample, first 10 genes):
[0.        6.613448  6.613448  0.        6.613448  7.305924  0.
 6.613448  6.613448  8.9148245]
Sample y_pred values (first sample, first 10 genes):
[1.0764375  5.5969563  3.5340128  2.6549418  7.2638645  5.2381477
 0.91001016 2.9457726  4.162312   8.705826  ]
y_true  -> mean=2.1278, std=3.4848, min=0.0000, max=12.6288
y_pred  -> mean=2.1017, std=2.2693, min=0.0000, max=12.8184
Batch 0 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6483
2025-11-14 20:51:30,920 - INFO - Fold 4 Train Epoch 27/200, Batch 50, Loss: 7.2902, Pearson: 0.6405, Spearman: 0.6057
2025-11-14 20:51:41,114 - INFO - Fold 4 Train Epoch 27/200, Batch 60, Loss: 6.9437, Pearson: 0.6470, Spearman: 0.6072
2025-11-14 20:51:51,260 - INFO - Fold 4 Train Epoch 27/200, Batch 70, Loss: 6.7782, Pearson: 0.6601, Spearman: 0.6110
2025-11-14 20:52:01,423 - INFO - Fold 4 Train Epoch 27/200, Batch 80, Loss: 7.1587, Pearson: 0.6622, Spearman: 0.6188
2025-11-14 20:52:04,653 - INFO - Fold 4 Train Epoch 27/200, Train Loss: 7.0343, Pearson Mean: 0.6499, Spearman Mean: 0.6120
2025-11-14 20:52:04,653 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4616, 'spearman_mean_genewise': 0.4157, 'l1_error_mean': 1.8536, 'l2_errors_mean': 7.0345, 'r2_scores_mean': 0.2254, 'pearson_std': 0.1132, 'l2_error_q1': 4.8436, 'l2_error_q2': 6.7003, 'l2_error_q3': 8.9651, 'r2_score_q1': 0.145, 'r2_score_q2': 0.1985, 'r2_score_q3': 0.2727, 'mape_mean': 57.2328, 'mape_std': 18.0061, 'rmse_mean': 2.6093, 'rmse_std': 0.4754}
2025-11-14 20:52:05,057 - INFO - Fold 4 Val Epoch 27/200, Batch 0, Loss: 9.7628, Pearson: 0.5433, Spearman: 0.5500
2025-11-14 20:52:06,745 - INFO - Fold 4 Val Epoch 27/200, Batch 10, Loss: 8.6916, Pearson: 0.5681, Spearman: 0.5729
2025-11-14 20:52:08,413 - INFO - Fold 4 Val Epoch 27/200, Batch 20, Loss: 6.0730, Pearson: 0.5649, Spearman: 0.5627
2025-11-14 20:52:11,257 - INFO - Fold 4 Val Epoch 27/200, Val Loss: 8.1197, Pearson Mean: 0.5568, Spearman Mean: 0.5571
2025-11-14 20:52:11,258 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.298, 'spearman_mean_genewise': 0.2739, 'l1_error_mean': 2.0252, 'l2_errors_mean': 8.1422, 'r2_scores_mean': 0.094, 'pearson_std': 0.1252, 'l2_error_q1': 5.0434, 'l2_error_q2': 7.6633, 'l2_error_q3': 11.198, 'r2_score_q1': 0.0331, 'r2_score_q2': 0.0709, 'r2_score_q3': 0.1246, 'mape_mean': 66.3916, 'mape_std': 19.2549, 'rmse_mean': 2.7849, 'rmse_std': 0.6219}
2025-11-14 20:52:11,258 - INFO - Learning rate for epoch 27: 1.0000000000000002e-06
2025-11-14 20:52:11,258 - INFO - No improvement in spearman genewise. Patience: 12/30
2025-11-14 20:52:12,232 - INFO - Fold 4 Train Epoch 28/200, Batch 0, Loss: 6.6197, Pearson: 0.6638, Spearman: 0.6143
2025-11-14 20:52:20,433 - INFO - Fold 4 Train Epoch 28/200, Batch 10, Loss: 6.9424, Pearson: 0.6541, Spearman: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6401
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6483
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.034530162811279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 28 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 6.540399 6.540399]
Sample y_pred values (first sample, first 10 genes):
[0.32421902 0.         1.1022764  0.43837264 3.2041154  1.7469093
 0.30242825 1.0821086  4.5895996  8.099796  ]
y_true  -> mean=2.0262, std=3.4389, min=0.0000, max=13.8155
y_pred  -> mean=2.0882, std=2.3082, min=0.0000, max=13.3117
Batch 0 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6522
2025-11-14 20:52:28,626 - INFO - Fold 4 Train Epoch 28/200, Batch 20, Loss: 7.1566, Pearson: 0.6473, Spearman: 0.6073
2025-11-14 20:52:38,775 - INFO - Fold 4 Train Epoch 28/200, Batch 30, Loss: 7.2072, Pearson: 0.6374, Spearman: 0.6023
2025-11-14 20:52:48,935 - INFO - Fold 4 Train Epoch 28/200, Batch 40, Loss: 6.9979, Pearson: 0.6411, Spearman: 0.6046
2025-11-14 20:52:59,092 - INFO - Fold 4 Train Epoch 28/200, Batch 50, Loss: 6.7077, Pearson: 0.6608, Spearman: 0.6211
2025-11-14 20:53:09,251 - INFO - Fold 4 Train Epoch 28/200, Batch 60, Loss: 7.2342, Pearson: 0.6402, Spearman: 0.5997
2025-11-14 20:53:19,418 - INFO - Fold 4 Train Epoch 28/200, Batch 70, Loss: 6.9299, Pearson: 0.6485, Spearman: 0.6121
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6514
2025-11-14 20:53:29,566 - INFO - Fold 4 Train Epoch 28/200, Batch 80, Loss: 7.0124, Pearson: 0.6450, Spearman: 0.6093
2025-11-14 20:53:32,907 - INFO - Fold 4 Train Epoch 28/200, Train Loss: 7.0321, Pearson Mean: 0.6501, Spearman Mean: 0.6122
2025-11-14 20:53:32,908 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4618, 'spearman_mean_genewise': 0.4159, 'l1_error_mean': 1.8533, 'l2_errors_mean': 7.0322, 'r2_scores_mean': 0.2256, 'pearson_std': 0.1132, 'l2_error_q1': 4.8374, 'l2_error_q2': 6.6929, 'l2_error_q3': 8.961, 'r2_score_q1': 0.1448, 'r2_score_q2': 0.1984, 'r2_score_q3': 0.2729, 'mape_mean': 57.2094, 'mape_std': 17.9957, 'rmse_mean': 2.6089, 'rmse_std': 0.4754}
2025-11-14 20:53:33,204 - INFO - Fold 4 Val Epoch 28/200, Batch 0, Loss: 9.7472, Pearson: 0.5444, Spearman: 0.5511
2025-11-14 20:53:34,952 - INFO - Fold 4 Val Epoch 28/200, Batch 10, Loss: 8.6813, Pearson: 0.5693, Spearman: 0.5739
2025-11-14 20:53:36,713 - INFO - Fold 4 Val Epoch 28/200, Batch 20, Loss: 6.0668, Pearson: 0.5647, Spearman: 0.5623
2025-11-14 20:53:39,576 - INFO - Fold 4 Val Epoch 28/200, Val Loss: 8.1123, Pearson Mean: 0.5575, Spearman Mean: 0.5575
2025-11-14 20:53:39,577 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2983, 'spearman_mean_genewise': 0.274, 'l1_error_mean': 2.0159, 'l2_errors_mean': 8.1349, 'r2_scores_mean': 0.095, 'pearson_std': 0.1256, 'l2_error_q1': 5.039, 'l2_error_q2': 7.6759, 'l2_error_q3': 11.2021, 'r2_score_q1': 0.0337, 'r2_score_q2': 0.0722, 'r2_score_q3': 0.1266, 'mape_mean': 66.8719, 'mape_std': 19.2904, 'rmse_mean': 2.7835, 'rmse_std': 0.622}
2025-11-14 20:53:39,577 - INFO - Learning rate for epoch 28: 1.0000000000000002e-06
2025-11-14 20:53:39,577 - INFO - No improvement in spearman genewise. Patience: 13/30
2025-11-14 20:53:40,536 - INFO - Fold 4 Train Epoch 29/200, Batch 0, Loss: 6.7968, Pearson: 0.6558, Spearman: 0.6175
2025-11-14 20:53:48,598 - INFO - Fold 4 Train Epoch 29/200, Batch 10, Loss: 6.9034, Pearson: 0.6573, Spearman: 0.6204
2025-11-14 20:53:57,174 - INFO - Fold 4 Train Epoch 29/200, Batch 20, Loss: 6.9371, Pearson: 0.6420, Spearman: 0.6053
2025-11-14 20:54:07,294 - INFO - Fold 4 Train Epoch 29/200, Batch 30, Loss: 7.2021, Pearson: 0.6493, Spearman: 0.6203
2025-11-14 20:54:17,430 - INFO - Fold 4 Train Epoch 29/200, Batch 40, Loss: 6.9099, Pearson: 0.6496, Spearman: 0.6113
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6470
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6519
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.03220796585083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 29 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       7.648471 0.       0.       7.648471
 7.648471 9.034408]
Sample y_pred values (first sample, first 10 genes):
[1.3471844  0.         1.3215896  2.370799   2.3671052  2.0104117
 0.86722857 3.414525   3.939127   5.844977  ]
y_true  -> mean=2.0325, std=3.4522, min=0.0000, max=12.7969
y_pred  -> mean=2.0888, std=2.2377, min=0.0000, max=13.3075
Batch 0 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6643
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6461
2025-11-14 20:54:27,542 - INFO - Fold 4 Train Epoch 29/200, Batch 50, Loss: 6.8377, Pearson: 0.6563, Spearman: 0.6110
2025-11-14 20:54:37,701 - INFO - Fold 4 Train Epoch 29/200, Batch 60, Loss: 6.9408, Pearson: 0.6565, Spearman: 0.6144
2025-11-14 20:54:47,814 - INFO - Fold 4 Train Epoch 29/200, Batch 70, Loss: 7.1910, Pearson: 0.6459, Spearman: 0.6124
2025-11-14 20:54:57,981 - INFO - Fold 4 Train Epoch 29/200, Batch 80, Loss: 6.8625, Pearson: 0.6566, Spearman: 0.6081
2025-11-14 20:55:01,176 - INFO - Fold 4 Train Epoch 29/200, Train Loss: 7.0307, Pearson Mean: 0.6502, Spearman Mean: 0.6123
2025-11-14 20:55:01,176 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4619, 'spearman_mean_genewise': 0.416, 'l1_error_mean': 1.8533, 'l2_errors_mean': 7.0307, 'r2_scores_mean': 0.2257, 'pearson_std': 0.1133, 'l2_error_q1': 4.828, 'l2_error_q2': 6.6821, 'l2_error_q3': 8.9602, 'r2_score_q1': 0.1455, 'r2_score_q2': 0.1981, 'r2_score_q3': 0.2745, 'mape_mean': 57.2072, 'mape_std': 18.0051, 'rmse_mean': 2.6086, 'rmse_std': 0.4753}
2025-11-14 20:55:01,553 - INFO - Fold 4 Val Epoch 29/200, Batch 0, Loss: 9.7622, Pearson: 0.5438, Spearman: 0.5505
2025-11-14 20:55:03,261 - INFO - Fold 4 Val Epoch 29/200, Batch 10, Loss: 8.7092, Pearson: 0.5666, Spearman: 0.5714
2025-11-14 20:55:04,987 - INFO - Fold 4 Val Epoch 29/200, Batch 20, Loss: 6.0933, Pearson: 0.5642, Spearman: 0.5620
2025-11-14 20:55:07,897 - INFO - Fold 4 Val Epoch 29/200, Val Loss: 8.1279, Pearson Mean: 0.5562, Spearman Mean: 0.5566
2025-11-14 20:55:07,898 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2972, 'spearman_mean_genewise': 0.2731, 'l1_error_mean': 2.0355, 'l2_errors_mean': 8.1501, 'r2_scores_mean': 0.0933, 'pearson_std': 0.1251, 'l2_error_q1': 5.0429, 'l2_error_q2': 7.6761, 'l2_error_q3': 11.2191, 'r2_score_q1': 0.0326, 'r2_score_q2': 0.0703, 'r2_score_q3': 0.123, 'mape_mean': 65.9914, 'mape_std': 19.4183, 'rmse_mean': 2.7862, 'rmse_std': 0.6224}
2025-11-14 20:55:07,898 - INFO - Learning rate for epoch 29: 1.0000000000000002e-06
2025-11-14 20:55:07,898 - INFO - No improvement in spearman genewise. Patience: 14/30
2025-11-14 20:55:08,940 - INFO - Fold 4 Train Epoch 30/200, Batch 0, Loss: 6.7968, Pearson: 0.6569, Spearman: 0.6187
2025-11-14 20:55:16,601 - INFO - Fold 4 Train Epoch 30/200, Batch 10, Loss: 7.1557, Pearson: 0.6447, Spearman: 0.6063
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6438
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6430
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.03070068359375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 30 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.         0.11378431 0.35580617 0.         0.6536248  0.
 0.24879646 1.0342895  1.128186   6.679535  ]
y_true  -> mean=2.0521, std=3.4570, min=0.0000, max=12.5023
y_pred  -> mean=2.0927, std=2.2463, min=0.0000, max=13.0878
Batch 0 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 20:55:25,571 - INFO - Fold 4 Train Epoch 30/200, Batch 20, Loss: 6.8640, Pearson: 0.6496, Spearman: 0.6075
2025-11-14 20:55:35,760 - INFO - Fold 4 Train Epoch 30/200, Batch 30, Loss: 6.9925, Pearson: 0.6495, Spearman: 0.6072
2025-11-14 20:55:45,929 - INFO - Fold 4 Train Epoch 30/200, Batch 40, Loss: 7.0573, Pearson: 0.6466, Spearman: 0.6091
2025-11-14 20:55:56,080 - INFO - Fold 4 Train Epoch 30/200, Batch 50, Loss: 7.0710, Pearson: 0.6400, Spearman: 0.6078
2025-11-14 20:56:06,235 - INFO - Fold 4 Train Epoch 30/200, Batch 60, Loss: 7.0042, Pearson: 0.6546, Spearman: 0.6126
2025-11-14 20:56:16,426 - INFO - Fold 4 Train Epoch 30/200, Batch 70, Loss: 7.0507, Pearson: 0.6490, Spearman: 0.6115
Batch 19 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6423
2025-11-14 20:56:26,578 - INFO - Fold 4 Train Epoch 30/200, Batch 80, Loss: 6.9795, Pearson: 0.6537, Spearman: 0.6171
2025-11-14 20:56:29,796 - INFO - Fold 4 Train Epoch 30/200, Train Loss: 7.0313, Pearson Mean: 0.6501, Spearman Mean: 0.6121
2025-11-14 20:56:29,796 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4619, 'spearman_mean_genewise': 0.416, 'l1_error_mean': 1.8532, 'l2_errors_mean': 7.032, 'r2_scores_mean': 0.2257, 'pearson_std': 0.1131, 'l2_error_q1': 4.8282, 'l2_error_q2': 6.6822, 'l2_error_q3': 8.9595, 'r2_score_q1': 0.1464, 'r2_score_q2': 0.1991, 'r2_score_q3': 0.2738, 'mape_mean': 57.1777, 'mape_std': 17.9931, 'rmse_mean': 2.6088, 'rmse_std': 0.4757}
2025-11-14 20:56:30,185 - INFO - Fold 4 Val Epoch 30/200, Batch 0, Loss: 9.7743, Pearson: 0.5431, Spearman: 0.5498
2025-11-14 20:56:31,858 - INFO - Fold 4 Val Epoch 30/200, Batch 10, Loss: 8.6891, Pearson: 0.5678, Spearman: 0.5726
2025-11-14 20:56:33,533 - INFO - Fold 4 Val Epoch 30/200, Batch 20, Loss: 6.1105, Pearson: 0.5629, Spearman: 0.5608
2025-11-14 20:56:36,366 - INFO - Fold 4 Val Epoch 30/200, Val Loss: 8.1294, Pearson Mean: 0.5562, Spearman Mean: 0.5566
2025-11-14 20:56:36,367 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.297, 'spearman_mean_genewise': 0.2732, 'l1_error_mean': 2.0389, 'l2_errors_mean': 8.1514, 'r2_scores_mean': 0.0931, 'pearson_std': 0.1249, 'l2_error_q1': 5.0482, 'l2_error_q2': 7.6589, 'l2_error_q3': 11.2243, 'r2_score_q1': 0.0322, 'r2_score_q2': 0.0703, 'r2_score_q3': 0.1231, 'mape_mean': 65.9344, 'mape_std': 19.4619, 'rmse_mean': 2.7864, 'rmse_std': 0.6223}
2025-11-14 20:56:36,367 - INFO - Learning rate for epoch 30: 1.0000000000000002e-06
2025-11-14 20:56:36,367 - INFO - No improvement in spearman genewise. Patience: 15/30
2025-11-14 20:56:37,202 - INFO - Fold 4 Train Epoch 31/200, Batch 0, Loss: 6.8705, Pearson: 0.6516, Spearman: 0.6161
2025-11-14 20:56:44,537 - INFO - Fold 4 Train Epoch 31/200, Batch 10, Loss: 7.0681, Pearson: 0.6480, Spearman: 0.6133
2025-11-14 20:56:53,521 - INFO - Fold 4 Train Epoch 31/200, Batch 20, Loss: 6.8543, Pearson: 0.6587, Spearman: 0.6163
2025-11-14 20:57:03,679 - INFO - Fold 4 Train Epoch 31/200, Batch 30, Loss: 7.0920, Pearson: 0.6466, Spearman: 0.6127
2025-11-14 20:57:13,841 - INFO - Fold 4 Train Epoch 31/200, Batch 40, Loss: 7.1497, Pearson: 0.6507, Spearman: 0.6171
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6518
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6619
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.031990051269531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 31 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 0.        0.        1.5549495]
y_true  -> mean=2.0468, std=3.4545, min=0.0000, max=13.8155
y_pred  -> mean=2.1045, std=2.2533, min=0.0000, max=13.7307
Batch 0 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 20:57:24,010 - INFO - Fold 4 Train Epoch 31/200, Batch 50, Loss: 7.1948, Pearson: 0.6503, Spearman: 0.6098
2025-11-14 20:57:34,171 - INFO - Fold 4 Train Epoch 31/200, Batch 60, Loss: 6.9693, Pearson: 0.6531, Spearman: 0.6168
2025-11-14 20:57:44,325 - INFO - Fold 4 Train Epoch 31/200, Batch 70, Loss: 7.0442, Pearson: 0.6389, Spearman: 0.6060
2025-11-14 20:57:54,468 - INFO - Fold 4 Train Epoch 31/200, Batch 80, Loss: 6.9929, Pearson: 0.6562, Spearman: 0.6174
2025-11-14 20:57:57,683 - INFO - Fold 4 Train Epoch 31/200, Train Loss: 7.0387, Pearson Mean: 0.6497, Spearman Mean: 0.6120
2025-11-14 20:57:57,683 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4612, 'spearman_mean_genewise': 0.4154, 'l1_error_mean': 1.855, 'l2_errors_mean': 7.0385, 'r2_scores_mean': 0.225, 'pearson_std': 0.1131, 'l2_error_q1': 4.8254, 'l2_error_q2': 6.6938, 'l2_error_q3': 8.9744, 'r2_score_q1': 0.1445, 'r2_score_q2': 0.1984, 'r2_score_q3': 0.273, 'mape_mean': 57.2532, 'mape_std': 18.0126, 'rmse_mean': 2.61, 'rmse_std': 0.4758}
2025-11-14 20:57:58,051 - INFO - Fold 4 Val Epoch 31/200, Batch 0, Loss: 9.7725, Pearson: 0.5430, Spearman: 0.5497
2025-11-14 20:57:59,750 - INFO - Fold 4 Val Epoch 31/200, Batch 10, Loss: 8.6862, Pearson: 0.5682, Spearman: 0.5728
2025-11-14 20:58:01,395 - INFO - Fold 4 Val Epoch 31/200, Batch 20, Loss: 6.0979, Pearson: 0.5636, Spearman: 0.5614
2025-11-14 20:58:04,295 - INFO - Fold 4 Val Epoch 31/200, Val Loss: 8.1168, Pearson Mean: 0.5572, Spearman Mean: 0.5576
2025-11-14 20:58:04,296 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2977, 'spearman_mean_genewise': 0.2733, 'l1_error_mean': 2.0359, 'l2_errors_mean': 8.1387, 'r2_scores_mean': 0.0945, 'pearson_std': 0.1255, 'l2_error_q1': 5.0437, 'l2_error_q2': 7.6651, 'l2_error_q3': 11.2411, 'r2_score_q1': 0.0332, 'r2_score_q2': 0.0703, 'r2_score_q3': 0.1243, 'mape_mean': 66.0854, 'mape_std': 19.4255, 'rmse_mean': 2.7842, 'rmse_std': 0.6222}
2025-11-14 20:58:04,296 - INFO - Learning rate for epoch 31: 1.0000000000000002e-07
2025-11-14 20:58:04,296 - INFO - No improvement in spearman genewise. Patience: 16/30
2025-11-14 20:58:05,211 - INFO - Fold 4 Train Epoch 32/200, Batch 0, Loss: 6.9886, Pearson: 0.6426, Spearman: 0.6059
2025-11-14 20:58:12,384 - INFO - Fold 4 Train Epoch 32/200, Batch 10, Loss: 6.9407, Pearson: 0.6462, Spearman: 0.6099
Batch 49 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6418
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6469
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.038553237915039
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 32 =====================
Sample y_true values (first sample, first 10 genes):
[0.        7.8737483 0.        7.1809816 6.488595  8.096816  0.
 6.488595  6.488595  8.279086 ]
Sample y_pred values (first sample, first 10 genes):
[0.8323278  5.3987966  3.7027645  2.8087173  6.9080973  5.8475604
 0.94706887 2.1343632  4.4120145  9.266182  ]
y_true  -> mean=1.9855, std=3.4472, min=0.0000, max=12.6369
y_pred  -> mean=2.0946, std=2.2365, min=0.0000, max=13.6522
Batch 0 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 20:58:21,842 - INFO - Fold 4 Train Epoch 32/200, Batch 20, Loss: 6.8783, Pearson: 0.6478, Spearman: 0.6067
2025-11-14 20:58:31,971 - INFO - Fold 4 Train Epoch 32/200, Batch 30, Loss: 7.0347, Pearson: 0.6575, Spearman: 0.6132
2025-11-14 20:58:42,144 - INFO - Fold 4 Train Epoch 32/200, Batch 40, Loss: 6.8888, Pearson: 0.6580, Spearman: 0.6178
2025-11-14 20:58:52,278 - INFO - Fold 4 Train Epoch 32/200, Batch 50, Loss: 7.0349, Pearson: 0.6517, Spearman: 0.6157
2025-11-14 20:59:02,457 - INFO - Fold 4 Train Epoch 32/200, Batch 60, Loss: 7.0376, Pearson: 0.6511, Spearman: 0.6145
2025-11-14 20:59:12,595 - INFO - Fold 4 Train Epoch 32/200, Batch 70, Loss: 6.8835, Pearson: 0.6543, Spearman: 0.6152
Batch 20 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6447
2025-11-14 20:59:22,772 - INFO - Fold 4 Train Epoch 32/200, Batch 80, Loss: 6.9457, Pearson: 0.6443, Spearman: 0.6078
2025-11-14 20:59:25,884 - INFO - Fold 4 Train Epoch 32/200, Train Loss: 7.0297, Pearson Mean: 0.6502, Spearman Mean: 0.6123
2025-11-14 20:59:25,884 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4622, 'spearman_mean_genewise': 0.4162, 'l1_error_mean': 1.8521, 'l2_errors_mean': 7.0293, 'r2_scores_mean': 0.2259, 'pearson_std': 0.1131, 'l2_error_q1': 4.8287, 'l2_error_q2': 6.6835, 'l2_error_q3': 8.9629, 'r2_score_q1': 0.1456, 'r2_score_q2': 0.1991, 'r2_score_q3': 0.2735, 'mape_mean': 57.1753, 'mape_std': 17.9943, 'rmse_mean': 2.6083, 'rmse_std': 0.4754}
2025-11-14 20:59:26,235 - INFO - Fold 4 Val Epoch 32/200, Batch 0, Loss: 9.7630, Pearson: 0.5438, Spearman: 0.5504
2025-11-14 20:59:27,948 - INFO - Fold 4 Val Epoch 32/200, Batch 10, Loss: 8.6699, Pearson: 0.5691, Spearman: 0.5737
2025-11-14 20:59:29,678 - INFO - Fold 4 Val Epoch 32/200, Batch 20, Loss: 6.0998, Pearson: 0.5638, Spearman: 0.5617
2025-11-14 20:59:32,486 - INFO - Fold 4 Val Epoch 32/200, Val Loss: 8.1131, Pearson Mean: 0.5574, Spearman Mean: 0.5578
2025-11-14 20:59:32,486 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2982, 'spearman_mean_genewise': 0.2738, 'l1_error_mean': 2.04, 'l2_errors_mean': 8.1352, 'r2_scores_mean': 0.0948, 'pearson_std': 0.1255, 'l2_error_q1': 5.0464, 'l2_error_q2': 7.6595, 'l2_error_q3': 11.2232, 'r2_score_q1': 0.0332, 'r2_score_q2': 0.0708, 'r2_score_q3': 0.1248, 'mape_mean': 65.8923, 'mape_std': 19.4094, 'rmse_mean': 2.7836, 'rmse_std': 0.6218}
2025-11-14 20:59:32,486 - INFO - Learning rate for epoch 32: 1.0000000000000002e-07
2025-11-14 20:59:32,487 - INFO - No improvement in spearman genewise. Patience: 17/30
2025-11-14 20:59:33,390 - INFO - Fold 4 Train Epoch 33/200, Batch 0, Loss: 7.0157, Pearson: 0.6483, Spearman: 0.6106
2025-11-14 20:59:40,626 - INFO - Fold 4 Train Epoch 33/200, Batch 10, Loss: 7.2381, Pearson: 0.6478, Spearman: 0.6121
2025-11-14 20:59:50,436 - INFO - Fold 4 Train Epoch 33/200, Batch 20, Loss: 7.1478, Pearson: 0.6392, Spearman: 0.6114
2025-11-14 21:00:00,603 - INFO - Fold 4 Train Epoch 33/200, Batch 30, Loss: 6.8193, Pearson: 0.6527, Spearman: 0.6158
2025-11-14 21:00:10,750 - INFO - Fold 4 Train Epoch 33/200, Batch 40, Loss: 6.9355, Pearson: 0.6416, Spearman: 0.6055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6497
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6418
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.029345989227295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 33 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        6.373554  0.        0.
 0.        0.        8.6746025]
Sample y_pred values (first sample, first 10 genes):
[0.54919523 1.065308   1.0408804  0.8003479  4.263687   1.8125465
 0.56641066 1.2593014  3.8249197  8.146702  ]
y_true  -> mean=2.0908, std=3.4780, min=0.0000, max=12.7954
y_pred  -> mean=2.0899, std=2.2029, min=0.0000, max=13.0070
Batch 0 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6309
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6421
2025-11-14 21:00:20,877 - INFO - Fold 4 Train Epoch 33/200, Batch 50, Loss: 6.9981, Pearson: 0.6549, Spearman: 0.6170
2025-11-14 21:00:31,052 - INFO - Fold 4 Train Epoch 33/200, Batch 60, Loss: 7.1213, Pearson: 0.6531, Spearman: 0.6184
2025-11-14 21:00:41,202 - INFO - Fold 4 Train Epoch 33/200, Batch 70, Loss: 7.2371, Pearson: 0.6518, Spearman: 0.6160
2025-11-14 21:00:51,363 - INFO - Fold 4 Train Epoch 33/200, Batch 80, Loss: 6.9366, Pearson: 0.6606, Spearman: 0.6274
2025-11-14 21:00:54,495 - INFO - Fold 4 Train Epoch 33/200, Train Loss: 7.0331, Pearson Mean: 0.6500, Spearman Mean: 0.6122
2025-11-14 21:00:54,495 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4618, 'spearman_mean_genewise': 0.4159, 'l1_error_mean': 1.8537, 'l2_errors_mean': 7.033, 'r2_scores_mean': 0.2256, 'pearson_std': 0.1131, 'l2_error_q1': 4.8335, 'l2_error_q2': 6.6863, 'l2_error_q3': 8.9571, 'r2_score_q1': 0.1444, 'r2_score_q2': 0.1987, 'r2_score_q3': 0.274, 'mape_mean': 57.2158, 'mape_std': 17.9975, 'rmse_mean': 2.609, 'rmse_std': 0.4754}
2025-11-14 21:00:54,894 - INFO - Fold 4 Val Epoch 33/200, Batch 0, Loss: 9.7563, Pearson: 0.5439, Spearman: 0.5506
2025-11-14 21:00:56,664 - INFO - Fold 4 Val Epoch 33/200, Batch 10, Loss: 8.7032, Pearson: 0.5677, Spearman: 0.5724
2025-11-14 21:00:58,369 - INFO - Fold 4 Val Epoch 33/200, Batch 20, Loss: 6.0666, Pearson: 0.5658, Spearman: 0.5630
2025-11-14 21:01:01,211 - INFO - Fold 4 Val Epoch 33/200, Val Loss: 8.1211, Pearson Mean: 0.5572, Spearman Mean: 0.5573
2025-11-14 21:01:01,211 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2971, 'spearman_mean_genewise': 0.2729, 'l1_error_mean': 2.0199, 'l2_errors_mean': 8.1446, 'r2_scores_mean': 0.0943, 'pearson_std': 0.1253, 'l2_error_q1': 5.041, 'l2_error_q2': 7.6935, 'l2_error_q3': 11.1928, 'r2_score_q1': 0.0338, 'r2_score_q2': 0.071, 'r2_score_q3': 0.1245, 'mape_mean': 66.7773, 'mape_std': 19.4232, 'rmse_mean': 2.785, 'rmse_std': 0.6234}
2025-11-14 21:01:01,211 - INFO - Learning rate for epoch 33: 1.0000000000000002e-07
2025-11-14 21:01:01,211 - INFO - No improvement in spearman genewise. Patience: 18/30
2025-11-14 21:01:01,954 - INFO - Fold 4 Train Epoch 34/200, Batch 0, Loss: 6.9364, Pearson: 0.6566, Spearman: 0.6221
2025-11-14 21:01:09,100 - INFO - Fold 4 Train Epoch 34/200, Batch 10, Loss: 7.1224, Pearson: 0.6540, Spearman: 0.6128
2025-11-14 21:01:18,893 - INFO - Fold 4 Train Epoch 34/200, Batch 20, Loss: 7.0725, Pearson: 0.6509, Spearman: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6441
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6517
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.032954216003418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 34 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.883642 0.       0.       0.       0.       0.
 0.       7.883642]
Sample y_pred values (first sample, first 10 genes):
[1.4313359  1.1535712  3.0810952  0.09368905 3.1635652  4.033003
 1.1007302  1.876482   3.0278335  5.2139196 ]
y_true  -> mean=2.1500, std=3.4896, min=0.0000, max=12.9766
y_pred  -> mean=2.0878, std=2.2161, min=0.0000, max=13.3340
Batch 0 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6509
2025-11-14 21:01:29,035 - INFO - Fold 4 Train Epoch 34/200, Batch 30, Loss: 6.9619, Pearson: 0.6537, Spearman: 0.6144
2025-11-14 21:01:39,203 - INFO - Fold 4 Train Epoch 34/200, Batch 40, Loss: 6.9057, Pearson: 0.6522, Spearman: 0.6216
2025-11-14 21:01:49,374 - INFO - Fold 4 Train Epoch 34/200, Batch 50, Loss: 7.1824, Pearson: 0.6507, Spearman: 0.6135
2025-11-14 21:01:59,528 - INFO - Fold 4 Train Epoch 34/200, Batch 60, Loss: 7.0662, Pearson: 0.6546, Spearman: 0.6189
2025-11-14 21:02:09,679 - INFO - Fold 4 Train Epoch 34/200, Batch 70, Loss: 7.0910, Pearson: 0.6422, Spearman: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6518
2025-11-14 21:02:19,822 - INFO - Fold 4 Train Epoch 34/200, Batch 80, Loss: 7.0210, Pearson: 0.6573, Spearman: 0.6194
2025-11-14 21:02:22,918 - INFO - Fold 4 Train Epoch 34/200, Train Loss: 7.0348, Pearson Mean: 0.6499, Spearman Mean: 0.6120
2025-11-14 21:02:22,918 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4616, 'spearman_mean_genewise': 0.4157, 'l1_error_mean': 1.8538, 'l2_errors_mean': 7.0351, 'r2_scores_mean': 0.2254, 'pearson_std': 0.113, 'l2_error_q1': 4.8284, 'l2_error_q2': 6.7043, 'l2_error_q3': 8.9736, 'r2_score_q1': 0.1454, 'r2_score_q2': 0.1987, 'r2_score_q3': 0.273, 'mape_mean': 57.2116, 'mape_std': 18.0011, 'rmse_mean': 2.6094, 'rmse_std': 0.4756}
2025-11-14 21:02:23,229 - INFO - Fold 4 Val Epoch 34/200, Batch 0, Loss: 9.7633, Pearson: 0.5435, Spearman: 0.5503
2025-11-14 21:02:24,895 - INFO - Fold 4 Val Epoch 34/200, Batch 10, Loss: 8.7109, Pearson: 0.5667, Spearman: 0.5715
2025-11-14 21:02:26,582 - INFO - Fold 4 Val Epoch 34/200, Batch 20, Loss: 6.0725, Pearson: 0.5655, Spearman: 0.5632
2025-11-14 21:02:29,389 - INFO - Fold 4 Val Epoch 34/200, Val Loss: 8.1141, Pearson Mean: 0.5574, Spearman Mean: 0.5577
2025-11-14 21:02:29,390 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.298, 'spearman_mean_genewise': 0.2737, 'l1_error_mean': 2.0292, 'l2_errors_mean': 8.1367, 'r2_scores_mean': 0.0946, 'pearson_std': 0.1257, 'l2_error_q1': 5.0472, 'l2_error_q2': 7.6899, 'l2_error_q3': 11.198, 'r2_score_q1': 0.0334, 'r2_score_q2': 0.07, 'r2_score_q3': 0.124, 'mape_mean': 66.2875, 'mape_std': 19.3478, 'rmse_mean': 2.7839, 'rmse_std': 0.6219}
2025-11-14 21:02:29,390 - INFO - Learning rate for epoch 34: 1.0000000000000002e-07
2025-11-14 21:02:29,390 - INFO - No improvement in spearman genewise. Patience: 19/30
2025-11-14 21:02:30,122 - INFO - Fold 4 Train Epoch 35/200, Batch 0, Loss: 6.9517, Pearson: 0.6466, Spearman: 0.6065
2025-11-14 21:02:37,116 - INFO - Fold 4 Train Epoch 35/200, Batch 10, Loss: 7.1457, Pearson: 0.6427, Spearman: 0.6110
2025-11-14 21:02:47,052 - INFO - Fold 4 Train Epoch 35/200, Batch 20, Loss: 7.0264, Pearson: 0.6504, Spearman: 0.6086
2025-11-14 21:02:57,210 - INFO - Fold 4 Train Epoch 35/200, Batch 30, Loss: 6.8613, Pearson: 0.6553, Spearman: 0.6133
2025-11-14 21:03:07,384 - INFO - Fold 4 Train Epoch 35/200, Batch 40, Loss: 7.2247, Pearson: 0.6420, Spearman: 0.6121
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6501
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6583
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.0351057052612305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 35 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        6.566283  0.        6.566283
 0.        6.566283  7.2587266]
Sample y_pred values (first sample, first 10 genes):
[0.38991693 0.         1.1423842  0.         3.1016212  1.3972806
 0.60624534 0.9124367  3.5475583  7.9311724 ]
y_true  -> mean=2.0425, std=3.4559, min=0.0000, max=12.6253
y_pred  -> mean=2.0844, std=2.2324, min=0.0000, max=13.6252
Batch 0 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6580
2025-11-14 21:03:17,539 - INFO - Fold 4 Train Epoch 35/200, Batch 50, Loss: 7.0099, Pearson: 0.6416, Spearman: 0.6076
2025-11-14 21:03:27,706 - INFO - Fold 4 Train Epoch 35/200, Batch 60, Loss: 6.9293, Pearson: 0.6492, Spearman: 0.6100
2025-11-14 21:03:37,830 - INFO - Fold 4 Train Epoch 35/200, Batch 70, Loss: 6.9686, Pearson: 0.6523, Spearman: 0.6183
2025-11-14 21:03:48,004 - INFO - Fold 4 Train Epoch 35/200, Batch 80, Loss: 6.9913, Pearson: 0.6331, Spearman: 0.6030
2025-11-14 21:03:51,097 - INFO - Fold 4 Train Epoch 35/200, Train Loss: 7.0266, Pearson Mean: 0.6504, Spearman Mean: 0.6125
2025-11-14 21:03:51,097 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4624, 'spearman_mean_genewise': 0.4164, 'l1_error_mean': 1.8524, 'l2_errors_mean': 7.0264, 'r2_scores_mean': 0.2261, 'pearson_std': 0.1132, 'l2_error_q1': 4.8186, 'l2_error_q2': 6.6873, 'l2_error_q3': 8.9538, 'r2_score_q1': 0.1451, 'r2_score_q2': 0.1988, 'r2_score_q3': 0.2743, 'mape_mean': 57.1708, 'mape_std': 18.0047, 'rmse_mean': 2.6079, 'rmse_std': 0.4749}
2025-11-14 21:03:51,404 - INFO - Fold 4 Val Epoch 35/200, Batch 0, Loss: 9.7646, Pearson: 0.5434, Spearman: 0.5500
2025-11-14 21:03:53,095 - INFO - Fold 4 Val Epoch 35/200, Batch 10, Loss: 8.7032, Pearson: 0.5671, Spearman: 0.5717
2025-11-14 21:03:54,814 - INFO - Fold 4 Val Epoch 35/200, Batch 20, Loss: 6.0796, Pearson: 0.5659, Spearman: 0.5634
2025-11-14 21:03:57,576 - INFO - Fold 4 Val Epoch 35/200, Val Loss: 8.1223, Pearson Mean: 0.5572, Spearman Mean: 0.5573
2025-11-14 21:03:57,576 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2973, 'spearman_mean_genewise': 0.2731, 'l1_error_mean': 2.0346, 'l2_errors_mean': 8.1452, 'r2_scores_mean': 0.094, 'pearson_std': 0.1253, 'l2_error_q1': 5.0482, 'l2_error_q2': 7.68, 'l2_error_q3': 11.2071, 'r2_score_q1': 0.0329, 'r2_score_q2': 0.0703, 'r2_score_q3': 0.1231, 'mape_mean': 66.1212, 'mape_std': 19.4432, 'rmse_mean': 2.7852, 'rmse_std': 0.6228}
2025-11-14 21:03:57,576 - INFO - Learning rate for epoch 35: 1.0000000000000002e-07
2025-11-14 21:03:57,577 - INFO - No improvement in spearman genewise. Patience: 20/30
2025-11-14 21:03:58,322 - INFO - Fold 4 Train Epoch 36/200, Batch 0, Loss: 7.1840, Pearson: 0.6572, Spearman: 0.6236
2025-11-14 21:04:05,422 - INFO - Fold 4 Train Epoch 36/200, Batch 10, Loss: 6.9315, Pearson: 0.6540, Spearman: 0.6139
2025-11-14 21:04:15,397 - INFO - Fold 4 Train Epoch 36/200, Batch 20, Loss: 6.9599, Pearson: 0.6419, Spearman: 0.6089
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6390
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6501
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.026435375213623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 36 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        6.3329206 7.025179  7.025179  7.025179  0.
 0.        0.        8.896228 ]
Sample y_pred values (first sample, first 10 genes):
[1.1047711 3.5616238 2.764554  1.8006161 5.011004  4.188383  0.3508978
 2.282315  3.7076817 8.751671 ]
y_true  -> mean=2.2964, std=3.5440, min=0.0000, max=12.8889
y_pred  -> mean=2.1032, std=2.2202, min=0.0000, max=13.7875
Batch 0 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6419
2025-11-14 21:04:25,561 - INFO - Fold 4 Train Epoch 36/200, Batch 30, Loss: 6.9533, Pearson: 0.6547, Spearman: 0.6157
2025-11-14 21:04:35,693 - INFO - Fold 4 Train Epoch 36/200, Batch 40, Loss: 6.9309, Pearson: 0.6510, Spearman: 0.6111
2025-11-14 21:04:45,849 - INFO - Fold 4 Train Epoch 36/200, Batch 50, Loss: 7.1854, Pearson: 0.6499, Spearman: 0.6083
2025-11-14 21:04:56,018 - INFO - Fold 4 Train Epoch 36/200, Batch 60, Loss: 7.0200, Pearson: 0.6612, Spearman: 0.6177
2025-11-14 21:05:06,143 - INFO - Fold 4 Train Epoch 36/200, Batch 70, Loss: 7.1543, Pearson: 0.6461, Spearman: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6649
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6462
2025-11-14 21:05:16,302 - INFO - Fold 4 Train Epoch 36/200, Batch 80, Loss: 6.8502, Pearson: 0.6409, Spearman: 0.6056
2025-11-14 21:05:19,395 - INFO - Fold 4 Train Epoch 36/200, Train Loss: 7.0301, Pearson Mean: 0.6502, Spearman Mean: 0.6124
2025-11-14 21:05:19,395 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4621, 'spearman_mean_genewise': 0.4161, 'l1_error_mean': 1.8526, 'l2_errors_mean': 7.0299, 'r2_scores_mean': 0.2259, 'pearson_std': 0.1132, 'l2_error_q1': 4.8266, 'l2_error_q2': 6.6869, 'l2_error_q3': 8.9739, 'r2_score_q1': 0.1456, 'r2_score_q2': 0.1991, 'r2_score_q3': 0.2733, 'mape_mean': 57.1832, 'mape_std': 17.9948, 'rmse_mean': 2.6084, 'rmse_std': 0.4755}
2025-11-14 21:05:19,685 - INFO - Fold 4 Val Epoch 36/200, Batch 0, Loss: 9.7633, Pearson: 0.5434, Spearman: 0.5501
2025-11-14 21:05:21,394 - INFO - Fold 4 Val Epoch 36/200, Batch 10, Loss: 8.6833, Pearson: 0.5684, Spearman: 0.5730
2025-11-14 21:05:23,137 - INFO - Fold 4 Val Epoch 36/200, Batch 20, Loss: 6.1085, Pearson: 0.5630, Spearman: 0.5608
2025-11-14 21:05:25,913 - INFO - Fold 4 Val Epoch 36/200, Val Loss: 8.1270, Pearson Mean: 0.5563, Spearman Mean: 0.5566
2025-11-14 21:05:25,913 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.297, 'spearman_mean_genewise': 0.2729, 'l1_error_mean': 2.0383, 'l2_errors_mean': 8.1491, 'r2_scores_mean': 0.0934, 'pearson_std': 0.125, 'l2_error_q1': 5.043, 'l2_error_q2': 7.669, 'l2_error_q3': 11.2079, 'r2_score_q1': 0.0324, 'r2_score_q2': 0.0702, 'r2_score_q3': 0.1245, 'mape_mean': 66.0377, 'mape_std': 19.43, 'rmse_mean': 2.786, 'rmse_std': 0.6224}
2025-11-14 21:05:25,913 - INFO - Learning rate for epoch 36: 1.0000000000000002e-07
2025-11-14 21:05:25,913 - INFO - No improvement in spearman genewise. Patience: 21/30
2025-11-14 21:05:26,659 - INFO - Fold 4 Train Epoch 37/200, Batch 0, Loss: 7.2027, Pearson: 0.6579, Spearman: 0.6228
2025-11-14 21:05:33,828 - INFO - Fold 4 Train Epoch 37/200, Batch 10, Loss: 7.1817, Pearson: 0.6461, Spearman: 0.6109
2025-11-14 21:05:43,610 - INFO - Fold 4 Train Epoch 37/200, Batch 20, Loss: 6.9403, Pearson: 0.6582, Spearman: 0.6212
2025-11-14 21:05:53,789 - INFO - Fold 4 Train Epoch 37/200, Batch 30, Loss: 7.0102, Pearson: 0.6592, Spearman: 0.6195
2025-11-14 21:06:03,949 - INFO - Fold 4 Train Epoch 37/200, Batch 40, Loss: 6.8575, Pearson: 0.6555, Spearman: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6485
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6546
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.029914379119873
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 37 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       7.00848  7.00848  0.       0.
 8.394096 9.204901]
Sample y_pred values (first sample, first 10 genes):
[0.12143326 0.0146882  0.8093455  0.6000697  3.2197015  1.2258558
 0.4438575  1.0033059  4.4410806  7.3446484 ]
y_true  -> mean=2.3428, std=3.5448, min=0.0000, max=12.7220
y_pred  -> mean=2.0921, std=2.2156, min=0.0000, max=13.0657
Batch 0 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6708
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6449
2025-11-14 21:06:14,101 - INFO - Fold 4 Train Epoch 37/200, Batch 50, Loss: 7.1381, Pearson: 0.6521, Spearman: 0.6142
2025-11-14 21:06:24,263 - INFO - Fold 4 Train Epoch 37/200, Batch 60, Loss: 6.8648, Pearson: 0.6527, Spearman: 0.6093
2025-11-14 21:06:34,445 - INFO - Fold 4 Train Epoch 37/200, Batch 70, Loss: 7.0272, Pearson: 0.6556, Spearman: 0.6164
2025-11-14 21:06:44,549 - INFO - Fold 4 Train Epoch 37/200, Batch 80, Loss: 7.1329, Pearson: 0.6445, Spearman: 0.6078
2025-11-14 21:06:47,655 - INFO - Fold 4 Train Epoch 37/200, Train Loss: 7.0237, Pearson Mean: 0.6505, Spearman Mean: 0.6126
2025-11-14 21:06:47,655 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4626, 'spearman_mean_genewise': 0.4167, 'l1_error_mean': 1.8517, 'l2_errors_mean': 7.0236, 'r2_scores_mean': 0.2264, 'pearson_std': 0.1133, 'l2_error_q1': 4.8303, 'l2_error_q2': 6.6844, 'l2_error_q3': 8.9624, 'r2_score_q1': 0.1454, 'r2_score_q2': 0.1989, 'r2_score_q3': 0.2742, 'mape_mean': 57.1511, 'mape_std': 18.0097, 'rmse_mean': 2.6073, 'rmse_std': 0.4749}
2025-11-14 21:06:47,971 - INFO - Fold 4 Val Epoch 37/200, Batch 0, Loss: 9.7612, Pearson: 0.5436, Spearman: 0.5503
2025-11-14 21:06:49,669 - INFO - Fold 4 Val Epoch 37/200, Batch 10, Loss: 8.6903, Pearson: 0.5680, Spearman: 0.5727
2025-11-14 21:06:51,425 - INFO - Fold 4 Val Epoch 37/200, Batch 20, Loss: 6.0834, Pearson: 0.5649, Spearman: 0.5625
2025-11-14 21:06:54,238 - INFO - Fold 4 Val Epoch 37/200, Val Loss: 8.1156, Pearson Mean: 0.5574, Spearman Mean: 0.5576
2025-11-14 21:06:54,239 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2978, 'spearman_mean_genewise': 0.2734, 'l1_error_mean': 2.0309, 'l2_errors_mean': 8.1382, 'r2_scores_mean': 0.0947, 'pearson_std': 0.1254, 'l2_error_q1': 5.042, 'l2_error_q2': 7.6764, 'l2_error_q3': 11.1925, 'r2_score_q1': 0.0334, 'r2_score_q2': 0.0712, 'r2_score_q3': 0.1245, 'mape_mean': 66.2764, 'mape_std': 19.3992, 'rmse_mean': 2.784, 'rmse_std': 0.6225}
2025-11-14 21:06:54,239 - INFO - Learning rate for epoch 37: 1.0000000000000004e-08
2025-11-14 21:06:54,239 - INFO - No improvement in spearman genewise. Patience: 22/30
2025-11-14 21:06:54,971 - INFO - Fold 4 Train Epoch 38/200, Batch 0, Loss: 6.9668, Pearson: 0.6417, Spearman: 0.6044
2025-11-14 21:07:02,116 - INFO - Fold 4 Train Epoch 38/200, Batch 10, Loss: 7.0584, Pearson: 0.6491, Spearman: 0.6129
2025-11-14 21:07:11,959 - INFO - Fold 4 Train Epoch 38/200, Batch 20, Loss: 6.9543, Pearson: 0.6517, Spearman: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6482
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6429
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.023550510406494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 38 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       7.36252  0.       0.
 0.       9.307886]
Sample y_pred values (first sample, first 10 genes):
[0.51053876 2.8817525  1.9775329  0.978567   4.739511   3.3672643
 0.         1.6671205  3.1968045  8.858892  ]
y_true  -> mean=1.9807, std=3.4375, min=0.0000, max=12.9888
y_pred  -> mean=2.0958, std=2.2535, min=0.0000, max=13.0369
Batch 0 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6517
2025-11-14 21:07:22,112 - INFO - Fold 4 Train Epoch 38/200, Batch 30, Loss: 7.0756, Pearson: 0.6552, Spearman: 0.6130
2025-11-14 21:07:32,299 - INFO - Fold 4 Train Epoch 38/200, Batch 40, Loss: 7.1150, Pearson: 0.6502, Spearman: 0.6121
2025-11-14 21:07:42,424 - INFO - Fold 4 Train Epoch 38/200, Batch 50, Loss: 6.9715, Pearson: 0.6426, Spearman: 0.6057
2025-11-14 21:07:52,556 - INFO - Fold 4 Train Epoch 38/200, Batch 60, Loss: 7.1504, Pearson: 0.6513, Spearman: 0.6115
2025-11-14 21:08:02,705 - INFO - Fold 4 Train Epoch 38/200, Batch 70, Loss: 7.0747, Pearson: 0.6464, Spearman: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6552
2025-11-14 21:08:12,874 - INFO - Fold 4 Train Epoch 38/200, Batch 80, Loss: 6.9858, Pearson: 0.6461, Spearman: 0.6079
2025-11-14 21:08:15,963 - INFO - Fold 4 Train Epoch 38/200, Train Loss: 7.0299, Pearson Mean: 0.6502, Spearman Mean: 0.6124
2025-11-14 21:08:15,963 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4621, 'spearman_mean_genewise': 0.4163, 'l1_error_mean': 1.8529, 'l2_errors_mean': 7.0302, 'r2_scores_mean': 0.2259, 'pearson_std': 0.113, 'l2_error_q1': 4.8219, 'l2_error_q2': 6.6828, 'l2_error_q3': 8.954, 'r2_score_q1': 0.1451, 'r2_score_q2': 0.1987, 'r2_score_q3': 0.2736, 'mape_mean': 57.1782, 'mape_std': 17.9943, 'rmse_mean': 2.6085, 'rmse_std': 0.4755}
2025-11-14 21:08:16,294 - INFO - Fold 4 Val Epoch 38/200, Batch 0, Loss: 9.7708, Pearson: 0.5430, Spearman: 0.5498
2025-11-14 21:08:18,004 - INFO - Fold 4 Val Epoch 38/200, Batch 10, Loss: 8.6892, Pearson: 0.5680, Spearman: 0.5727
2025-11-14 21:08:19,720 - INFO - Fold 4 Val Epoch 38/200, Batch 20, Loss: 6.0846, Pearson: 0.5648, Spearman: 0.5625
2025-11-14 21:08:22,514 - INFO - Fold 4 Val Epoch 38/200, Val Loss: 8.1258, Pearson Mean: 0.5564, Spearman Mean: 0.5567
2025-11-14 21:08:22,515 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2973, 'spearman_mean_genewise': 0.2736, 'l1_error_mean': 2.0338, 'l2_errors_mean': 8.1484, 'r2_scores_mean': 0.0933, 'pearson_std': 0.1251, 'l2_error_q1': 5.0486, 'l2_error_q2': 7.6706, 'l2_error_q3': 11.2151, 'r2_score_q1': 0.0325, 'r2_score_q2': 0.0707, 'r2_score_q3': 0.1241, 'mape_mean': 66.1408, 'mape_std': 19.3704, 'rmse_mean': 2.786, 'rmse_std': 0.6219}
2025-11-14 21:08:22,515 - INFO - Learning rate for epoch 38: 1.0000000000000004e-08
2025-11-14 21:08:22,515 - INFO - No improvement in spearman genewise. Patience: 23/30
2025-11-14 21:08:23,248 - INFO - Fold 4 Train Epoch 39/200, Batch 0, Loss: 6.8448, Pearson: 0.6512, Spearman: 0.6136
2025-11-14 21:08:30,344 - INFO - Fold 4 Train Epoch 39/200, Batch 10, Loss: 7.0348, Pearson: 0.6449, Spearman: 0.6121
2025-11-14 21:08:40,136 - INFO - Fold 4 Train Epoch 39/200, Batch 20, Loss: 7.1065, Pearson: 0.6491, Spearman: 0.6069
2025-11-14 21:08:50,286 - INFO - Fold 4 Train Epoch 39/200, Batch 30, Loss: 6.9468, Pearson: 0.6445, Spearman: 0.6111
2025-11-14 21:09:00,417 - INFO - Fold 4 Train Epoch 39/200, Batch 40, Loss: 6.9442, Pearson: 0.6496, Spearman: 0.6074
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6467
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6484
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.030160427093506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 39 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        8.703689  0.        7.4513407
 6.758774  0.        8.703689 ]
Sample y_pred values (first sample, first 10 genes):
[0.92824894 2.6481822  2.7180622  1.9543729  6.7632895  3.9700143
 1.0491916  2.9726632  4.5648546  8.807582  ]
y_true  -> mean=2.0009, std=3.4453, min=0.0000, max=12.7969
y_pred  -> mean=2.0855, std=2.2186, min=0.0000, max=13.6027
Batch 0 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6499
2025-11-14 21:09:10,576 - INFO - Fold 4 Train Epoch 39/200, Batch 50, Loss: 7.0679, Pearson: 0.6465, Spearman: 0.6152
2025-11-14 21:09:20,757 - INFO - Fold 4 Train Epoch 39/200, Batch 60, Loss: 7.1097, Pearson: 0.6597, Spearman: 0.6103
2025-11-14 21:09:30,912 - INFO - Fold 4 Train Epoch 39/200, Batch 70, Loss: 6.9710, Pearson: 0.6469, Spearman: 0.6084
2025-11-14 21:09:41,054 - INFO - Fold 4 Train Epoch 39/200, Batch 80, Loss: 7.1556, Pearson: 0.6513, Spearman: 0.6070
2025-11-14 21:09:44,148 - INFO - Fold 4 Train Epoch 39/200, Train Loss: 7.0353, Pearson Mean: 0.6499, Spearman Mean: 0.6120
2025-11-14 21:09:44,148 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4617, 'spearman_mean_genewise': 0.4157, 'l1_error_mean': 1.8538, 'l2_errors_mean': 7.0349, 'r2_scores_mean': 0.2255, 'pearson_std': 0.113, 'l2_error_q1': 4.8412, 'l2_error_q2': 6.6943, 'l2_error_q3': 8.9675, 'r2_score_q1': 0.1454, 'r2_score_q2': 0.1991, 'r2_score_q3': 0.274, 'mape_mean': 57.2095, 'mape_std': 17.9799, 'rmse_mean': 2.6093, 'rmse_std': 0.4758}
2025-11-14 21:09:44,508 - INFO - Fold 4 Val Epoch 39/200, Batch 0, Loss: 9.7538, Pearson: 0.5441, Spearman: 0.5508
2025-11-14 21:09:46,215 - INFO - Fold 4 Val Epoch 39/200, Batch 10, Loss: 8.7022, Pearson: 0.5673, Spearman: 0.5720
2025-11-14 21:09:47,926 - INFO - Fold 4 Val Epoch 39/200, Batch 20, Loss: 6.0773, Pearson: 0.5654, Spearman: 0.5630
2025-11-14 21:09:50,742 - INFO - Fold 4 Val Epoch 39/200, Val Loss: 8.1277, Pearson Mean: 0.5563, Spearman Mean: 0.5565
2025-11-14 21:09:50,742 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2968, 'spearman_mean_genewise': 0.2731, 'l1_error_mean': 2.0266, 'l2_errors_mean': 8.1508, 'r2_scores_mean': 0.0932, 'pearson_std': 0.1252, 'l2_error_q1': 5.0417, 'l2_error_q2': 7.6774, 'l2_error_q3': 11.1773, 'r2_score_q1': 0.0332, 'r2_score_q2': 0.0708, 'r2_score_q3': 0.1246, 'mape_mean': 66.37, 'mape_std': 19.4465, 'rmse_mean': 2.7862, 'rmse_std': 0.6227}
2025-11-14 21:09:50,742 - INFO - Learning rate for epoch 39: 1.0000000000000004e-08
2025-11-14 21:09:50,742 - INFO - No improvement in spearman genewise. Patience: 24/30
2025-11-14 21:09:51,492 - INFO - Fold 4 Train Epoch 40/200, Batch 0, Loss: 7.1634, Pearson: 0.6494, Spearman: 0.6095
2025-11-14 21:09:58,661 - INFO - Fold 4 Train Epoch 40/200, Batch 10, Loss: 7.1248, Pearson: 0.6541, Spearman: 0.6134
2025-11-14 21:10:08,437 - INFO - Fold 4 Train Epoch 40/200, Batch 20, Loss: 7.2790, Pearson: 0.6529, Spearman: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6434
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6423
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.034908294677734
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 40 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       8.221068 0.       0.       0.
 0.       8.914081]
Sample y_pred values (first sample, first 10 genes):
[0.06080731 0.         0.42139828 0.41400993 1.586344   0.8610094
 0.38504994 0.55018365 1.2874639  7.1458373 ]
y_true  -> mean=2.1753, std=3.5180, min=0.0000, max=12.5696
y_pred  -> mean=2.0988, std=2.2533, min=0.0000, max=13.5329
Batch 0 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6529
2025-11-14 21:10:18,596 - INFO - Fold 4 Train Epoch 40/200, Batch 30, Loss: 7.0566, Pearson: 0.6494, Spearman: 0.6088
2025-11-14 21:10:28,758 - INFO - Fold 4 Train Epoch 40/200, Batch 40, Loss: 7.1669, Pearson: 0.6541, Spearman: 0.6134
2025-11-14 21:10:38,939 - INFO - Fold 4 Train Epoch 40/200, Batch 50, Loss: 6.9191, Pearson: 0.6550, Spearman: 0.6164
2025-11-14 21:10:49,070 - INFO - Fold 4 Train Epoch 40/200, Batch 60, Loss: 7.1115, Pearson: 0.6467, Spearman: 0.6071
2025-11-14 21:10:59,250 - INFO - Fold 4 Train Epoch 40/200, Batch 70, Loss: 7.2910, Pearson: 0.6449, Spearman: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6458
2025-11-14 21:11:09,403 - INFO - Fold 4 Train Epoch 40/200, Batch 80, Loss: 7.1008, Pearson: 0.6533, Spearman: 0.6132
2025-11-14 21:11:12,472 - INFO - Fold 4 Train Epoch 40/200, Train Loss: 7.0280, Pearson Mean: 0.6503, Spearman Mean: 0.6124
2025-11-14 21:11:12,472 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4622, 'spearman_mean_genewise': 0.4163, 'l1_error_mean': 1.8528, 'l2_errors_mean': 7.0284, 'r2_scores_mean': 0.226, 'pearson_std': 0.1132, 'l2_error_q1': 4.8328, 'l2_error_q2': 6.6841, 'l2_error_q3': 8.9563, 'r2_score_q1': 0.1458, 'r2_score_q2': 0.199, 'r2_score_q3': 0.274, 'mape_mean': 57.174, 'mape_std': 18.0013, 'rmse_mean': 2.6081, 'rmse_std': 0.4755}
2025-11-14 21:11:12,790 - INFO - Fold 4 Val Epoch 40/200, Batch 0, Loss: 9.7715, Pearson: 0.5432, Spearman: 0.5499
2025-11-14 21:11:14,477 - INFO - Fold 4 Val Epoch 40/200, Batch 10, Loss: 8.6699, Pearson: 0.5694, Spearman: 0.5740
2025-11-14 21:11:16,181 - INFO - Fold 4 Val Epoch 40/200, Batch 20, Loss: 6.1004, Pearson: 0.5631, Spearman: 0.5608
2025-11-14 21:11:18,944 - INFO - Fold 4 Val Epoch 40/200, Val Loss: 8.1261, Pearson Mean: 0.5564, Spearman Mean: 0.5567
2025-11-14 21:11:18,945 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2972, 'spearman_mean_genewise': 0.2732, 'l1_error_mean': 2.0348, 'l2_errors_mean': 8.1481, 'r2_scores_mean': 0.0936, 'pearson_std': 0.125, 'l2_error_q1': 5.0475, 'l2_error_q2': 7.6677, 'l2_error_q3': 11.2035, 'r2_score_q1': 0.0326, 'r2_score_q2': 0.0708, 'r2_score_q3': 0.1241, 'mape_mean': 66.2172, 'mape_std': 19.3655, 'rmse_mean': 2.7858, 'rmse_std': 0.6224}
2025-11-14 21:11:18,945 - INFO - Learning rate for epoch 40: 1.0000000000000004e-08
2025-11-14 21:11:18,945 - INFO - No improvement in spearman genewise. Patience: 25/30
2025-11-14 21:11:19,699 - INFO - Fold 4 Train Epoch 41/200, Batch 0, Loss: 7.0870, Pearson: 0.6450, Spearman: 0.6067
2025-11-14 21:11:26,853 - INFO - Fold 4 Train Epoch 41/200, Batch 10, Loss: 7.0150, Pearson: 0.6576, Spearman: 0.6191
2025-11-14 21:11:36,637 - INFO - Fold 4 Train Epoch 41/200, Batch 20, Loss: 6.8750, Pearson: 0.6578, Spearman: 0.6174
2025-11-14 21:11:46,808 - INFO - Fold 4 Train Epoch 41/200, Batch 30, Loss: 7.0006, Pearson: 0.6520, Spearman: 0.6154
2025-11-14 21:11:56,978 - INFO - Fold 4 Train Epoch 41/200, Batch 40, Loss: 7.0357, Pearson: 0.6582, Spearman: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6464
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6600
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.028350353240967
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 41 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        8.042834  0.        0.        0.        0.
 0.        7.3500085 9.1412325]
Sample y_pred values (first sample, first 10 genes):
[0.4566606 4.7107487 2.9658842 1.2903693 4.5754833 2.2562346 0.6470196
 2.1001503 3.538146  9.595442 ]
y_true  -> mean=2.0817, std=3.4838, min=0.0000, max=12.9120
y_pred  -> mean=2.0889, std=2.2380, min=0.0000, max=13.5927
Batch 0 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6444
2025-11-14 21:12:07,167 - INFO - Fold 4 Train Epoch 41/200, Batch 50, Loss: 7.0056, Pearson: 0.6538, Spearman: 0.6183
2025-11-14 21:12:17,324 - INFO - Fold 4 Train Epoch 41/200, Batch 60, Loss: 7.1612, Pearson: 0.6320, Spearman: 0.5968
2025-11-14 21:12:27,501 - INFO - Fold 4 Train Epoch 41/200, Batch 70, Loss: 6.9917, Pearson: 0.6543, Spearman: 0.6159
2025-11-14 21:12:37,658 - INFO - Fold 4 Train Epoch 41/200, Batch 80, Loss: 6.7285, Pearson: 0.6687, Spearman: 0.6237
2025-11-14 21:12:40,748 - INFO - Fold 4 Train Epoch 41/200, Train Loss: 7.0275, Pearson Mean: 0.6504, Spearman Mean: 0.6125
2025-11-14 21:12:40,748 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4623, 'spearman_mean_genewise': 0.4164, 'l1_error_mean': 1.8525, 'l2_errors_mean': 7.0274, 'r2_scores_mean': 0.2261, 'pearson_std': 0.1132, 'l2_error_q1': 4.8306, 'l2_error_q2': 6.6854, 'l2_error_q3': 8.9545, 'r2_score_q1': 0.1462, 'r2_score_q2': 0.199, 'r2_score_q3': 0.2742, 'mape_mean': 57.1821, 'mape_std': 18.0021, 'rmse_mean': 2.608, 'rmse_std': 0.4752}
2025-11-14 21:12:41,111 - INFO - Fold 4 Val Epoch 41/200, Batch 0, Loss: 9.7642, Pearson: 0.5435, Spearman: 0.5501
2025-11-14 21:12:42,860 - INFO - Fold 4 Val Epoch 41/200, Batch 10, Loss: 8.6862, Pearson: 0.5688, Spearman: 0.5733
2025-11-14 21:12:44,530 - INFO - Fold 4 Val Epoch 41/200, Batch 20, Loss: 6.0741, Pearson: 0.5652, Spearman: 0.5627
2025-11-14 21:12:47,354 - INFO - Fold 4 Val Epoch 41/200, Val Loss: 8.1089, Pearson Mean: 0.5580, Spearman Mean: 0.5581
2025-11-14 21:12:47,355 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2982, 'spearman_mean_genewise': 0.2735, 'l1_error_mean': 2.0232, 'l2_errors_mean': 8.1316, 'r2_scores_mean': 0.0955, 'pearson_std': 0.1257, 'l2_error_q1': 5.0423, 'l2_error_q2': 7.6778, 'l2_error_q3': 11.2104, 'r2_score_q1': 0.0339, 'r2_score_q2': 0.071, 'r2_score_q3': 0.125, 'mape_mean': 66.5895, 'mape_std': 19.4107, 'rmse_mean': 2.7828, 'rmse_std': 0.6227}
2025-11-14 21:12:47,355 - INFO - Learning rate for epoch 41: 1.0000000000000004e-08
2025-11-14 21:12:47,355 - INFO - No improvement in spearman genewise. Patience: 26/30
2025-11-14 21:12:48,084 - INFO - Fold 4 Train Epoch 42/200, Batch 0, Loss: 7.0034, Pearson: 0.6512, Spearman: 0.6118
2025-11-14 21:12:55,265 - INFO - Fold 4 Train Epoch 42/200, Batch 10, Loss: 6.8282, Pearson: 0.6562, Spearman: 0.6106
2025-11-14 21:13:05,088 - INFO - Fold 4 Train Epoch 42/200, Batch 20, Loss: 6.8929, Pearson: 0.6550, Spearman: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6687
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6448
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6500
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.027416229248047
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 42 =====================
Sample y_true values (first sample, first 10 genes):
[0.        7.4258657 0.        7.4258657 0.        0.        0.
 0.        0.        8.524081 ]
Sample y_pred values (first sample, first 10 genes):
[0.33247626 2.8127935  3.8967974  1.127712   5.4412313  3.8342066
 1.380533   2.8587406  4.456637   7.3953953 ]
y_true  -> mean=2.0805, std=3.4869, min=0.0000, max=12.6870
y_pred  -> mean=2.0921, std=2.2435, min=0.0000, max=12.8319
Batch 0 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6550
2025-11-14 21:13:15,235 - INFO - Fold 4 Train Epoch 42/200, Batch 30, Loss: 7.0405, Pearson: 0.6552, Spearman: 0.6126
2025-11-14 21:13:25,422 - INFO - Fold 4 Train Epoch 42/200, Batch 40, Loss: 6.9510, Pearson: 0.6520, Spearman: 0.6191
2025-11-14 21:13:35,598 - INFO - Fold 4 Train Epoch 42/200, Batch 50, Loss: 6.6879, Pearson: 0.6545, Spearman: 0.6112
2025-11-14 21:13:45,769 - INFO - Fold 4 Train Epoch 42/200, Batch 60, Loss: 6.9967, Pearson: 0.6398, Spearman: 0.6030
2025-11-14 21:13:55,906 - INFO - Fold 4 Train Epoch 42/200, Batch 70, Loss: 6.8071, Pearson: 0.6537, Spearman: 0.6169
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6549
2025-11-14 21:14:06,053 - INFO - Fold 4 Train Epoch 42/200, Batch 80, Loss: 6.9302, Pearson: 0.6532, Spearman: 0.6124
2025-11-14 21:14:09,171 - INFO - Fold 4 Train Epoch 42/200, Train Loss: 7.0310, Pearson Mean: 0.6501, Spearman Mean: 0.6122
2025-11-14 21:14:09,171 - INFO - Training Metrics: {'pearson_mean_genewise': 0.462, 'spearman_mean_genewise': 0.4161, 'l1_error_mean': 1.853, 'l2_errors_mean': 7.0313, 'r2_scores_mean': 0.2258, 'pearson_std': 0.113, 'l2_error_q1': 4.8309, 'l2_error_q2': 6.6965, 'l2_error_q3': 8.9556, 'r2_score_q1': 0.1454, 'r2_score_q2': 0.1982, 'r2_score_q3': 0.2735, 'mape_mean': 57.1806, 'mape_std': 17.9841, 'rmse_mean': 2.6087, 'rmse_std': 0.4756}
2025-11-14 21:14:09,516 - INFO - Fold 4 Val Epoch 42/200, Batch 0, Loss: 9.7610, Pearson: 0.5436, Spearman: 0.5503
2025-11-14 21:14:11,293 - INFO - Fold 4 Val Epoch 42/200, Batch 10, Loss: 8.6731, Pearson: 0.5692, Spearman: 0.5737
2025-11-14 21:14:12,977 - INFO - Fold 4 Val Epoch 42/200, Batch 20, Loss: 6.1041, Pearson: 0.5634, Spearman: 0.5610
2025-11-14 21:14:15,784 - INFO - Fold 4 Val Epoch 42/200, Val Loss: 8.1294, Pearson Mean: 0.5561, Spearman Mean: 0.5563
2025-11-14 21:14:15,785 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2969, 'spearman_mean_genewise': 0.273, 'l1_error_mean': 2.0346, 'l2_errors_mean': 8.1517, 'r2_scores_mean': 0.093, 'pearson_std': 0.125, 'l2_error_q1': 5.0452, 'l2_error_q2': 7.6603, 'l2_error_q3': 11.2156, 'r2_score_q1': 0.0328, 'r2_score_q2': 0.0704, 'r2_score_q3': 0.1243, 'mape_mean': 66.1251, 'mape_std': 19.4531, 'rmse_mean': 2.7865, 'rmse_std': 0.6223}
2025-11-14 21:14:15,785 - INFO - Learning rate for epoch 42: 1.0000000000000004e-08
2025-11-14 21:14:15,785 - INFO - No improvement in spearman genewise. Patience: 27/30
2025-11-14 21:14:16,512 - INFO - Fold 4 Train Epoch 43/200, Batch 0, Loss: 7.2643, Pearson: 0.6410, Spearman: 0.6072
2025-11-14 21:14:23,637 - INFO - Fold 4 Train Epoch 43/200, Batch 10, Loss: 7.1095, Pearson: 0.6507, Spearman: 0.6177
2025-11-14 21:14:33,368 - INFO - Fold 4 Train Epoch 43/200, Batch 20, Loss: 7.1373, Pearson: 0.6534, Spearman: 0.6108
2025-11-14 21:14:43,528 - INFO - Fold 4 Train Epoch 43/200, Batch 30, Loss: 7.0334, Pearson: 0.6473, Spearman: 0.6080
2025-11-14 21:14:53,685 - INFO - Fold 4 Train Epoch 43/200, Batch 40, Loss: 6.8897, Pearson: 0.6574, Spearman: 0.6153
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6497
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6414
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.031271457672119
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 43 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.339188 7.339188 7.339188 0.       7.339188 0.
 0.       8.724995]
Sample y_pred values (first sample, first 10 genes):
[ 0.35890418  2.107895    1.6550419   1.0049974   3.1015768   1.7766855
  0.3820633   1.8974369   3.2145793  10.157337  ]
y_true  -> mean=2.1353, std=3.5108, min=0.0000, max=12.6870
y_pred  -> mean=2.0898, std=2.2127, min=0.0000, max=12.8580
Batch 0 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6392
2025-11-14 21:15:03,824 - INFO - Fold 4 Train Epoch 43/200, Batch 50, Loss: 6.9364, Pearson: 0.6502, Spearman: 0.6161
2025-11-14 21:15:13,967 - INFO - Fold 4 Train Epoch 43/200, Batch 60, Loss: 6.9534, Pearson: 0.6508, Spearman: 0.6061
2025-11-14 21:15:24,122 - INFO - Fold 4 Train Epoch 43/200, Batch 70, Loss: 6.9784, Pearson: 0.6535, Spearman: 0.6130
2025-11-14 21:15:34,281 - INFO - Fold 4 Train Epoch 43/200, Batch 80, Loss: 6.9902, Pearson: 0.6558, Spearman: 0.6211
2025-11-14 21:15:37,383 - INFO - Fold 4 Train Epoch 43/200, Train Loss: 7.0304, Pearson Mean: 0.6502, Spearman Mean: 0.6123
2025-11-14 21:15:37,384 - INFO - Training Metrics: {'pearson_mean_genewise': 0.462, 'spearman_mean_genewise': 0.4161, 'l1_error_mean': 1.8532, 'l2_errors_mean': 7.0308, 'r2_scores_mean': 0.2257, 'pearson_std': 0.1131, 'l2_error_q1': 4.827, 'l2_error_q2': 6.6874, 'l2_error_q3': 8.9565, 'r2_score_q1': 0.1444, 'r2_score_q2': 0.1986, 'r2_score_q3': 0.2728, 'mape_mean': 57.1996, 'mape_std': 18.0032, 'rmse_mean': 2.6086, 'rmse_std': 0.4752}
2025-11-14 21:15:37,749 - INFO - Fold 4 Val Epoch 43/200, Batch 0, Loss: 9.7577, Pearson: 0.5439, Spearman: 0.5505
2025-11-14 21:15:39,484 - INFO - Fold 4 Val Epoch 43/200, Batch 10, Loss: 8.6959, Pearson: 0.5679, Spearman: 0.5725
2025-11-14 21:15:41,179 - INFO - Fold 4 Val Epoch 43/200, Batch 20, Loss: 6.0841, Pearson: 0.5645, Spearman: 0.5620
2025-11-14 21:15:44,016 - INFO - Fold 4 Val Epoch 43/200, Val Loss: 8.1177, Pearson Mean: 0.5574, Spearman Mean: 0.5574
2025-11-14 21:15:44,017 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2974, 'spearman_mean_genewise': 0.2731, 'l1_error_mean': 2.0247, 'l2_errors_mean': 8.1404, 'r2_scores_mean': 0.0945, 'pearson_std': 0.1254, 'l2_error_q1': 5.0417, 'l2_error_q2': 7.674, 'l2_error_q3': 11.2087, 'r2_score_q1': 0.0338, 'r2_score_q2': 0.0711, 'r2_score_q3': 0.1246, 'mape_mean': 66.5334, 'mape_std': 19.4839, 'rmse_mean': 2.7844, 'rmse_std': 0.6226}
2025-11-14 21:15:44,017 - INFO - Learning rate for epoch 43: 1.0000000000000004e-08
2025-11-14 21:15:44,017 - INFO - No improvement in spearman genewise. Patience: 28/30
2025-11-14 21:15:44,743 - INFO - Fold 4 Train Epoch 44/200, Batch 0, Loss: 6.9387, Pearson: 0.6526, Spearman: 0.6145
2025-11-14 21:15:51,856 - INFO - Fold 4 Train Epoch 44/200, Batch 10, Loss: 6.8826, Pearson: 0.6545, Spearman: 0.6124
2025-11-14 21:16:01,603 - INFO - Fold 4 Train Epoch 44/200, Batch 20, Loss: 6.8075, Pearson: 0.6537, Spearman: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6556
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6436
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.030815601348877
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 44 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       7.611448 0.       7.611448 0.       0.       0.
 7.611448 8.304347]
Sample y_pred values (first sample, first 10 genes):
[1.0103676 3.8443308 2.4659202 1.2887723 4.0404634 2.6094842 0.7248272
 1.6948206 3.376464  8.836808 ]
y_true  -> mean=2.1174, std=3.4760, min=0.0000, max=13.8155
y_pred  -> mean=2.0982, std=2.2294, min=0.0000, max=12.8996
Batch 0 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6537
2025-11-14 21:16:11,771 - INFO - Fold 4 Train Epoch 44/200, Batch 30, Loss: 6.9766, Pearson: 0.6464, Spearman: 0.6147
2025-11-14 21:16:21,924 - INFO - Fold 4 Train Epoch 44/200, Batch 40, Loss: 6.8851, Pearson: 0.6563, Spearman: 0.6138
2025-11-14 21:16:32,080 - INFO - Fold 4 Train Epoch 44/200, Batch 50, Loss: 7.1455, Pearson: 0.6437, Spearman: 0.6104
2025-11-14 21:16:42,266 - INFO - Fold 4 Train Epoch 44/200, Batch 60, Loss: 7.0773, Pearson: 0.6433, Spearman: 0.6089
2025-11-14 21:16:52,395 - INFO - Fold 4 Train Epoch 44/200, Batch 70, Loss: 6.9476, Pearson: 0.6382, Spearman: 0.6072
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6547
2025-11-14 21:17:02,536 - INFO - Fold 4 Train Epoch 44/200, Batch 80, Loss: 7.2108, Pearson: 0.6523, Spearman: 0.6173
2025-11-14 21:17:05,593 - INFO - Fold 4 Train Epoch 44/200, Train Loss: 7.0321, Pearson Mean: 0.6501, Spearman Mean: 0.6122
2025-11-14 21:17:05,593 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4619, 'spearman_mean_genewise': 0.416, 'l1_error_mean': 1.8535, 'l2_errors_mean': 7.032, 'r2_scores_mean': 0.2256, 'pearson_std': 0.1131, 'l2_error_q1': 4.8298, 'l2_error_q2': 6.6872, 'l2_error_q3': 8.9607, 'r2_score_q1': 0.145, 'r2_score_q2': 0.1986, 'r2_score_q3': 0.2728, 'mape_mean': 57.2087, 'mape_std': 18.0034, 'rmse_mean': 2.6088, 'rmse_std': 0.4753}
2025-11-14 21:17:05,905 - INFO - Fold 4 Val Epoch 44/200, Batch 0, Loss: 9.7540, Pearson: 0.5440, Spearman: 0.5506
2025-11-14 21:17:07,614 - INFO - Fold 4 Val Epoch 44/200, Batch 10, Loss: 8.7190, Pearson: 0.5671, Spearman: 0.5718
2025-11-14 21:17:09,353 - INFO - Fold 4 Val Epoch 44/200, Batch 20, Loss: 6.0584, Pearson: 0.5658, Spearman: 0.5631
2025-11-14 21:17:12,142 - INFO - Fold 4 Val Epoch 44/200, Val Loss: 8.1202, Pearson Mean: 0.5572, Spearman Mean: 0.5573
2025-11-14 21:17:12,143 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2972, 'spearman_mean_genewise': 0.273, 'l1_error_mean': 2.0146, 'l2_errors_mean': 8.1435, 'r2_scores_mean': 0.0942, 'pearson_std': 0.1256, 'l2_error_q1': 5.0384, 'l2_error_q2': 7.6801, 'l2_error_q3': 11.2012, 'r2_score_q1': 0.0336, 'r2_score_q2': 0.0707, 'r2_score_q3': 0.1245, 'mape_mean': 67.0436, 'mape_std': 19.3263, 'rmse_mean': 2.7849, 'rmse_std': 0.6229}
2025-11-14 21:17:12,143 - INFO - Learning rate for epoch 44: 1.0000000000000004e-08
2025-11-14 21:17:12,143 - INFO - No improvement in spearman genewise. Patience: 29/30
2025-11-14 21:17:12,880 - INFO - Fold 4 Train Epoch 45/200, Batch 0, Loss: 6.8239, Pearson: 0.6563, Spearman: 0.6131
2025-11-14 21:17:20,006 - INFO - Fold 4 Train Epoch 45/200, Batch 10, Loss: 7.3677, Pearson: 0.6521, Spearman: 0.6169
2025-11-14 21:17:29,857 - INFO - Fold 4 Train Epoch 45/200, Batch 20, Loss: 7.2625, Pearson: 0.6465, Spearman: 0.6112
2025-11-14 21:17:40,005 - INFO - Fold 4 Train Epoch 45/200, Batch 30, Loss: 7.2262, Pearson: 0.6409, Spearman: 0.6092
2025-11-14 21:17:50,151 - INFO - Fold 4 Train Epoch 45/200, Batch 40, Loss: 6.8759, Pearson: 0.6558, Spearman: 0.6145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6535
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6491
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.031980991363525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 45 =====================
Sample y_true values (first sample, first 10 genes):
[7.8120284 0.        0.        0.        0.        0.        7.8120284
 0.        0.        8.504973 ]
Sample y_pred values (first sample, first 10 genes):
[0.3352201  2.7581282  2.0546515  0.27231687 3.3380017  1.5512323
 0.05275315 1.1216645  3.0801165  8.739505  ]
y_true  -> mean=2.0827, std=3.4618, min=0.0000, max=12.4921
y_pred  -> mean=2.0896, std=2.2396, min=0.0000, max=13.4073
Batch 0 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6693
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6668
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6623
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6491
2025-11-14 21:18:00,316 - INFO - Fold 4 Train Epoch 45/200, Batch 50, Loss: 7.1371, Pearson: 0.6376, Spearman: 0.6087
2025-11-14 21:18:10,487 - INFO - Fold 4 Train Epoch 45/200, Batch 60, Loss: 6.9964, Pearson: 0.6513, Spearman: 0.6109
2025-11-14 21:18:20,645 - INFO - Fold 4 Train Epoch 45/200, Batch 70, Loss: 6.8608, Pearson: 0.6453, Spearman: 0.6084
2025-11-14 21:18:30,794 - INFO - Fold 4 Train Epoch 45/200, Batch 80, Loss: 7.2084, Pearson: 0.6479, Spearman: 0.6132
2025-11-14 21:18:33,899 - INFO - Fold 4 Train Epoch 45/200, Train Loss: 7.0323, Pearson Mean: 0.6501, Spearman Mean: 0.6122
2025-11-14 21:18:33,899 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4619, 'spearman_mean_genewise': 0.416, 'l1_error_mean': 1.8538, 'l2_errors_mean': 7.0321, 'r2_scores_mean': 0.2257, 'pearson_std': 0.113, 'l2_error_q1': 4.8403, 'l2_error_q2': 6.6836, 'l2_error_q3': 8.9512, 'r2_score_q1': 0.1453, 'r2_score_q2': 0.1989, 'r2_score_q3': 0.2728, 'mape_mean': 57.2096, 'mape_std': 17.9969, 'rmse_mean': 2.6088, 'rmse_std': 0.4754}
2025-11-14 21:18:34,260 - INFO - Fold 4 Val Epoch 45/200, Batch 0, Loss: 9.7632, Pearson: 0.5437, Spearman: 0.5505
2025-11-14 21:18:36,022 - INFO - Fold 4 Val Epoch 45/200, Batch 10, Loss: 8.7049, Pearson: 0.5671, Spearman: 0.5718
2025-11-14 21:18:37,719 - INFO - Fold 4 Val Epoch 45/200, Batch 20, Loss: 6.0888, Pearson: 0.5639, Spearman: 0.5615
2025-11-14 21:18:40,583 - INFO - Fold 4 Val Epoch 45/200, Val Loss: 8.1151, Pearson Mean: 0.5574, Spearman Mean: 0.5576
2025-11-14 21:18:40,583 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2977, 'spearman_mean_genewise': 0.2732, 'l1_error_mean': 2.0277, 'l2_errors_mean': 8.1374, 'r2_scores_mean': 0.0949, 'pearson_std': 0.1254, 'l2_error_q1': 5.0424, 'l2_error_q2': 7.6738, 'l2_error_q3': 11.2131, 'r2_score_q1': 0.0338, 'r2_score_q2': 0.0713, 'r2_score_q3': 0.1239, 'mape_mean': 66.4363, 'mape_std': 19.4318, 'rmse_mean': 2.7838, 'rmse_std': 0.6226}
2025-11-14 21:18:40,583 - INFO - Learning rate for epoch 45: 1.0000000000000004e-08
2025-11-14 21:18:40,583 - INFO - No improvement in spearman genewise. Patience: 30/30
2025-11-14 21:18:40,583 - INFO - Early stopping triggered. Breaking training loop.
2025-11-14 21:18:40,584 - INFO - ===== Completed Fold 4/5 =====
2025-11-14 21:18:40,585 - INFO - 
===== Starting Fold 5/5 =====
2025-11-14 21:18:40,585 - INFO - Fold 5: Train=29, Val=7
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6703
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6551
image : torch.Size([104, 3, 224, 224]), y_true: torch.Size([104, 785]), y_pred: torch.Size([104, 785])
Batch 82 Pearson correlation: 0.6502
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.032054424285889
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A1.h5ad
 Loaded images: (346, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (346, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A2.h5ad
 Loaded images: (325, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (325, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A3.h5ad
 Loaded images: (359, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (359, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A4.h5ad
 Loaded images: (343, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (343, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A5.h5ad
 Loaded images: (332, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (332, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/A6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/A6.h5ad
 Loaded images: (360, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (360, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B3.h5ad
 Loaded images: (298, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (298, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B4.h5ad
 Loaded images: (283, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (283, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B5.h5ad
 Loaded images: (289, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (289, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B6.h5ad
 Loaded images: (277, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (277, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C1.h5ad
 Loaded images: (176, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (176, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C2.h5ad
 Loaded images: (187, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (187, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C4.h5ad
 Loaded images: (184, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (184, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C5.h5ad
 Loaded images: (181, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (181, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C6.h5ad
 Loaded images: (178, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (178, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D2.h5ad
 Loaded images: (303, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (303, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D4.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D4.h5ad
 Loaded images: (302, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (302, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D5.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D5.h5ad
 Loaded images: (306, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (306, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D6.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D6.h5ad
 Loaded images: (315, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (315, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E1.h5ad
 Loaded images: (587, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (587, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E3.h5ad
 Loaded images: (570, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (570, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F1.h5ad
 Loaded images: (691, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (691, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F3.h5ad
 Loaded images: (712, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (712, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G1.h5ad
 Loaded images: (441, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (441, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G2.h5ad
 Loaded images: (467, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (467, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/G3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/G3.h5ad
 Loaded images: (463, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (463, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H1.h5ad
 Loaded images: (613, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (613, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H2.h5ad
 Loaded images: (603, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (603, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/H3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/H3.h5ad
 Loaded images: (510, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (510, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B1.h5ad
 Loaded images: (295, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (295, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/B2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/B2.h5ad
 Loaded images: (270, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
2025-11-14 21:18:58,296 - INFO - Fold 5: Train=11001, Val=2619
2025-11-14 21:18:58,296 - INFO - train_datasets length:  11001
2025-11-14 21:18:58,296 - INFO - Number of train batches: 86
2025-11-14 21:18:58,296 - INFO - Initializing model...
2025-11-14 21:18:58,393 - INFO - Model
STNet(
  (features): Sequential(
    (conv0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu0): ReLU(inplace=True)
    (pool0): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (denseblock1): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(96, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition1): _Transition(
      (norm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock2): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(160, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(192, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(224, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition2): _Transition(
      (norm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock3): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(288, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(352, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(352, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(384, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(416, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(416, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(448, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(448, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(480, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer17): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer18): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer19): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer20): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer21): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer22): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer23): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer24): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (transition3): _Transition(
      (norm): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (pool): AvgPool2d(kernel_size=2, stride=2, padding=0)
    )
    (denseblock4): _DenseBlock(
      (denselayer1): _DenseLayer(
        (norm1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer2): _DenseLayer(
        (norm1): BatchNorm2d(544, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(544, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer3): _DenseLayer(
        (norm1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(576, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer4): _DenseLayer(
        (norm1): BatchNorm2d(608, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(608, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer5): _DenseLayer(
        (norm1): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(640, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer6): _DenseLayer(
        (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(672, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer7): _DenseLayer(
        (norm1): BatchNorm2d(704, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(704, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer8): _DenseLayer(
        (norm1): BatchNorm2d(736, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(736, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer9): _DenseLayer(
        (norm1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer10): _DenseLayer(
        (norm1): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(800, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer11): _DenseLayer(
        (norm1): BatchNorm2d(832, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer12): _DenseLayer(
        (norm1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(864, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer13): _DenseLayer(
        (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(896, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer14): _DenseLayer(
        (norm1): BatchNorm2d(928, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(928, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer15): _DenseLayer(
        (norm1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(960, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
      (denselayer16): _DenseLayer(
        (norm1): BatchNorm2d(992, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu1): ReLU(inplace=True)
        (conv1): Conv2d(992, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu2): ReLU(inplace=True)
        (conv2): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      )
    )
    (norm5): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  )
  (global_pool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Linear(in_features=1024, out_features=785, bias=True)
)
2025-11-14 21:18:58,396 - INFO - Using device: cuda
2025-11-14 21:18:59,342 - INFO - Fold 5 Train Epoch 1/200, Batch 0, Loss: 15.5998, Pearson: -0.0088, Spearman: -0.0037
2025-11-14 21:19:08,772 - INFO - Fold 5 Train Epoch 1/200, Batch 10, Loss: 15.4360, Pearson: 0.1095, Spearman: 0.0693
2025-11-14 21:19:18,148 - INFO - Fold 5 Train Epoch 1/200, Batch 20, Loss: 13.5830, Pearson: 0.2228, Spearman: 0.1426
2025-11-14 21:19:27,524 - INFO - Fold 5 Train Epoch 1/200, Batch 30, Loss: 13.3846, Pearson: 0.2599, Spearman: 0.2233
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (270, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/C3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/C3.h5ad
 Loaded images: (180, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (180, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D1.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D1.h5ad
 Loaded images: (306, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (306, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/D3.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/D3.h5ad
 Loaded images: (301, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (301, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/E2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/E2.h5ad
 Loaded images: (572, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (572, 785)
Initializing PatchDataset...
Image file: /home/puneet/maninder/data/her2st_dataset/224/F2.h5
Gene file:  /home/puneet/maninder/data/her2st_dataset/224/F2.h5ad
 Loaded images: (695, 224, 224, 3)
 Using 785 valid genes from gene list (785 total).
 Applying custom log1p normalization...
 Applied log1p normalization with scale_factor=1000000.
 Final gene matrix shape: (695, 785)
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 1 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       7.461715
 0.       7.461715]
Sample y_pred values (first sample, first 10 genes):
[0.         0.6642467  0.3402564  0.         0.43027163 0.19113287
 0.38947448 0.38141567 0.         0.        ]
y_true  -> mean=2.0315, std=3.4692, min=0.0000, max=12.6870
y_pred  -> mean=0.1632, std=0.2429, min=0.0000, max=1.9027
Batch 0 Pearson correlation: -0.0088
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.0087
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.0182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.0446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.0530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.0676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.0816
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.0842
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.0998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.1166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.1095
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.1339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.1593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.1672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.1859
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.1747
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.1870
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.2181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.2233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.2201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.2228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.2289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.2221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.2426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.2637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.2218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.2373
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.2630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.2865
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.2777
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.2599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.3124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.2843
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.3378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.2928
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.3086
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.3119
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.3389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.3652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.3268
2025-11-14 21:19:36,943 - INFO - Fold 5 Train Epoch 1/200, Batch 40, Loss: 11.4049, Pearson: 0.3719, Spearman: 0.3265
2025-11-14 21:19:46,394 - INFO - Fold 5 Train Epoch 1/200, Batch 50, Loss: 10.2829, Pearson: 0.3884, Spearman: 0.3705
2025-11-14 21:19:55,772 - INFO - Fold 5 Train Epoch 1/200, Batch 60, Loss: 9.7859, Pearson: 0.4269, Spearman: 0.4119
2025-11-14 21:20:03,620 - INFO - Fold 5 Train Epoch 1/200, Batch 70, Loss: 9.4688, Pearson: 0.4613, Spearman: 0.4457
2025-11-14 21:20:11,168 - INFO - Fold 5 Train Epoch 1/200, Batch 80, Loss: 9.6423, Pearson: 0.4720, Spearman: 0.4590
2025-11-14 21:20:17,185 - INFO - Fold 5 Train Epoch 1/200, Train Loss: 11.8508, Pearson Mean: 0.3232, Spearman Mean: 0.2885
2025-11-14 21:20:17,186 - INFO - Training Metrics: {'pearson_mean_genewise': 0.1824, 'spearman_mean_genewise': 0.1671, 'l1_error_mean': 2.2209, 'l2_errors_mean': 11.8525, 'r2_scores_mean': -0.1791, 'pearson_std': 0.0754, 'l2_error_q1': 5.6362, 'l2_error_q2': 8.9136, 'l2_error_q3': 14.8802, 'r2_score_q1': -0.1066, 'r2_score_q2': -0.0161, 'r2_score_q3': 0.0107, 'mape_mean': 80.2995, 'mape_std': 7.2283, 'rmse_mean': 3.2463, 'rmse_std': 1.1464}
2025-11-14 21:20:17,459 - INFO - Fold 5 Val Epoch 1/200, Batch 0, Loss: 9.6860, Pearson: 0.1908, Spearman: 0.1969
2025-11-14 21:20:19,137 - INFO - Fold 5 Val Epoch 1/200, Batch 10, Loss: 9.5761, Pearson: 0.5133, Spearman: 0.5101
2025-11-14 21:20:20,750 - INFO - Fold 5 Val Epoch 1/200, Batch 20, Loss: 10.3356, Pearson: 0.4673, Spearman: 0.4932
2025-11-14 21:20:23,004 - INFO - Fold 5 Val Epoch 1/200, Val Loss: 9.7289, Pearson Mean: 0.4584, Spearman Mean: 0.4485
2025-11-14 21:20:23,004 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2316, 'spearman_mean_genewise': 0.2053, 'l1_error_mean': 2.3746, 'l2_errors_mean': 9.7129, 'r2_scores_mean': -0.0586, 'pearson_std': 0.1001, 'l2_error_q1': 5.9206, 'l2_error_q2': 8.7417, 'l2_error_q3': 12.4375, 'r2_score_q1': 0.0028, 'r2_score_q2': 0.0286, 'r2_score_q3': 0.0593, 'mape_mean': 68.8104, 'mape_std': 15.3093, 'rmse_mean': 3.0223, 'rmse_std': 0.7609}
2025-11-14 21:20:23,004 - INFO - Learning rate for epoch 1: 0.0001
2025-11-14 21:20:23,048 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_5/best_model.pth
2025-11-14 21:20:23,958 - INFO - Fold 5 Train Epoch 2/200, Batch 0, Loss: 9.1481, Pearson: 0.4890, Spearman: 0.4771
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.3719
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.3561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.3457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.3534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.3286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.3703
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.3917
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.3914
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.3798
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.4089
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.3884
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.3731
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.3918
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.3935
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.4243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.4175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.4336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.4332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.4365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.4458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.4269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.4109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.4417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.4448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.4353
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.4557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.4317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.4606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.4281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.4516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.4613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.4606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.4659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.4627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.4858
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.4839
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.4798
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.4783
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.4703
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.4728
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.4720
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.4843
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5014
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.4904
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.4857
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.4640
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 11.85254955291748
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 2 =====================
Sample y_true values (first sample, first 10 genes):
[7.1724887 0.        0.        0.        7.865252  0.        7.865252
 7.865252  7.865252  7.865252 ]
Sample y_pred values (first sample, first 10 genes):
[1.3118894 2.310867  3.4531605 1.3144145 5.3034735 4.641377  1.1576655
 2.7545528 4.8754406 4.7655044]
y_true  -> mean=1.9926, std=3.4561, min=0.0000, max=12.7365
y_pred  -> mean=1.7512, std=1.6612, min=0.0000, max=10.9748
Batch 0 Pearson correlation: 0.4890
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.4996
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.4826
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.4977
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.4967
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5004
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.4991
2025-11-14 21:20:33,858 - INFO - Fold 5 Train Epoch 2/200, Batch 10, Loss: 8.9973, Pearson: 0.4977, Spearman: 0.4772
2025-11-14 21:20:43,940 - INFO - Fold 5 Train Epoch 2/200, Batch 20, Loss: 8.9114, Pearson: 0.5163, Spearman: 0.4887
2025-11-14 21:20:54,086 - INFO - Fold 5 Train Epoch 2/200, Batch 30, Loss: 9.1846, Pearson: 0.5060, Spearman: 0.4874
2025-11-14 21:21:04,177 - INFO - Fold 5 Train Epoch 2/200, Batch 40, Loss: 8.8731, Pearson: 0.5237, Spearman: 0.5087
2025-11-14 21:21:14,311 - INFO - Fold 5 Train Epoch 2/200, Batch 50, Loss: 8.9544, Pearson: 0.5366, Spearman: 0.5148
2025-11-14 21:21:24,418 - INFO - Fold 5 Train Epoch 2/200, Batch 60, Loss: 9.1481, Pearson: 0.5384, Spearman: 0.5260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.4850
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5067
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.4977
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.4917
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5093
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5113
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.4954
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.4810
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.4981
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5060
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5238
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5051
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5201
2025-11-14 21:21:32,311 - INFO - Fold 5 Train Epoch 2/200, Batch 70, Loss: 8.5502, Pearson: 0.5302, Spearman: 0.5150
2025-11-14 21:21:40,761 - INFO - Fold 5 Train Epoch 2/200, Batch 80, Loss: 8.8952, Pearson: 0.5351, Spearman: 0.5154
2025-11-14 21:21:46,882 - INFO - Fold 5 Train Epoch 2/200, Train Loss: 8.8739, Pearson Mean: 0.5231, Spearman Mean: 0.5005
2025-11-14 21:21:46,882 - INFO - Training Metrics: {'pearson_mean_genewise': 0.2971, 'spearman_mean_genewise': 0.2771, 'l1_error_mean': 2.162, 'l2_errors_mean': 8.8741, 'r2_scores_mean': 0.0592, 'pearson_std': 0.1011, 'l2_error_q1': 5.2675, 'l2_error_q2': 7.6963, 'l2_error_q3': 11.6688, 'r2_score_q1': 0.0417, 'r2_score_q2': 0.0742, 'r2_score_q3': 0.116, 'mape_mean': 68.3101, 'mape_std': 15.3658, 'rmse_mean': 2.8883, 'rmse_std': 0.7294}
2025-11-14 21:21:47,163 - INFO - Fold 5 Val Epoch 2/200, Batch 0, Loss: 7.8595, Pearson: 0.3825, Spearman: 0.3974
2025-11-14 21:21:48,861 - INFO - Fold 5 Val Epoch 2/200, Batch 10, Loss: 8.3928, Pearson: 0.5854, Spearman: 0.5769
2025-11-14 21:21:50,349 - INFO - Fold 5 Val Epoch 2/200, Batch 20, Loss: 9.5376, Pearson: 0.5253, Spearman: 0.5373
2025-11-14 21:21:52,648 - INFO - Fold 5 Val Epoch 2/200, Val Loss: 8.7907, Pearson Mean: 0.5252, Spearman Mean: 0.5159
2025-11-14 21:21:52,648 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2741, 'spearman_mean_genewise': 0.2415, 'l1_error_mean': 2.2664, 'l2_errors_mean': 8.771, 'r2_scores_mean': 0.0471, 'pearson_std': 0.1001, 'l2_error_q1': 5.7319, 'l2_error_q2': 8.466, 'l2_error_q3': 11.3514, 'r2_score_q1': 0.0324, 'r2_score_q2': 0.0578, 'r2_score_q3': 0.095, 'mape_mean': 66.0472, 'mape_std': 18.035, 'rmse_mean': 2.8963, 'rmse_std': 0.6182}
2025-11-14 21:21:52,648 - INFO - Learning rate for epoch 2: 0.0001
2025-11-14 21:21:52,708 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_5/best_model.pth
2025-11-14 21:21:53,629 - INFO - Fold 5 Train Epoch 3/200, Batch 0, Loss: 8.6278, Pearson: 0.5310, Spearman: 0.5027
2025-11-14 21:22:03,663 - INFO - Fold 5 Train Epoch 3/200, Batch 10, Loss: 8.9158, Pearson: 0.5350, Spearman: 0.5168
2025-11-14 21:22:13,765 - INFO - Fold 5 Train Epoch 3/200, Batch 20, Loss: 8.3623, Pearson: 0.5521, Spearman: 0.5221
2025-11-14 21:22:23,885 - INFO - Fold 5 Train Epoch 3/200, Batch 30, Loss: 8.3740, Pearson: 0.5736, Spearman: 0.5368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5682
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.5505
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.5450
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.874070167541504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 3 =====================
Sample y_true values (first sample, first 10 genes):
[0.       7.85232  0.       0.       0.       0.       0.       7.85232
 8.545273 9.643756]
Sample y_pred values (first sample, first 10 genes):
[0.3332774  1.1228807  1.1910796  0.16118659 2.1599658  0.95886433
 0.2319438  0.83810145 2.59719    5.420422  ]
y_true  -> mean=2.0070, std=3.4652, min=0.0000, max=12.5937
y_pred  -> mean=1.9398, std=1.8750, min=0.0000, max=12.0910
Batch 0 Pearson correlation: 0.5310
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5665
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5661
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5736
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5705
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5725
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5603
2025-11-14 21:22:34,007 - INFO - Fold 5 Train Epoch 3/200, Batch 40, Loss: 8.4322, Pearson: 0.5484, Spearman: 0.5265
2025-11-14 21:22:44,120 - INFO - Fold 5 Train Epoch 3/200, Batch 50, Loss: 8.4244, Pearson: 0.5549, Spearman: 0.5302
2025-11-14 21:22:54,229 - INFO - Fold 5 Train Epoch 3/200, Batch 60, Loss: 8.3196, Pearson: 0.5551, Spearman: 0.5429
2025-11-14 21:23:01,298 - INFO - Fold 5 Train Epoch 3/200, Batch 70, Loss: 8.1432, Pearson: 0.5491, Spearman: 0.5279
2025-11-14 21:23:10,893 - INFO - Fold 5 Train Epoch 3/200, Batch 80, Loss: 8.2613, Pearson: 0.5688, Spearman: 0.5474
2025-11-14 21:23:17,136 - INFO - Fold 5 Train Epoch 3/200, Train Loss: 8.3443, Pearson Mean: 0.5603, Spearman Mean: 0.5337
2025-11-14 21:23:17,136 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3295, 'spearman_mean_genewise': 0.304, 'l1_error_mean': 2.0886, 'l2_errors_mean': 8.3443, 'r2_scores_mean': 0.1032, 'pearson_std': 0.1031, 'l2_error_q1': 5.1612, 'l2_error_q2': 7.5711, 'l2_error_q3': 11.135, 'r2_score_q1': 0.0599, 'r2_score_q2': 0.0953, 'r2_score_q3': 0.1459, 'mape_mean': 65.6188, 'mape_std': 16.7787, 'rmse_mean': 2.8179, 'rmse_std': 0.6356}
2025-11-14 21:23:17,474 - INFO - Fold 5 Val Epoch 3/200, Batch 0, Loss: 8.5375, Pearson: 0.3260, Spearman: 0.3432
2025-11-14 21:23:19,095 - INFO - Fold 5 Val Epoch 3/200, Batch 10, Loss: 8.1669, Pearson: 0.6035, Spearman: 0.5974
2025-11-14 21:23:20,634 - INFO - Fold 5 Val Epoch 3/200, Batch 20, Loss: 9.4013, Pearson: 0.5300, Spearman: 0.5434
2025-11-14 21:23:22,929 - INFO - Fold 5 Val Epoch 3/200, Val Loss: 8.7577, Pearson Mean: 0.5316, Spearman Mean: 0.5230
2025-11-14 21:23:22,929 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.282, 'spearman_mean_genewise': 0.2525, 'l1_error_mean': 2.1713, 'l2_errors_mean': 8.7408, 'r2_scores_mean': 0.0504, 'pearson_std': 0.0959, 'l2_error_q1': 5.7372, 'l2_error_q2': 8.559, 'l2_error_q3': 11.3888, 'r2_score_q1': 0.0323, 'r2_score_q2': 0.0557, 'r2_score_q3': 0.0904, 'mape_mean': 67.536, 'mape_std': 17.5743, 'rmse_mean': 2.8993, 'rmse_std': 0.5788}
2025-11-14 21:23:22,930 - INFO - Learning rate for epoch 3: 0.0001
2025-11-14 21:23:22,981 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_5/best_model.pth
2025-11-14 21:23:24,019 - INFO - Fold 5 Train Epoch 4/200, Batch 0, Loss: 7.9693, Pearson: 0.5941, Spearman: 0.5550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5694
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5685
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5620
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5754
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5763
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5767
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5723
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5653
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5688
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5691
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5714
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5778
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5735
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5711
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5804
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5764
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5720
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5793
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5688
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5696
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5732
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5786
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.5802
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.5780
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.344347953796387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 4 =====================
Sample y_true values (first sample, first 10 genes):
[8.079248 0.       0.       0.       0.       0.       8.079248 0.
 0.       0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.6661439  1.6812358  3.2751327  0.6181403  3.77143    3.619351
 0.36211377 1.7183986  3.6682448  5.065325  ]
y_true  -> mean=2.1756, std=3.4991, min=0.0000, max=12.8896
y_pred  -> mean=1.9966, std=1.9544, min=0.0000, max=12.2981
Batch 0 Pearson correlation: 0.5941
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5786
2025-11-14 21:23:34,021 - INFO - Fold 5 Train Epoch 4/200, Batch 10, Loss: 7.9664, Pearson: 0.5865, Spearman: 0.5505
2025-11-14 21:23:44,164 - INFO - Fold 5 Train Epoch 4/200, Batch 20, Loss: 8.1404, Pearson: 0.5818, Spearman: 0.5485
2025-11-14 21:23:54,263 - INFO - Fold 5 Train Epoch 4/200, Batch 30, Loss: 8.3055, Pearson: 0.5771, Spearman: 0.5480
2025-11-14 21:24:04,387 - INFO - Fold 5 Train Epoch 4/200, Batch 40, Loss: 8.3568, Pearson: 0.5750, Spearman: 0.5461
2025-11-14 21:24:14,524 - INFO - Fold 5 Train Epoch 4/200, Batch 50, Loss: 7.9344, Pearson: 0.5805, Spearman: 0.5531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5843
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.5477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5761
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5851
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5758
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5697
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5710
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5629
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5865
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5756
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5882
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5889
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.5871
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5705
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5860
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5731
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5818
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5756
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5779
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5895
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5886
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5843
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5724
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5799
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5717
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5771
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5829
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5767
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5882
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5787
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.5676
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5818
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.5774
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5726
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5750
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5831
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5964
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5727
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.5868
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5782
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5823
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5906
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5767
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5775
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5805
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5792
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5825
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5780
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5847
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5886
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5751
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5790
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5732
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.5782
2025-11-14 21:24:23,954 - INFO - Fold 5 Train Epoch 4/200, Batch 60, Loss: 7.8788, Pearson: 0.5891, Spearman: 0.5569
2025-11-14 21:24:31,229 - INFO - Fold 5 Train Epoch 4/200, Batch 70, Loss: 8.0856, Pearson: 0.5855, Spearman: 0.5552
2025-11-14 21:24:41,219 - INFO - Fold 5 Train Epoch 4/200, Batch 80, Loss: 8.1942, Pearson: 0.5747, Spearman: 0.5488
2025-11-14 21:24:47,457 - INFO - Fold 5 Train Epoch 4/200, Train Loss: 8.0423, Pearson Mean: 0.5805, Spearman Mean: 0.5511
2025-11-14 21:24:47,458 - INFO - Training Metrics: {'pearson_mean_genewise': 0.351, 'spearman_mean_genewise': 0.3216, 'l1_error_mean': 2.0394, 'l2_errors_mean': 8.0425, 'r2_scores_mean': 0.1277, 'pearson_std': 0.1072, 'l2_error_q1': 5.0547, 'l2_error_q2': 7.459, 'l2_error_q3': 10.8195, 'r2_score_q1': 0.0715, 'r2_score_q2': 0.1114, 'r2_score_q3': 0.1643, 'mape_mean': 63.9348, 'mape_std': 17.4506, 'rmse_mean': 2.7754, 'rmse_std': 0.5829}
2025-11-14 21:24:47,826 - INFO - Fold 5 Val Epoch 4/200, Batch 0, Loss: 7.6158, Pearson: 0.4188, Spearman: 0.4290
2025-11-14 21:24:49,468 - INFO - Fold 5 Val Epoch 4/200, Batch 10, Loss: 8.0346, Pearson: 0.6144, Spearman: 0.6049
2025-11-14 21:24:50,983 - INFO - Fold 5 Val Epoch 4/200, Batch 20, Loss: 9.2581, Pearson: 0.5441, Spearman: 0.5523
2025-11-14 21:24:53,278 - INFO - Fold 5 Val Epoch 4/200, Val Loss: 8.5977, Pearson Mean: 0.5434, Spearman Mean: 0.5373
2025-11-14 21:24:53,279 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.285, 'spearman_mean_genewise': 0.2593, 'l1_error_mean': 2.1867, 'l2_errors_mean': 8.5803, 'r2_scores_mean': 0.0674, 'pearson_std': 0.0979, 'l2_error_q1': 5.7133, 'l2_error_q2': 8.4024, 'l2_error_q3': 11.277, 'r2_score_q1': 0.0356, 'r2_score_q2': 0.0621, 'r2_score_q3': 0.0969, 'mape_mean': 66.9953, 'mape_std': 17.6282, 'rmse_mean': 2.8761, 'rmse_std': 0.5551}
2025-11-14 21:24:53,279 - INFO - Learning rate for epoch 4: 0.0001
2025-11-14 21:24:53,340 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_5/best_model.pth
2025-11-14 21:24:54,318 - INFO - Fold 5 Train Epoch 5/200, Batch 0, Loss: 7.8904, Pearson: 0.5815, Spearman: 0.5478
2025-11-14 21:25:04,331 - INFO - Fold 5 Train Epoch 5/200, Batch 10, Loss: 8.1218, Pearson: 0.5959, Spearman: 0.5605
2025-11-14 21:25:14,488 - INFO - Fold 5 Train Epoch 5/200, Batch 20, Loss: 7.7959, Pearson: 0.5882, Spearman: 0.5611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5891
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5887
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5912
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5898
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5871
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5744
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5812
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5707
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5849
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5822
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5855
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.5862
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5785
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5834
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5831
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5888
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5889
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5882
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5913
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5784
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5747
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5860
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5841
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5819
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.5928
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.5812
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 8.042465209960938
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 5 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[1.2256085 2.3242612 3.7082546 1.0639446 4.657897  4.2000933 0.9692629
 2.2076342 3.7814295 5.8752413]
y_true  -> mean=2.0090, std=3.4522, min=0.0000, max=13.1224
y_pred  -> mean=1.9550, std=2.0132, min=0.0000, max=13.1492
Batch 0 Pearson correlation: 0.5815
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.5809
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.5920
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6000
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5960
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5897
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.5785
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.5991
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.5816
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.5959
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5918
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5916
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5852
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.5939
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6031
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6003
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5948
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.5745
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5882
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5942
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6075
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5936
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5825
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5936
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.5977
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.5940
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 21:25:24,618 - INFO - Fold 5 Train Epoch 5/200, Batch 30, Loss: 7.9298, Pearson: 0.5830, Spearman: 0.5521
2025-11-14 21:25:34,751 - INFO - Fold 5 Train Epoch 5/200, Batch 40, Loss: 8.0194, Pearson: 0.5951, Spearman: 0.5634
2025-11-14 21:25:44,869 - INFO - Fold 5 Train Epoch 5/200, Batch 50, Loss: 7.8090, Pearson: 0.5979, Spearman: 0.5623
2025-11-14 21:25:53,787 - INFO - Fold 5 Train Epoch 5/200, Batch 60, Loss: 7.7391, Pearson: 0.5961, Spearman: 0.5656
2025-11-14 21:26:01,414 - INFO - Fold 5 Train Epoch 5/200, Batch 70, Loss: 7.8596, Pearson: 0.5974, Spearman: 0.5684
2025-11-14 21:26:11,530 - INFO - Fold 5 Train Epoch 5/200, Batch 80, Loss: 7.8171, Pearson: 0.5981, Spearman: 0.5662
2025-11-14 21:26:17,775 - INFO - Fold 5 Train Epoch 5/200, Train Loss: 7.8753, Pearson Mean: 0.5933, Spearman Mean: 0.5607
2025-11-14 21:26:17,775 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3667, 'spearman_mean_genewise': 0.3336, 'l1_error_mean': 2.008, 'l2_errors_mean': 7.8752, 'r2_scores_mean': 0.1421, 'pearson_std': 0.1096, 'l2_error_q1': 5.0098, 'l2_error_q2': 7.3593, 'l2_error_q3': 10.5136, 'r2_score_q1': 0.0804, 'r2_score_q2': 0.1225, 'r2_score_q3': 0.1806, 'mape_mean': 63.0635, 'mape_std': 17.4927, 'rmse_mean': 2.7488, 'rmse_std': 0.5649}
2025-11-14 21:26:18,067 - INFO - Fold 5 Val Epoch 5/200, Batch 0, Loss: 7.9197, Pearson: 0.3908, Spearman: 0.4064
2025-11-14 21:26:19,713 - INFO - Fold 5 Val Epoch 5/200, Batch 10, Loss: 8.1327, Pearson: 0.6020, Spearman: 0.5996
2025-11-14 21:26:21,239 - INFO - Fold 5 Val Epoch 5/200, Batch 20, Loss: 9.2341, Pearson: 0.5363, Spearman: 0.5545
2025-11-14 21:26:23,522 - INFO - Fold 5 Val Epoch 5/200, Val Loss: 8.6003, Pearson Mean: 0.5421, Spearman Mean: 0.5359
2025-11-14 21:26:23,522 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.28, 'spearman_mean_genewise': 0.2546, 'l1_error_mean': 2.2802, 'l2_errors_mean': 8.5836, 'r2_scores_mean': 0.066, 'pearson_std': 0.0938, 'l2_error_q1': 5.7483, 'l2_error_q2': 8.3437, 'l2_error_q3': 11.2455, 'r2_score_q1': 0.0303, 'r2_score_q2': 0.0536, 'r2_score_q3': 0.091, 'mape_mean': 61.3682, 'mape_std': 18.8498, 'rmse_mean': 2.878, 'rmse_std': 0.5482}
2025-11-14 21:26:23,522 - INFO - Learning rate for epoch 5: 0.0001
2025-11-14 21:26:23,522 - INFO - No improvement in spearman genewise. Patience: 1/30
Batch 28 Pearson correlation: 0.6000
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5927
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.5830
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.5915
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5981
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.5906
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5954
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6010
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.5900
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6005
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.5916
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.5951
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.5950
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.5860
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5992
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6002
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.5970
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5845
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5778
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.5839
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5722
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.5979
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5970
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.5956
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.5846
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5901
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5951
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5946
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.5961
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6034
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.5961
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.5825
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5986
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.5913
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6004
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.5981
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6085
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.5931
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.5878
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.5869
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.5974
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6040
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5912
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.5908
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.5903
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5958
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.5824
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5944
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.5836
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.5906
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5981
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6107
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5894
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6037
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.5779
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.5814
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.8752121925354
2025-11-14 21:26:24,452 - INFO - Fold 5 Train Epoch 6/200, Batch 0, Loss: 7.8449, Pearson: 0.6030, Spearman: 0.5682
2025-11-14 21:26:34,422 - INFO - Fold 5 Train Epoch 6/200, Batch 10, Loss: 7.9132, Pearson: 0.6002, Spearman: 0.5693
2025-11-14 21:26:44,560 - INFO - Fold 5 Train Epoch 6/200, Batch 20, Loss: 8.0775, Pearson: 0.5927, Spearman: 0.5622
2025-11-14 21:26:54,718 - INFO - Fold 5 Train Epoch 6/200, Batch 30, Loss: 7.4336, Pearson: 0.6106, Spearman: 0.5754
2025-11-14 21:27:04,809 - INFO - Fold 5 Train Epoch 6/200, Batch 40, Loss: 7.9299, Pearson: 0.6066, Spearman: 0.5756
2025-11-14 21:27:14,917 - INFO - Fold 5 Train Epoch 6/200, Batch 50, Loss: 7.5748, Pearson: 0.6010, Spearman: 0.5609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 6 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       7.978123 0.       8.671099 0.       0.
 0.       8.671099]
Sample y_pred values (first sample, first 10 genes):
[1.1133385  1.7995356  3.264124   0.6529673  3.8633072  3.86392
 0.54999596 1.6356916  3.2047818  4.5137415 ]
y_true  -> mean=2.1327, std=3.5067, min=0.0000, max=12.7568
y_pred  -> mean=2.0029, std=2.0713, min=0.0000, max=13.8066
Batch 0 Pearson correlation: 0.6030
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6008
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6048
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.5972
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5984
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5977
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6054
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6002
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6012
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.5958
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.5939
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6005
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6031
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6137
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.5927
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.5998
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5957
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.5986
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.5913
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.5968
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.5833
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6001
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.5964
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6042
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.5987
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5892
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6080
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6003
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6047
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6014
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6013
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6001
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5991
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6008
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5947
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.5943
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6086
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.5963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6010
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5976
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6000
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6084
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.5930
2025-11-14 21:27:23,477 - INFO - Fold 5 Train Epoch 6/200, Batch 60, Loss: 7.6830, Pearson: 0.6081, Spearman: 0.5712
2025-11-14 21:27:31,410 - INFO - Fold 5 Train Epoch 6/200, Batch 70, Loss: 7.4112, Pearson: 0.6162, Spearman: 0.5771
2025-11-14 21:27:41,526 - INFO - Fold 5 Train Epoch 6/200, Batch 80, Loss: 7.7821, Pearson: 0.5915, Spearman: 0.5607
2025-11-14 21:27:47,706 - INFO - Fold 5 Train Epoch 6/200, Train Loss: 7.7367, Pearson Mean: 0.6025, Spearman Mean: 0.5683
2025-11-14 21:27:47,706 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3794, 'spearman_mean_genewise': 0.3437, 'l1_error_mean': 1.9795, 'l2_errors_mean': 7.7366, 'r2_scores_mean': 0.1538, 'pearson_std': 0.1126, 'l2_error_q1': 4.9828, 'l2_error_q2': 7.2607, 'l2_error_q3': 10.3019, 'r2_score_q1': 0.0868, 'r2_score_q2': 0.132, 'r2_score_q3': 0.1924, 'mape_mean': 62.7229, 'mape_std': 17.5952, 'rmse_mean': 2.7264, 'rmse_std': 0.5506}
2025-11-14 21:27:47,998 - INFO - Fold 5 Val Epoch 6/200, Batch 0, Loss: 7.5159, Pearson: 0.4304, Spearman: 0.4376
2025-11-14 21:27:49,682 - INFO - Fold 5 Val Epoch 6/200, Batch 10, Loss: 8.0876, Pearson: 0.6082, Spearman: 0.6035
2025-11-14 21:27:51,169 - INFO - Fold 5 Val Epoch 6/200, Batch 20, Loss: 9.0288, Pearson: 0.5497, Spearman: 0.5616
2025-11-14 21:27:53,470 - INFO - Fold 5 Val Epoch 6/200, Val Loss: 8.4810, Pearson Mean: 0.5506, Spearman Mean: 0.5432
2025-11-14 21:27:53,471 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2847, 'spearman_mean_genewise': 0.2601, 'l1_error_mean': 2.2404, 'l2_errors_mean': 8.4666, 'r2_scores_mean': 0.0779, 'pearson_std': 0.0969, 'l2_error_q1': 5.6705, 'l2_error_q2': 8.2742, 'l2_error_q3': 11.1508, 'r2_score_q1': 0.0403, 'r2_score_q2': 0.0657, 'r2_score_q3': 0.1033, 'mape_mean': 64.8248, 'mape_std': 18.6873, 'rmse_mean': 2.859, 'rmse_std': 0.5407}
2025-11-14 21:27:53,471 - INFO - Learning rate for epoch 6: 0.0001
2025-11-14 21:27:53,538 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_5/best_model.pth
2025-11-14 21:27:54,536 - INFO - Fold 5 Train Epoch 7/200, Batch 0, Loss: 7.6739, Pearson: 0.6083, Spearman: 0.5693
2025-11-14 21:28:04,533 - INFO - Fold 5 Train Epoch 7/200, Batch 10, Loss: 7.8904, Pearson: 0.6057, Spearman: 0.5686
2025-11-14 21:28:14,647 - INFO - Fold 5 Train Epoch 7/200, Batch 20, Loss: 7.7429, Pearson: 0.6208, Spearman: 0.5826
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.5982
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.5818
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6026
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6081
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6029
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.5928
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6025
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6067
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6004
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6090
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.5993
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.5995
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6015
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.5915
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.5996
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.5982
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.5907
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.5911
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6021
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.736638069152832
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 7 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.888395]
Sample y_pred values (first sample, first 10 genes):
[0.12414532 0.87691265 0.873209   0.         0.9877663  0.08584277
 0.17385231 0.22033527 1.4736584  6.3675566 ]
y_true  -> mean=2.0768, std=3.4896, min=0.0000, max=12.7194
y_pred  -> mean=2.0404, std=2.0812, min=0.0000, max=14.5588
Batch 0 Pearson correlation: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6005
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6079
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5986
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.5954
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6045
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6137
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6101
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6082
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.5971
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.5970
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.5932
2025-11-14 21:28:24,769 - INFO - Fold 5 Train Epoch 7/200, Batch 30, Loss: 7.5828, Pearson: 0.6160, Spearman: 0.5788
2025-11-14 21:28:34,925 - INFO - Fold 5 Train Epoch 7/200, Batch 40, Loss: 7.3796, Pearson: 0.6190, Spearman: 0.5785
2025-11-14 21:28:45,024 - INFO - Fold 5 Train Epoch 7/200, Batch 50, Loss: 7.5896, Pearson: 0.6053, Spearman: 0.5739
2025-11-14 21:28:52,868 - INFO - Fold 5 Train Epoch 7/200, Batch 60, Loss: 7.6741, Pearson: 0.6108, Spearman: 0.5786
2025-11-14 21:29:01,642 - INFO - Fold 5 Train Epoch 7/200, Batch 70, Loss: 7.4517, Pearson: 0.6131, Spearman: 0.5748
2025-11-14 21:29:11,784 - INFO - Fold 5 Train Epoch 7/200, Batch 80, Loss: 7.2694, Pearson: 0.6146, Spearman: 0.5788
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6026
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6042
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.5989
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6001
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6107
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6015
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6022
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6038
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6094
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6114
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6082
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6103
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6028
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.5873
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6063
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.5938
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6064
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6063
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6053
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.5984
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6038
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6100
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6062
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.5986
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6106
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6011
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6065
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6062
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6021
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6091
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6098
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6151
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.5988
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6069
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6146
2025-11-14 21:29:18,064 - INFO - Fold 5 Train Epoch 7/200, Train Loss: 7.6343, Pearson Mean: 0.6084, Spearman Mean: 0.5732
2025-11-14 21:29:18,064 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3882, 'spearman_mean_genewise': 0.3511, 'l1_error_mean': 1.9652, 'l2_errors_mean': 7.6344, 'r2_scores_mean': 0.1624, 'pearson_std': 0.1148, 'l2_error_q1': 4.9433, 'l2_error_q2': 7.1636, 'l2_error_q3': 10.162, 'r2_score_q1': 0.0916, 'r2_score_q2': 0.1372, 'r2_score_q3': 0.2036, 'mape_mean': 61.7727, 'mape_std': 17.9685, 'rmse_mean': 2.7094, 'rmse_std': 0.5416}
2025-11-14 21:29:18,436 - INFO - Fold 5 Val Epoch 7/200, Batch 0, Loss: 7.3645, Pearson: 0.4465, Spearman: 0.4541
2025-11-14 21:29:20,091 - INFO - Fold 5 Val Epoch 7/200, Batch 10, Loss: 7.8585, Pearson: 0.6196, Spearman: 0.6125
2025-11-14 21:29:21,682 - INFO - Fold 5 Val Epoch 7/200, Batch 20, Loss: 9.2539, Pearson: 0.5330, Spearman: 0.5430
2025-11-14 21:29:24,027 - INFO - Fold 5 Val Epoch 7/200, Val Loss: 8.4021, Pearson Mean: 0.5518, Spearman Mean: 0.5468
2025-11-14 21:29:24,028 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3012, 'spearman_mean_genewise': 0.2721, 'l1_error_mean': 2.192, 'l2_errors_mean': 8.3797, 'r2_scores_mean': 0.0879, 'pearson_std': 0.1003, 'l2_error_q1': 5.6626, 'l2_error_q2': 8.0643, 'l2_error_q3': 11.021, 'r2_score_q1': 0.0437, 'r2_score_q2': 0.0719, 'r2_score_q3': 0.1136, 'mape_mean': 62.1235, 'mape_std': 18.7956, 'rmse_mean': 2.8437, 'rmse_std': 0.5413}
2025-11-14 21:29:24,028 - INFO - Learning rate for epoch 7: 0.0001
2025-11-14 21:29:24,092 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_5/best_model.pth
2025-11-14 21:29:25,022 - INFO - Fold 5 Train Epoch 8/200, Batch 0, Loss: 7.6969, Pearson: 0.6146, Spearman: 0.5791
2025-11-14 21:29:34,888 - INFO - Fold 5 Train Epoch 8/200, Batch 10, Loss: 7.5284, Pearson: 0.6206, Spearman: 0.5780
2025-11-14 21:29:45,003 - INFO - Fold 5 Train Epoch 8/200, Batch 20, Loss: 7.4942, Pearson: 0.6134, Spearman: 0.5717
2025-11-14 21:29:55,126 - INFO - Fold 5 Train Epoch 8/200, Batch 30, Loss: 7.5614, Pearson: 0.6142, Spearman: 0.5814
2025-11-14 21:30:05,247 - INFO - Fold 5 Train Epoch 8/200, Batch 40, Loss: 7.5537, Pearson: 0.6070, Spearman: 0.5753
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6054
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6097
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6140
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6098
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.634363174438477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 8 =====================
Sample y_true values (first sample, first 10 genes):
[0.       7.597412 0.       0.       0.       8.69569  0.       0.
 0.       0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.8538747  1.3661116  2.9582872  0.35389334 3.802166   3.4125752
 0.7274028  1.7668716  3.254685   4.8922215 ]
y_true  -> mean=2.1131, std=3.5141, min=0.0000, max=12.9766
y_pred  -> mean=2.0218, std=2.0993, min=0.0000, max=14.0528
Batch 0 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6029
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.5990
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.5986
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6095
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6080
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6047
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.5957
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6070
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6116
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6148
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6060
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6093
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6173
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6067
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6085
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6152
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6077
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6097
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.5972
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6007
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6204
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6084
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6070
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6051
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6091
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6073
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6232
2025-11-14 21:30:15,024 - INFO - Fold 5 Train Epoch 8/200, Batch 50, Loss: 7.4029, Pearson: 0.6263, Spearman: 0.5864
2025-11-14 21:30:22,017 - INFO - Fold 5 Train Epoch 8/200, Batch 60, Loss: 7.4863, Pearson: 0.6039, Spearman: 0.5723
2025-11-14 21:30:31,998 - INFO - Fold 5 Train Epoch 8/200, Batch 70, Loss: 7.5091, Pearson: 0.6055, Spearman: 0.5716
2025-11-14 21:30:42,124 - INFO - Fold 5 Train Epoch 8/200, Batch 80, Loss: 7.5185, Pearson: 0.6148, Spearman: 0.5802
2025-11-14 21:30:48,379 - INFO - Fold 5 Train Epoch 8/200, Train Loss: 7.5852, Pearson Mean: 0.6117, Spearman Mean: 0.5763
2025-11-14 21:30:48,380 - INFO - Training Metrics: {'pearson_mean_genewise': 0.3932, 'spearman_mean_genewise': 0.356, 'l1_error_mean': 1.9448, 'l2_errors_mean': 7.5851, 'r2_scores_mean': 0.1667, 'pearson_std': 0.1156, 'l2_error_q1': 4.9216, 'l2_error_q2': 7.1205, 'l2_error_q3': 10.1026, 'r2_score_q1': 0.0956, 'r2_score_q2': 0.1424, 'r2_score_q3': 0.2082, 'mape_mean': 61.4641, 'mape_std': 18.0998, 'rmse_mean': 2.7011, 'rmse_std': 0.5376}
2025-11-14 21:30:48,740 - INFO - Fold 5 Val Epoch 8/200, Batch 0, Loss: 7.4453, Pearson: 0.4558, Spearman: 0.4611
2025-11-14 21:30:50,366 - INFO - Fold 5 Val Epoch 8/200, Batch 10, Loss: 7.8843, Pearson: 0.6214, Spearman: 0.6132
2025-11-14 21:30:51,861 - INFO - Fold 5 Val Epoch 8/200, Batch 20, Loss: 9.2140, Pearson: 0.5490, Spearman: 0.5579
2025-11-14 21:30:54,154 - INFO - Fold 5 Val Epoch 8/200, Val Loss: 8.4301, Pearson Mean: 0.5573, Spearman Mean: 0.5508
2025-11-14 21:30:54,155 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3046, 'spearman_mean_genewise': 0.277, 'l1_error_mean': 2.1168, 'l2_errors_mean': 8.4094, 'r2_scores_mean': 0.0837, 'pearson_std': 0.1033, 'l2_error_q1': 5.6608, 'l2_error_q2': 8.231, 'l2_error_q3': 11.0592, 'r2_score_q1': 0.0428, 'r2_score_q2': 0.0698, 'r2_score_q3': 0.1065, 'mape_mean': 66.8638, 'mape_std': 18.2677, 'rmse_mean': 2.8494, 'rmse_std': 0.5387}
2025-11-14 21:30:54,155 - INFO - Learning rate for epoch 8: 0.0001
2025-11-14 21:30:54,216 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_5/best_model.pth
2025-11-14 21:30:55,261 - INFO - Fold 5 Train Epoch 9/200, Batch 0, Loss: 7.5777, Pearson: 0.6240, Spearman: 0.5883
2025-11-14 21:31:05,382 - INFO - Fold 5 Train Epoch 9/200, Batch 10, Loss: 7.7450, Pearson: 0.6158, Spearman: 0.5827
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6263
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6072
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6024
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6082
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6039
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.5900
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6136
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6041
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6055
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6153
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6072
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6148
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6048
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.5974
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6189
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.58513879776001
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 9 =====================
Sample y_true values (first sample, first 10 genes):
[6.3770657 7.4745436 0.        6.3770657 7.069362  7.762084  0.
 0.        7.4745436 9.510932 ]
Sample y_pred values (first sample, first 10 genes):
[1.0684528 4.2136617 3.5617557 1.9626993 4.82942   4.332736  1.0774424
 3.0217557 3.3518856 8.132436 ]
y_true  -> mean=2.1526, std=3.5180, min=0.0000, max=12.8135
y_pred  -> mean=2.0331, std=2.1199, min=0.0000, max=13.3488
Batch 0 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6030
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6036
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6183
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6091
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6180
2025-11-14 21:31:15,531 - INFO - Fold 5 Train Epoch 9/200, Batch 20, Loss: 7.6293, Pearson: 0.6128, Spearman: 0.5798
2025-11-14 21:31:25,689 - INFO - Fold 5 Train Epoch 9/200, Batch 30, Loss: 7.5847, Pearson: 0.6172, Spearman: 0.5858
2025-11-14 21:31:35,827 - INFO - Fold 5 Train Epoch 9/200, Batch 40, Loss: 7.5936, Pearson: 0.6225, Spearman: 0.5867
2025-11-14 21:31:44,990 - INFO - Fold 5 Train Epoch 9/200, Batch 50, Loss: 7.5387, Pearson: 0.6173, Spearman: 0.5796
2025-11-14 21:31:52,447 - INFO - Fold 5 Train Epoch 9/200, Batch 60, Loss: 7.3547, Pearson: 0.6168, Spearman: 0.5808
2025-11-14 21:32:02,510 - INFO - Fold 5 Train Epoch 9/200, Batch 70, Loss: 7.6004, Pearson: 0.6042, Spearman: 0.5710
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6122
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6108
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6052
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6066
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6160
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6093
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6173
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6089
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6082
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6102
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6134
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6042
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6152
2025-11-14 21:32:12,634 - INFO - Fold 5 Train Epoch 9/200, Batch 80, Loss: 7.5844, Pearson: 0.6018, Spearman: 0.5717
2025-11-14 21:32:18,923 - INFO - Fold 5 Train Epoch 9/200, Train Loss: 7.5138, Pearson Mean: 0.6163, Spearman Mean: 0.5803
2025-11-14 21:32:18,923 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4007, 'spearman_mean_genewise': 0.3623, 'l1_error_mean': 1.9287, 'l2_errors_mean': 7.5138, 'r2_scores_mean': 0.1731, 'pearson_std': 0.1163, 'l2_error_q1': 4.9163, 'l2_error_q2': 7.0952, 'l2_error_q3': 9.9793, 'r2_score_q1': 0.0995, 'r2_score_q2': 0.1478, 'r2_score_q3': 0.2156, 'mape_mean': 60.9182, 'mape_std': 18.1833, 'rmse_mean': 2.689, 'rmse_std': 0.5318}
2025-11-14 21:32:19,294 - INFO - Fold 5 Val Epoch 9/200, Batch 0, Loss: 7.2167, Pearson: 0.4635, Spearman: 0.4688
2025-11-14 21:32:20,970 - INFO - Fold 5 Val Epoch 9/200, Batch 10, Loss: 7.8873, Pearson: 0.6176, Spearman: 0.6113
2025-11-14 21:32:22,492 - INFO - Fold 5 Val Epoch 9/200, Batch 20, Loss: 8.9373, Pearson: 0.5573, Spearman: 0.5663
2025-11-14 21:32:24,820 - INFO - Fold 5 Val Epoch 9/200, Val Loss: 8.3686, Pearson Mean: 0.5554, Spearman Mean: 0.5513
2025-11-14 21:32:24,820 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2988, 'spearman_mean_genewise': 0.2731, 'l1_error_mean': 2.1474, 'l2_errors_mean': 8.3536, 'r2_scores_mean': 0.0891, 'pearson_std': 0.1026, 'l2_error_q1': 5.661, 'l2_error_q2': 8.0753, 'l2_error_q3': 10.9515, 'r2_score_q1': 0.0432, 'r2_score_q2': 0.0707, 'r2_score_q3': 0.1137, 'mape_mean': 63.8755, 'mape_std': 19.5575, 'rmse_mean': 2.8403, 'rmse_std': 0.5351}
2025-11-14 21:32:24,820 - INFO - Learning rate for epoch 9: 0.0001
2025-11-14 21:32:24,820 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 21:32:25,825 - INFO - Fold 5 Train Epoch 10/200, Batch 0, Loss: 7.3883, Pearson: 0.6234, Spearman: 0.5899
2025-11-14 21:32:35,883 - INFO - Fold 5 Train Epoch 10/200, Batch 10, Loss: 7.7326, Pearson: 0.6126, Spearman: 0.5865
2025-11-14 21:32:45,998 - INFO - Fold 5 Train Epoch 10/200, Batch 20, Loss: 7.3293, Pearson: 0.6201, Spearman: 0.5864
2025-11-14 21:32:56,129 - INFO - Fold 5 Train Epoch 10/200, Batch 30, Loss: 7.3797, Pearson: 0.6223, Spearman: 0.5876
2025-11-14 21:33:06,282 - INFO - Fold 5 Train Epoch 10/200, Batch 40, Loss: 7.3951, Pearson: 0.6278, Spearman: 0.5883
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6105
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6018
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6103
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6262
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6159
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.513790130615234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 10 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       8.318586 0.       0.       0.       0.
 8.318586 0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.62833077 0.         0.9761803  1.5745189  1.6902752  1.3602669
 0.19568998 2.0757575  2.6186328  4.876316  ]
y_true  -> mean=2.0709, std=3.4742, min=0.0000, max=12.6253
y_pred  -> mean=1.9787, std=2.1337, min=0.0000, max=13.7489
Batch 0 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6096
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6126
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6121
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6044
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6041
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6041
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6071
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6109
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6092
2025-11-14 21:33:15,014 - INFO - Fold 5 Train Epoch 10/200, Batch 50, Loss: 7.3261, Pearson: 0.6150, Spearman: 0.5793
2025-11-14 21:33:22,769 - INFO - Fold 5 Train Epoch 10/200, Batch 60, Loss: 7.1169, Pearson: 0.6240, Spearman: 0.5780
2025-11-14 21:33:32,918 - INFO - Fold 5 Train Epoch 10/200, Batch 70, Loss: 7.6509, Pearson: 0.6223, Spearman: 0.5843
2025-11-14 21:33:43,039 - INFO - Fold 5 Train Epoch 10/200, Batch 80, Loss: 7.4468, Pearson: 0.6154, Spearman: 0.5816
2025-11-14 21:33:49,273 - INFO - Fold 5 Train Epoch 10/200, Train Loss: 7.4787, Pearson Mean: 0.6187, Spearman Mean: 0.5827
2025-11-14 21:33:49,273 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4042, 'spearman_mean_genewise': 0.3657, 'l1_error_mean': 1.9205, 'l2_errors_mean': 7.4787, 'r2_scores_mean': 0.1762, 'pearson_std': 0.1171, 'l2_error_q1': 4.895, 'l2_error_q2': 7.0473, 'l2_error_q3': 9.9433, 'r2_score_q1': 0.1002, 'r2_score_q2': 0.1502, 'r2_score_q3': 0.2186, 'mape_mean': 60.75, 'mape_std': 18.2617, 'rmse_mean': 2.683, 'rmse_std': 0.5295}
2025-11-14 21:33:49,572 - INFO - Fold 5 Val Epoch 10/200, Batch 0, Loss: 7.2291, Pearson: 0.4680, Spearman: 0.4744
2025-11-14 21:33:51,252 - INFO - Fold 5 Val Epoch 10/200, Batch 10, Loss: 7.7497, Pearson: 0.6261, Spearman: 0.6167
2025-11-14 21:33:52,880 - INFO - Fold 5 Val Epoch 10/200, Batch 20, Loss: 9.1120, Pearson: 0.5485, Spearman: 0.5570
2025-11-14 21:33:55,179 - INFO - Fold 5 Val Epoch 10/200, Val Loss: 8.3148, Pearson Mean: 0.5604, Spearman Mean: 0.5534
2025-11-14 21:33:55,180 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3087, 'spearman_mean_genewise': 0.2803, 'l1_error_mean': 2.1171, 'l2_errors_mean': 8.2938, 'r2_scores_mean': 0.0954, 'pearson_std': 0.1058, 'l2_error_q1': 5.6098, 'l2_error_q2': 8.0344, 'l2_error_q3': 10.909, 'r2_score_q1': 0.0488, 'r2_score_q2': 0.076, 'r2_score_q3': 0.1186, 'mape_mean': 64.2329, 'mape_std': 19.2643, 'rmse_mean': 2.8301, 'rmse_std': 0.5334}
2025-11-14 21:33:55,180 - INFO - Learning rate for epoch 10: 0.0001
2025-11-14 21:33:55,241 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_5/best_model.pth
2025-11-14 21:33:56,175 - INFO - Fold 5 Train Epoch 11/200, Batch 0, Loss: 7.3770, Pearson: 0.6289, Spearman: 0.5923
2025-11-14 21:34:06,136 - INFO - Fold 5 Train Epoch 11/200, Batch 10, Loss: 7.6566, Pearson: 0.6175, Spearman: 0.5816
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6130
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6111
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6270
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6165
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.5985
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6056
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6101
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6121
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6022
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6240
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6154
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.4786834716796875
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 11 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.398155]
Sample y_pred values (first sample, first 10 genes):
[0.22922015 0.5159749  1.0730271  0.6756017  2.5697398  1.0197386
 0.364739   1.0339656  2.6753938  7.7420373 ]
y_true  -> mean=2.1195, std=3.4922, min=0.0000, max=12.7169
y_pred  -> mean=2.0512, std=2.1843, min=0.0000, max=14.1276
Batch 0 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6107
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6175
2025-11-14 21:34:16,275 - INFO - Fold 5 Train Epoch 11/200, Batch 20, Loss: 7.5153, Pearson: 0.6280, Spearman: 0.5894
2025-11-14 21:34:26,403 - INFO - Fold 5 Train Epoch 11/200, Batch 30, Loss: 7.5092, Pearson: 0.6172, Spearman: 0.5779
2025-11-14 21:34:36,515 - INFO - Fold 5 Train Epoch 11/200, Batch 40, Loss: 7.6125, Pearson: 0.6211, Spearman: 0.5870
2025-11-14 21:34:44,922 - INFO - Fold 5 Train Epoch 11/200, Batch 50, Loss: 7.3513, Pearson: 0.6348, Spearman: 0.5923
2025-11-14 21:34:53,166 - INFO - Fold 5 Train Epoch 11/200, Batch 60, Loss: 7.5073, Pearson: 0.6264, Spearman: 0.5917
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6052
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6159
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6093
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6258
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6172
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6187
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6103
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6176
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6035
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6121
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6246
2025-11-14 21:35:03,331 - INFO - Fold 5 Train Epoch 11/200, Batch 70, Loss: 7.6287, Pearson: 0.6290, Spearman: 0.5850
2025-11-14 21:35:13,450 - INFO - Fold 5 Train Epoch 11/200, Batch 80, Loss: 7.6410, Pearson: 0.6272, Spearman: 0.5869
2025-11-14 21:35:19,809 - INFO - Fold 5 Train Epoch 11/200, Train Loss: 7.4305, Pearson Mean: 0.6219, Spearman Mean: 0.5854
2025-11-14 21:35:19,809 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4092, 'spearman_mean_genewise': 0.37, 'l1_error_mean': 1.9098, 'l2_errors_mean': 7.4305, 'r2_scores_mean': 0.1806, 'pearson_std': 0.1177, 'l2_error_q1': 4.8754, 'l2_error_q2': 7.0242, 'l2_error_q3': 9.8209, 'r2_score_q1': 0.1036, 'r2_score_q2': 0.1552, 'r2_score_q3': 0.2268, 'mape_mean': 60.4257, 'mape_std': 18.2427, 'rmse_mean': 2.6747, 'rmse_std': 0.5257}
2025-11-14 21:35:20,204 - INFO - Fold 5 Val Epoch 11/200, Batch 0, Loss: 7.3380, Pearson: 0.4596, Spearman: 0.4712
2025-11-14 21:35:21,859 - INFO - Fold 5 Val Epoch 11/200, Batch 10, Loss: 7.9891, Pearson: 0.6120, Spearman: 0.6088
2025-11-14 21:35:23,427 - INFO - Fold 5 Val Epoch 11/200, Batch 20, Loss: 9.1337, Pearson: 0.5420, Spearman: 0.5543
2025-11-14 21:35:25,758 - INFO - Fold 5 Val Epoch 11/200, Val Loss: 8.4042, Pearson Mean: 0.5542, Spearman Mean: 0.5485
2025-11-14 21:35:25,758 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3046, 'spearman_mean_genewise': 0.2733, 'l1_error_mean': 2.1637, 'l2_errors_mean': 8.385, 'r2_scores_mean': 0.0869, 'pearson_std': 0.1032, 'l2_error_q1': 5.6756, 'l2_error_q2': 8.1315, 'l2_error_q3': 11.0116, 'r2_score_q1': 0.0401, 'r2_score_q2': 0.0691, 'r2_score_q3': 0.1133, 'mape_mean': 61.5233, 'mape_std': 18.5506, 'rmse_mean': 2.8445, 'rmse_std': 0.542}
2025-11-14 21:35:25,758 - INFO - Learning rate for epoch 11: 0.0001
2025-11-14 21:35:25,758 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 21:35:26,789 - INFO - Fold 5 Train Epoch 12/200, Batch 0, Loss: 7.5475, Pearson: 0.6219, Spearman: 0.5886
2025-11-14 21:35:36,959 - INFO - Fold 5 Train Epoch 12/200, Batch 10, Loss: 7.4614, Pearson: 0.6252, Spearman: 0.5857
2025-11-14 21:35:47,123 - INFO - Fold 5 Train Epoch 12/200, Batch 20, Loss: 7.4562, Pearson: 0.6219, Spearman: 0.5872
2025-11-14 21:35:57,256 - INFO - Fold 5 Train Epoch 12/200, Batch 30, Loss: 7.3246, Pearson: 0.6230, Spearman: 0.5841
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6073
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6192
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6203
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.4305009841918945
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 12 =====================
Sample y_true values (first sample, first 10 genes):
[0.        7.6014023 0.        0.        7.6014023 8.2943    0.
 7.6014023 7.6014023 8.2943   ]
Sample y_pred values (first sample, first 10 genes):
[0.8954201 0.        1.2424502 1.9354221 2.386302  1.8775783 0.7224586
 2.6958134 3.181549  5.709369 ]
y_true  -> mean=2.0985, std=3.5065, min=0.0000, max=12.4503
y_pred  -> mean=2.0102, std=2.1733, min=0.0000, max=13.1599
Batch 0 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6215
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6083
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6095
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6219
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6209
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6204
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6204
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6156
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6117
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6205
2025-11-14 21:36:07,406 - INFO - Fold 5 Train Epoch 12/200, Batch 40, Loss: 7.2308, Pearson: 0.6251, Spearman: 0.5920
2025-11-14 21:36:14,483 - INFO - Fold 5 Train Epoch 12/200, Batch 50, Loss: 7.3219, Pearson: 0.6178, Spearman: 0.5765
2025-11-14 21:36:23,888 - INFO - Fold 5 Train Epoch 12/200, Batch 60, Loss: 7.2830, Pearson: 0.6249, Spearman: 0.5852
2025-11-14 21:36:34,028 - INFO - Fold 5 Train Epoch 12/200, Batch 70, Loss: 7.4080, Pearson: 0.6266, Spearman: 0.5869
2025-11-14 21:36:44,160 - INFO - Fold 5 Train Epoch 12/200, Batch 80, Loss: 7.7375, Pearson: 0.6006, Spearman: 0.5756
2025-11-14 21:36:50,410 - INFO - Fold 5 Train Epoch 12/200, Train Loss: 7.4228, Pearson Mean: 0.6224, Spearman Mean: 0.5862
2025-11-14 21:36:50,410 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4106, 'spearman_mean_genewise': 0.3711, 'l1_error_mean': 1.9057, 'l2_errors_mean': 7.4226, 'r2_scores_mean': 0.1816, 'pearson_std': 0.1173, 'l2_error_q1': 4.8674, 'l2_error_q2': 6.9886, 'l2_error_q3': 9.7466, 'r2_score_q1': 0.1052, 'r2_score_q2': 0.1556, 'r2_score_q3': 0.227, 'mape_mean': 60.4667, 'mape_std': 18.2738, 'rmse_mean': 2.6733, 'rmse_std': 0.5256}
2025-11-14 21:36:50,756 - INFO - Fold 5 Val Epoch 12/200, Batch 0, Loss: 7.2915, Pearson: 0.4673, Spearman: 0.4756
2025-11-14 21:36:52,358 - INFO - Fold 5 Val Epoch 12/200, Batch 10, Loss: 7.9920, Pearson: 0.6108, Spearman: 0.6074
2025-11-14 21:36:53,833 - INFO - Fold 5 Val Epoch 12/200, Batch 20, Loss: 8.9545, Pearson: 0.5541, Spearman: 0.5622
2025-11-14 21:36:56,190 - INFO - Fold 5 Val Epoch 12/200, Val Loss: 8.4145, Pearson Mean: 0.5545, Spearman Mean: 0.5492
2025-11-14 21:36:56,191 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2914, 'spearman_mean_genewise': 0.2658, 'l1_error_mean': 2.1854, 'l2_errors_mean': 8.4003, 'r2_scores_mean': 0.0843, 'pearson_std': 0.1018, 'l2_error_q1': 5.6685, 'l2_error_q2': 8.1492, 'l2_error_q3': 10.9896, 'r2_score_q1': 0.0405, 'r2_score_q2': 0.0681, 'r2_score_q3': 0.1068, 'mape_mean': 63.6142, 'mape_std': 20.1554, 'rmse_mean': 2.8483, 'rmse_std': 0.5361}
2025-11-14 21:36:56,191 - INFO - Learning rate for epoch 12: 0.0001
2025-11-14 21:36:56,191 - INFO - No improvement in spearman genewise. Patience: 2/30
2025-11-14 21:36:57,114 - INFO - Fold 5 Train Epoch 13/200, Batch 0, Loss: 7.4043, Pearson: 0.6189, Spearman: 0.5864
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6155
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6264
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6300
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6178
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6082
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6212
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6050
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6285
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6104
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6223
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6006
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6264
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6157
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.42259407043457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 13 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        7.5679846 0.        0.        0.
 7.5679846 0.        0.       ]
Sample y_pred values (first sample, first 10 genes):
[1.0314265 0.        1.4086754 1.9464138 2.3958426 2.0650525 0.7997289
 2.818008  3.2490103 5.5745115]
y_true  -> mean=2.0298, std=3.4639, min=0.0000, max=12.6231
y_pred  -> mean=2.0397, std=2.1817, min=0.0000, max=14.0448
Batch 0 Pearson correlation: 0.6189
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6227
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6284
2025-11-14 21:37:07,200 - INFO - Fold 5 Train Epoch 13/200, Batch 10, Loss: 7.3869, Pearson: 0.6206, Spearman: 0.5869
2025-11-14 21:37:17,344 - INFO - Fold 5 Train Epoch 13/200, Batch 20, Loss: 7.4706, Pearson: 0.6162, Spearman: 0.5817
2025-11-14 21:37:27,471 - INFO - Fold 5 Train Epoch 13/200, Batch 30, Loss: 7.4335, Pearson: 0.6245, Spearman: 0.5908
2025-11-14 21:37:36,884 - INFO - Fold 5 Train Epoch 13/200, Batch 40, Loss: 7.4544, Pearson: 0.6184, Spearman: 0.5857
2025-11-14 21:37:44,007 - INFO - Fold 5 Train Epoch 13/200, Batch 50, Loss: 7.2074, Pearson: 0.6157, Spearman: 0.5873
2025-11-14 21:37:54,085 - INFO - Fold 5 Train Epoch 13/200, Batch 60, Loss: 7.3835, Pearson: 0.6294, Spearman: 0.5844
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6158
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6243
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6151
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6094
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6162
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6188
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6204
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6213
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6112
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6100
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6249
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6391
2025-11-14 21:38:04,203 - INFO - Fold 5 Train Epoch 13/200, Batch 70, Loss: 7.4000, Pearson: 0.6184, Spearman: 0.5834
2025-11-14 21:38:14,345 - INFO - Fold 5 Train Epoch 13/200, Batch 80, Loss: 7.6092, Pearson: 0.6180, Spearman: 0.5843
2025-11-14 21:38:20,604 - INFO - Fold 5 Train Epoch 13/200, Train Loss: 7.4276, Pearson Mean: 0.6220, Spearman Mean: 0.5866
2025-11-14 21:38:20,604 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4103, 'spearman_mean_genewise': 0.3712, 'l1_error_mean': 1.9053, 'l2_errors_mean': 7.4276, 'r2_scores_mean': 0.1813, 'pearson_std': 0.1173, 'l2_error_q1': 4.8691, 'l2_error_q2': 6.9974, 'l2_error_q3': 9.8041, 'r2_score_q1': 0.1054, 'r2_score_q2': 0.1556, 'r2_score_q3': 0.2273, 'mape_mean': 60.3896, 'mape_std': 18.2011, 'rmse_mean': 2.674, 'rmse_std': 0.5267}
2025-11-14 21:38:21,002 - INFO - Fold 5 Val Epoch 13/200, Batch 0, Loss: 7.0564, Pearson: 0.4877, Spearman: 0.4886
2025-11-14 21:38:22,705 - INFO - Fold 5 Val Epoch 13/200, Batch 10, Loss: 7.9276, Pearson: 0.6153, Spearman: 0.6123
2025-11-14 21:38:24,315 - INFO - Fold 5 Val Epoch 13/200, Batch 20, Loss: 9.1790, Pearson: 0.5419, Spearman: 0.5545
2025-11-14 21:38:26,641 - INFO - Fold 5 Val Epoch 13/200, Val Loss: 8.3630, Pearson Mean: 0.5572, Spearman Mean: 0.5518
2025-11-14 21:38:26,642 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3062, 'spearman_mean_genewise': 0.2774, 'l1_error_mean': 2.1134, 'l2_errors_mean': 8.3415, 'r2_scores_mean': 0.0913, 'pearson_std': 0.1043, 'l2_error_q1': 5.6508, 'l2_error_q2': 8.0138, 'l2_error_q3': 10.977, 'r2_score_q1': 0.0455, 'r2_score_q2': 0.073, 'r2_score_q3': 0.1162, 'mape_mean': 64.0961, 'mape_std': 18.8851, 'rmse_mean': 2.8376, 'rmse_std': 0.5378}
2025-11-14 21:38:26,642 - INFO - Learning rate for epoch 13: 0.0001
2025-11-14 21:38:26,642 - INFO - No improvement in spearman genewise. Patience: 3/30
2025-11-14 21:38:27,548 - INFO - Fold 5 Train Epoch 14/200, Batch 0, Loss: 7.4588, Pearson: 0.6278, Spearman: 0.5938
2025-11-14 21:38:37,174 - INFO - Fold 5 Train Epoch 14/200, Batch 10, Loss: 7.3106, Pearson: 0.6197, Spearman: 0.5822
2025-11-14 21:38:47,277 - INFO - Fold 5 Train Epoch 14/200, Batch 20, Loss: 7.3503, Pearson: 0.6239, Spearman: 0.5919
2025-11-14 21:38:57,412 - INFO - Fold 5 Train Epoch 14/200, Batch 30, Loss: 7.2040, Pearson: 0.6279, Spearman: 0.5895
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6142
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6065
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6110
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6280
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6180
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6149
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6143
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6133
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.427605152130127
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 14 =====================
Sample y_true values (first sample, first 10 genes):
[6.998594  6.3063593 6.3063593 0.        7.403754  6.998594  0.
 0.        7.403754  9.482663 ]
Sample y_pred values (first sample, first 10 genes):
[1.6466181 4.6393847 4.355747  2.84132   6.0342383 5.4336534 1.1810207
 4.544734  4.6423683 9.187012 ]
y_true  -> mean=2.1324, std=3.5060, min=0.0000, max=12.5628
y_pred  -> mean=2.0255, std=2.1759, min=0.0000, max=13.8643
Batch 0 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6277
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6194
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6132
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6279
2025-11-14 21:39:06,394 - INFO - Fold 5 Train Epoch 14/200, Batch 40, Loss: 7.2998, Pearson: 0.6185, Spearman: 0.5900
2025-11-14 21:39:14,056 - INFO - Fold 5 Train Epoch 14/200, Batch 50, Loss: 7.4509, Pearson: 0.6274, Spearman: 0.5908
2025-11-14 21:39:24,183 - INFO - Fold 5 Train Epoch 14/200, Batch 60, Loss: 7.5146, Pearson: 0.6259, Spearman: 0.5846
2025-11-14 21:39:34,340 - INFO - Fold 5 Train Epoch 14/200, Batch 70, Loss: 7.2962, Pearson: 0.6181, Spearman: 0.5847
2025-11-14 21:39:44,454 - INFO - Fold 5 Train Epoch 14/200, Batch 80, Loss: 7.4885, Pearson: 0.6175, Spearman: 0.5867
2025-11-14 21:39:50,719 - INFO - Fold 5 Train Epoch 14/200, Train Loss: 7.4039, Pearson Mean: 0.6237, Spearman Mean: 0.5883
2025-11-14 21:39:50,719 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4127, 'spearman_mean_genewise': 0.373, 'l1_error_mean': 1.8978, 'l2_errors_mean': 7.4039, 'r2_scores_mean': 0.1834, 'pearson_std': 0.1172, 'l2_error_q1': 4.8715, 'l2_error_q2': 6.9977, 'l2_error_q3': 9.7245, 'r2_score_q1': 0.1064, 'r2_score_q2': 0.1576, 'r2_score_q3': 0.2301, 'mape_mean': 60.357, 'mape_std': 18.2849, 'rmse_mean': 2.67, 'rmse_std': 0.5245}
2025-11-14 21:39:51,090 - INFO - Fold 5 Val Epoch 14/200, Batch 0, Loss: 7.2170, Pearson: 0.4657, Spearman: 0.4706
2025-11-14 21:39:52,748 - INFO - Fold 5 Val Epoch 14/200, Batch 10, Loss: 8.5676, Pearson: 0.5775, Spearman: 0.5742
2025-11-14 21:39:54,332 - INFO - Fold 5 Val Epoch 14/200, Batch 20, Loss: 9.0376, Pearson: 0.5486, Spearman: 0.5597
2025-11-14 21:39:56,671 - INFO - Fold 5 Val Epoch 14/200, Val Loss: 8.7109, Pearson Mean: 0.5351, Spearman Mean: 0.5335
2025-11-14 21:39:56,672 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.2611, 'spearman_mean_genewise': 0.2448, 'l1_error_mean': 2.237, 'l2_errors_mean': 8.7022, 'r2_scores_mean': 0.0546, 'pearson_std': 0.0986, 'l2_error_q1': 5.8072, 'l2_error_q2': 8.4667, 'l2_error_q3': 11.4202, 'r2_score_q1': 0.025, 'r2_score_q2': 0.0495, 'r2_score_q3': 0.083, 'mape_mean': 66.7994, 'mape_std': 19.058, 'rmse_mean': 2.8962, 'rmse_std': 0.5605}
2025-11-14 21:39:56,672 - INFO - Learning rate for epoch 14: 0.0001
2025-11-14 21:39:56,672 - INFO - No improvement in spearman genewise. Patience: 4/30
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6143
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6257
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6144
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6185
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6246
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6197
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6148
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6120
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6140
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6131
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6184
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6198
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6157
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6225
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6154
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6217
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6175
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6191
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6170
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6176
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6219
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.403854846954346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 15 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
2025-11-14 21:39:57,570 - INFO - Fold 5 Train Epoch 15/200, Batch 0, Loss: 7.5543, Pearson: 0.6267, Spearman: 0.5913
2025-11-14 21:40:07,522 - INFO - Fold 5 Train Epoch 15/200, Batch 10, Loss: 7.4432, Pearson: 0.6276, Spearman: 0.5927
2025-11-14 21:40:17,668 - INFO - Fold 5 Train Epoch 15/200, Batch 20, Loss: 7.7629, Pearson: 0.6145, Spearman: 0.5866
2025-11-14 21:40:27,800 - INFO - Fold 5 Train Epoch 15/200, Batch 30, Loss: 7.7017, Pearson: 0.6130, Spearman: 0.5845
2025-11-14 21:40:36,312 - INFO - Fold 5 Train Epoch 15/200, Batch 40, Loss: 7.4058, Pearson: 0.6232, Spearman: 0.5849
2025-11-14 21:40:42,621 - INFO - Fold 5 Train Epoch 15/200, Batch 50, Loss: 7.1070, Pearson: 0.6385, Spearman: 0.5940
[0.90160215 1.2246957  1.8671026  0.44148025 2.2216487  3.675251
 0.30019164 0.8799365  1.6481516  3.7074876 ]
y_true  -> mean=2.1877, std=3.5221, min=0.0000, max=13.1224
y_pred  -> mean=2.0440, std=2.1800, min=0.0000, max=13.6641
Batch 0 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6241
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6186
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6203
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6156
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6234
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6192
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6226
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6218
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6146
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6145
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6337
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6130
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6214
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6092
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6282
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6199
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6231
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6232
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6137
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6181
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6240
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6138
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6207
2025-11-14 21:40:48,742 - INFO - Fold 5 Train Epoch 15/200, Batch 60, Loss: 7.6261, Pearson: 0.6125, Spearman: 0.5850
2025-11-14 21:40:54,831 - INFO - Fold 5 Train Epoch 15/200, Batch 70, Loss: 7.3284, Pearson: 0.6201, Spearman: 0.5856
2025-11-14 21:41:03,202 - INFO - Fold 5 Train Epoch 15/200, Batch 80, Loss: 7.2496, Pearson: 0.6267, Spearman: 0.5927
2025-11-14 21:41:09,344 - INFO - Fold 5 Train Epoch 15/200, Train Loss: 7.3995, Pearson Mean: 0.6240, Spearman Mean: 0.5889
2025-11-14 21:41:09,344 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4125, 'spearman_mean_genewise': 0.3726, 'l1_error_mean': 1.8922, 'l2_errors_mean': 7.3994, 'r2_scores_mean': 0.1833, 'pearson_std': 0.1187, 'l2_error_q1': 4.8246, 'l2_error_q2': 6.9795, 'l2_error_q3': 9.6809, 'r2_score_q1': 0.106, 'r2_score_q2': 0.1555, 'r2_score_q3': 0.2298, 'mape_mean': 60.437, 'mape_std': 18.0683, 'rmse_mean': 2.6691, 'rmse_std': 0.5248}
2025-11-14 21:41:09,675 - INFO - Fold 5 Val Epoch 15/200, Batch 0, Loss: 7.2397, Pearson: 0.4708, Spearman: 0.4788
2025-11-14 21:41:11,351 - INFO - Fold 5 Val Epoch 15/200, Batch 10, Loss: 7.8082, Pearson: 0.6228, Spearman: 0.6167
2025-11-14 21:41:12,956 - INFO - Fold 5 Val Epoch 15/200, Batch 20, Loss: 9.0808, Pearson: 0.5512, Spearman: 0.5607
2025-11-14 21:41:15,281 - INFO - Fold 5 Val Epoch 15/200, Val Loss: 8.3486, Pearson Mean: 0.5594, Spearman Mean: 0.5536
2025-11-14 21:41:15,282 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3052, 'spearman_mean_genewise': 0.2773, 'l1_error_mean': 2.0896, 'l2_errors_mean': 8.3293, 'r2_scores_mean': 0.0912, 'pearson_std': 0.1085, 'l2_error_q1': 5.6378, 'l2_error_q2': 8.1234, 'l2_error_q3': 10.9294, 'r2_score_q1': 0.0434, 'r2_score_q2': 0.072, 'r2_score_q3': 0.1131, 'mape_mean': 64.3596, 'mape_std': 19.1466, 'rmse_mean': 2.8362, 'rmse_std': 0.5342}
2025-11-14 21:41:15,282 - INFO - Learning rate for epoch 15: 0.0001
2025-11-14 21:41:15,282 - INFO - No improvement in spearman genewise. Patience: 5/30
2025-11-14 21:41:16,210 - INFO - Fold 5 Train Epoch 16/200, Batch 0, Loss: 7.0258, Pearson: 0.6478, Spearman: 0.6032
2025-11-14 21:41:26,238 - INFO - Fold 5 Train Epoch 16/200, Batch 10, Loss: 7.5662, Pearson: 0.6196, Spearman: 0.5927
2025-11-14 21:41:35,700 - INFO - Fold 5 Train Epoch 16/200, Batch 20, Loss: 7.0834, Pearson: 0.6334, Spearman: 0.5974
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6318
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6125
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6275
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6266
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6251
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6296
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6195
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6235
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6201
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6229
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6267
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6137
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6260
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.399388790130615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 16 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        8.163306  7.0652637 7.7579837 7.0652637 0.
 0.        0.        9.36708  ]
Sample y_pred values (first sample, first 10 genes):
[0.         1.5748255  2.103765   0.62027264 4.4814973  2.6826508
 0.09200768 1.6071413  2.8453984  9.207525  ]
y_true  -> mean=2.1043, std=3.4785, min=0.0000, max=12.5633
y_pred  -> mean=2.0516, std=2.2517, min=0.0000, max=13.8546
Batch 0 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6228
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6271
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6297
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6208
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6310
2025-11-14 21:41:45,185 - INFO - Fold 5 Train Epoch 16/200, Batch 30, Loss: 7.1808, Pearson: 0.6393, Spearman: 0.6003
2025-11-14 21:41:54,519 - INFO - Fold 5 Train Epoch 16/200, Batch 40, Loss: 7.4478, Pearson: 0.6260, Spearman: 0.5916
2025-11-14 21:42:03,878 - INFO - Fold 5 Train Epoch 16/200, Batch 50, Loss: 7.1681, Pearson: 0.6339, Spearman: 0.5861
2025-11-14 21:42:13,214 - INFO - Fold 5 Train Epoch 16/200, Batch 60, Loss: 7.3412, Pearson: 0.6245, Spearman: 0.5932
2025-11-14 21:42:21,151 - INFO - Fold 5 Train Epoch 16/200, Batch 70, Loss: 7.3161, Pearson: 0.6288, Spearman: 0.5990
2025-11-14 21:42:29,803 - INFO - Fold 5 Train Epoch 16/200, Batch 80, Loss: 7.2300, Pearson: 0.6286, Spearman: 0.5964
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6216
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6331
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6295
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6222
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6279
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6303
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6345
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6245
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6095
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6253
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6220
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6288
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6236
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6298
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6190
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6302
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6322
2025-11-14 21:42:36,056 - INFO - Fold 5 Train Epoch 16/200, Train Loss: 7.2929, Pearson Mean: 0.6309, Spearman Mean: 0.5945
2025-11-14 21:42:36,056 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4234, 'spearman_mean_genewise': 0.3817, 'l1_error_mean': 1.8835, 'l2_errors_mean': 7.293, 'r2_scores_mean': 0.1932, 'pearson_std': 0.1194, 'l2_error_q1': 4.759, 'l2_error_q2': 6.8758, 'l2_error_q3': 9.5371, 'r2_score_q1': 0.1126, 'r2_score_q2': 0.166, 'r2_score_q3': 0.2435, 'mape_mean': 59.5571, 'mape_std': 18.4153, 'rmse_mean': 2.6505, 'rmse_std': 0.5176}
2025-11-14 21:42:36,419 - INFO - Fold 5 Val Epoch 16/200, Batch 0, Loss: 7.0832, Pearson: 0.4855, Spearman: 0.4876
2025-11-14 21:42:38,194 - INFO - Fold 5 Val Epoch 16/200, Batch 10, Loss: 7.7528, Pearson: 0.6265, Spearman: 0.6174
2025-11-14 21:42:39,767 - INFO - Fold 5 Val Epoch 16/200, Batch 20, Loss: 9.1607, Pearson: 0.5495, Spearman: 0.5582
2025-11-14 21:42:42,119 - INFO - Fold 5 Val Epoch 16/200, Val Loss: 8.3109, Pearson Mean: 0.5625, Spearman Mean: 0.5553
2025-11-14 21:42:42,120 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3113, 'spearman_mean_genewise': 0.2818, 'l1_error_mean': 2.0806, 'l2_errors_mean': 8.2885, 'r2_scores_mean': 0.0953, 'pearson_std': 0.1089, 'l2_error_q1': 5.6368, 'l2_error_q2': 8.022, 'l2_error_q3': 10.8995, 'r2_score_q1': 0.0466, 'r2_score_q2': 0.0752, 'r2_score_q3': 0.1176, 'mape_mean': 64.5677, 'mape_std': 18.9583, 'rmse_mean': 2.8294, 'rmse_std': 0.5319}
2025-11-14 21:42:42,120 - INFO - Learning rate for epoch 16: 0.0001
2025-11-14 21:42:42,184 - INFO - Saved best model to /home/puneet/maninder/code_model_training/modelResults/her2st_stnet_pretrained_true_amt_224/result_vit_2025-11-14_16-48-59/fold_5/best_model.pth
2025-11-14 21:42:43,227 - INFO - Fold 5 Train Epoch 17/200, Batch 0, Loss: 7.1067, Pearson: 0.6299, Spearman: 0.5897
2025-11-14 21:42:53,356 - INFO - Fold 5 Train Epoch 17/200, Batch 10, Loss: 7.2603, Pearson: 0.6306, Spearman: 0.5995
2025-11-14 21:43:03,470 - INFO - Fold 5 Train Epoch 17/200, Batch 20, Loss: 7.1853, Pearson: 0.6356, Spearman: 0.5969
2025-11-14 21:43:13,603 - INFO - Fold 5 Train Epoch 17/200, Batch 30, Loss: 7.2795, Pearson: 0.6375, Spearman: 0.6027
2025-11-14 21:43:23,729 - INFO - Fold 5 Train Epoch 17/200, Batch 40, Loss: 7.1615, Pearson: 0.6313, Spearman: 0.5998
2025-11-14 21:43:33,848 - INFO - Fold 5 Train Epoch 17/200, Batch 50, Loss: 7.6170, Pearson: 0.6338, Spearman: 0.5974
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6262
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6198
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6311
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.292957782745361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 17 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 9.525125 0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.70878613 0.         0.7313747  0.
 0.         0.25047725 0.8192501  5.16683   ]
y_true  -> mean=1.9545, std=3.4300, min=0.0000, max=12.5794
y_pred  -> mean=2.0301, std=2.2257, min=0.0000, max=13.6603
Batch 0 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6306
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6247
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6196
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6311
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6179
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6338
2025-11-14 21:43:43,946 - INFO - Fold 5 Train Epoch 17/200, Batch 60, Loss: 7.1777, Pearson: 0.6362, Spearman: 0.5962
2025-11-14 21:43:51,846 - INFO - Fold 5 Train Epoch 17/200, Batch 70, Loss: 7.2546, Pearson: 0.6418, Spearman: 0.6063
2025-11-14 21:44:00,342 - INFO - Fold 5 Train Epoch 17/200, Batch 80, Loss: 7.4192, Pearson: 0.6319, Spearman: 0.5977
2025-11-14 21:44:06,611 - INFO - Fold 5 Train Epoch 17/200, Train Loss: 7.2435, Pearson Mean: 0.6341, Spearman Mean: 0.5974
2025-11-14 21:44:06,611 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4292, 'spearman_mean_genewise': 0.3864, 'l1_error_mean': 1.87, 'l2_errors_mean': 7.2434, 'r2_scores_mean': 0.198, 'pearson_std': 0.119, 'l2_error_q1': 4.7302, 'l2_error_q2': 6.8383, 'l2_error_q3': 9.4342, 'r2_score_q1': 0.1183, 'r2_score_q2': 0.1689, 'r2_score_q3': 0.2471, 'mape_mean': 59.5045, 'mape_std': 18.3606, 'rmse_mean': 2.6417, 'rmse_std': 0.5145}
2025-11-14 21:44:06,983 - INFO - Fold 5 Val Epoch 17/200, Batch 0, Loss: 7.1112, Pearson: 0.4791, Spearman: 0.4849
2025-11-14 21:44:08,724 - INFO - Fold 5 Val Epoch 17/200, Batch 10, Loss: 7.8081, Pearson: 0.6224, Spearman: 0.6142
2025-11-14 21:44:10,320 - INFO - Fold 5 Val Epoch 17/200, Batch 20, Loss: 9.0324, Pearson: 0.5494, Spearman: 0.5582
2025-11-14 21:44:12,645 - INFO - Fold 5 Val Epoch 17/200, Val Loss: 8.3150, Pearson Mean: 0.5601, Spearman Mean: 0.5528
2025-11-14 21:44:12,645 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3077, 'spearman_mean_genewise': 0.2794, 'l1_error_mean': 2.137, 'l2_errors_mean': 8.2961, 'r2_scores_mean': 0.0948, 'pearson_std': 0.1063, 'l2_error_q1': 5.6309, 'l2_error_q2': 8.032, 'l2_error_q3': 10.8935, 'r2_score_q1': 0.0455, 'r2_score_q2': 0.0756, 'r2_score_q3': 0.1198, 'mape_mean': 62.5766, 'mape_std': 19.3169, 'rmse_mean': 2.8306, 'rmse_std': 0.5326}
2025-11-14 21:44:12,646 - INFO - Learning rate for epoch 17: 0.0001
2025-11-14 21:44:12,646 - INFO - No improvement in spearman genewise. Patience: 1/30
2025-11-14 21:44:13,696 - INFO - Fold 5 Train Epoch 18/200, Batch 0, Loss: 7.1646, Pearson: 0.6399, Spearman: 0.6023
2025-11-14 21:44:23,787 - INFO - Fold 5 Train Epoch 18/200, Batch 10, Loss: 7.2770, Pearson: 0.6260, Spearman: 0.5958
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6294
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6273
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6326
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6291
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6352
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6233
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6313
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6308
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6252
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6239
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6337
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6306
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.243393898010254
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 18 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.38629583 0.29035026 1.4197239  0.53167045
 0.         0.22914672 1.4227625  5.5413337 ]
y_true  -> mean=2.0840, std=3.4827, min=0.0000, max=12.7196
y_pred  -> mean=2.0501, std=2.1914, min=0.0000, max=13.4816
Batch 0 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6321
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6281
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6486
2025-11-14 21:44:33,932 - INFO - Fold 5 Train Epoch 18/200, Batch 20, Loss: 7.1482, Pearson: 0.6371, Spearman: 0.6038
2025-11-14 21:44:44,014 - INFO - Fold 5 Train Epoch 18/200, Batch 30, Loss: 7.3384, Pearson: 0.6307, Spearman: 0.6059
2025-11-14 21:44:54,131 - INFO - Fold 5 Train Epoch 18/200, Batch 40, Loss: 7.2403, Pearson: 0.6414, Spearman: 0.6083
2025-11-14 21:45:04,245 - INFO - Fold 5 Train Epoch 18/200, Batch 50, Loss: 7.1436, Pearson: 0.6299, Spearman: 0.5929
2025-11-14 21:45:14,362 - INFO - Fold 5 Train Epoch 18/200, Batch 60, Loss: 7.0997, Pearson: 0.6193, Spearman: 0.5897
2025-11-14 21:45:22,210 - INFO - Fold 5 Train Epoch 18/200, Batch 70, Loss: 7.1416, Pearson: 0.6278, Spearman: 0.5923
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6237
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6269
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6261
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6329
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6248
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6299
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6301
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6323
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6256
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6286
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6330
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6193
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6283
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6284
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6278
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6274
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6310
2025-11-14 21:45:30,337 - INFO - Fold 5 Train Epoch 18/200, Batch 80, Loss: 6.9712, Pearson: 0.6447, Spearman: 0.6006
2025-11-14 21:45:36,626 - INFO - Fold 5 Train Epoch 18/200, Train Loss: 7.2032, Pearson Mean: 0.6367, Spearman Mean: 0.6000
2025-11-14 21:45:36,626 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4337, 'spearman_mean_genewise': 0.3902, 'l1_error_mean': 1.8687, 'l2_errors_mean': 7.2031, 'r2_scores_mean': 0.2019, 'pearson_std': 0.1188, 'l2_error_q1': 4.7136, 'l2_error_q2': 6.8147, 'l2_error_q3': 9.3885, 'r2_score_q1': 0.1217, 'r2_score_q2': 0.173, 'r2_score_q3': 0.2517, 'mape_mean': 59.191, 'mape_std': 18.4143, 'rmse_mean': 2.6346, 'rmse_std': 0.5119}
2025-11-14 21:45:36,982 - INFO - Fold 5 Val Epoch 18/200, Batch 0, Loss: 7.0331, Pearson: 0.4873, Spearman: 0.4891
2025-11-14 21:45:38,670 - INFO - Fold 5 Val Epoch 18/200, Batch 10, Loss: 7.8100, Pearson: 0.6227, Spearman: 0.6147
2025-11-14 21:45:40,277 - INFO - Fold 5 Val Epoch 18/200, Batch 20, Loss: 9.1673, Pearson: 0.5457, Spearman: 0.5558
2025-11-14 21:45:42,641 - INFO - Fold 5 Val Epoch 18/200, Val Loss: 8.3241, Pearson Mean: 0.5603, Spearman Mean: 0.5534
2025-11-14 21:45:42,642 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3065, 'spearman_mean_genewise': 0.2778, 'l1_error_mean': 2.1173, 'l2_errors_mean': 8.3019, 'r2_scores_mean': 0.0943, 'pearson_std': 0.1079, 'l2_error_q1': 5.6122, 'l2_error_q2': 8.0378, 'l2_error_q3': 10.8912, 'r2_score_q1': 0.0454, 'r2_score_q2': 0.075, 'r2_score_q3': 0.1168, 'mape_mean': 63.915, 'mape_std': 19.0001, 'rmse_mean': 2.8315, 'rmse_std': 0.5335}
2025-11-14 21:45:42,642 - INFO - Learning rate for epoch 18: 0.0001
2025-11-14 21:45:42,642 - INFO - No improvement in spearman genewise. Patience: 2/30
2025-11-14 21:45:43,521 - INFO - Fold 5 Train Epoch 19/200, Batch 0, Loss: 6.9605, Pearson: 0.6485, Spearman: 0.6031
2025-11-14 21:45:53,291 - INFO - Fold 5 Train Epoch 19/200, Batch 10, Loss: 7.2179, Pearson: 0.6455, Spearman: 0.6107
2025-11-14 21:46:03,451 - INFO - Fold 5 Train Epoch 19/200, Batch 20, Loss: 6.9943, Pearson: 0.6372, Spearman: 0.5965
2025-11-14 21:46:13,561 - INFO - Fold 5 Train Epoch 19/200, Batch 30, Loss: 7.3695, Pearson: 0.6389, Spearman: 0.6011
2025-11-14 21:46:23,675 - INFO - Fold 5 Train Epoch 19/200, Batch 40, Loss: 7.1789, Pearson: 0.6336, Spearman: 0.5961
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6292
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6452
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6326
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.20307731628418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 19 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       8.290309 0.       0.
 0.       7.597412]
Sample y_pred values (first sample, first 10 genes):
[1.0959775  1.4259242  3.7537575  0.10888062 3.8057837  3.9744096
 0.69050616 2.057372   3.2135284  5.136609  ]
y_true  -> mean=2.0810, std=3.4651, min=0.0000, max=12.5403
y_pred  -> mean=2.0227, std=2.2278, min=0.0000, max=13.8027
Batch 0 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6332
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6290
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6317
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6346
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6276
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6289
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6347
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6339
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6308
2025-11-14 21:46:33,820 - INFO - Fold 5 Train Epoch 19/200, Batch 50, Loss: 7.6287, Pearson: 0.6320, Spearman: 0.6028
2025-11-14 21:46:43,925 - INFO - Fold 5 Train Epoch 19/200, Batch 60, Loss: 7.1189, Pearson: 0.6434, Spearman: 0.6070
2025-11-14 21:46:52,195 - INFO - Fold 5 Train Epoch 19/200, Batch 70, Loss: 7.2900, Pearson: 0.6375, Spearman: 0.5949
2025-11-14 21:47:00,860 - INFO - Fold 5 Train Epoch 19/200, Batch 80, Loss: 7.1764, Pearson: 0.6442, Spearman: 0.6115
2025-11-14 21:47:07,076 - INFO - Fold 5 Train Epoch 19/200, Train Loss: 7.1671, Pearson Mean: 0.6390, Spearman Mean: 0.6025
2025-11-14 21:47:07,076 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4379, 'spearman_mean_genewise': 0.3938, 'l1_error_mean': 1.8598, 'l2_errors_mean': 7.1671, 'r2_scores_mean': 0.2055, 'pearson_std': 0.1183, 'l2_error_q1': 4.7002, 'l2_error_q2': 6.7852, 'l2_error_q3': 9.3484, 'r2_score_q1': 0.1249, 'r2_score_q2': 0.1758, 'r2_score_q3': 0.256, 'mape_mean': 59.05, 'mape_std': 18.3565, 'rmse_mean': 2.6281, 'rmse_std': 0.51}
2025-11-14 21:47:07,415 - INFO - Fold 5 Val Epoch 19/200, Batch 0, Loss: 7.0340, Pearson: 0.4856, Spearman: 0.4888
2025-11-14 21:47:09,022 - INFO - Fold 5 Val Epoch 19/200, Batch 10, Loss: 7.8347, Pearson: 0.6213, Spearman: 0.6134
2025-11-14 21:47:10,591 - INFO - Fold 5 Val Epoch 19/200, Batch 20, Loss: 9.0527, Pearson: 0.5477, Spearman: 0.5561
2025-11-14 21:47:12,968 - INFO - Fold 5 Val Epoch 19/200, Val Loss: 8.3232, Pearson Mean: 0.5604, Spearman Mean: 0.5530
2025-11-14 21:47:12,969 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3053, 'spearman_mean_genewise': 0.2764, 'l1_error_mean': 2.1479, 'l2_errors_mean': 8.3039, 'r2_scores_mean': 0.0937, 'pearson_std': 0.108, 'l2_error_q1': 5.6277, 'l2_error_q2': 8.033, 'l2_error_q3': 10.9029, 'r2_score_q1': 0.0432, 'r2_score_q2': 0.0742, 'r2_score_q3': 0.1191, 'mape_mean': 62.2524, 'mape_std': 19.8022, 'rmse_mean': 2.832, 'rmse_std': 0.5328}
2025-11-14 21:47:12,969 - INFO - Learning rate for epoch 19: 0.0001
2025-11-14 21:47:12,969 - INFO - No improvement in spearman genewise. Patience: 3/30
2025-11-14 21:47:13,833 - INFO - Fold 5 Train Epoch 20/200, Batch 0, Loss: 7.3426, Pearson: 0.6336, Spearman: 0.6026
2025-11-14 21:47:23,675 - INFO - Fold 5 Train Epoch 20/200, Batch 10, Loss: 7.4207, Pearson: 0.6357, Spearman: 0.6059
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6260
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6305
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6350
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6347
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6422
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.167083740234375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 20 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.251251]
Sample y_pred values (first sample, first 10 genes):
[0.751719   0.         0.6442933  0.9749526  0.9536779  0.962199
 0.29795513 1.5695186  1.7744753  5.2280617 ]
y_true  -> mean=2.0951, std=3.5015, min=0.0000, max=12.6216
y_pred  -> mean=2.0321, std=2.1890, min=0.0000, max=13.5645
Batch 0 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6365
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6411
2025-11-14 21:47:33,799 - INFO - Fold 5 Train Epoch 20/200, Batch 20, Loss: 6.9746, Pearson: 0.6492, Spearman: 0.6084
2025-11-14 21:47:43,921 - INFO - Fold 5 Train Epoch 20/200, Batch 30, Loss: 7.1025, Pearson: 0.6487, Spearman: 0.6033
2025-11-14 21:47:54,046 - INFO - Fold 5 Train Epoch 20/200, Batch 40, Loss: 7.3079, Pearson: 0.6417, Spearman: 0.6037
2025-11-14 21:48:04,152 - INFO - Fold 5 Train Epoch 20/200, Batch 50, Loss: 7.1673, Pearson: 0.6387, Spearman: 0.6052
2025-11-14 21:48:14,289 - INFO - Fold 5 Train Epoch 20/200, Batch 60, Loss: 7.2744, Pearson: 0.6384, Spearman: 0.6083
2025-11-14 21:48:22,563 - INFO - Fold 5 Train Epoch 20/200, Batch 70, Loss: 7.1402, Pearson: 0.6362, Spearman: 0.6057
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6268
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6304
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6324
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6335
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6250
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6362
2025-11-14 21:48:31,188 - INFO - Fold 5 Train Epoch 20/200, Batch 80, Loss: 7.1492, Pearson: 0.6486, Spearman: 0.6075
2025-11-14 21:48:37,439 - INFO - Fold 5 Train Epoch 20/200, Train Loss: 7.1323, Pearson Mean: 0.6412, Spearman Mean: 0.6049
2025-11-14 21:48:37,439 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4421, 'spearman_mean_genewise': 0.3973, 'l1_error_mean': 1.8538, 'l2_errors_mean': 7.1324, 'r2_scores_mean': 0.209, 'pearson_std': 0.1176, 'l2_error_q1': 4.6826, 'l2_error_q2': 6.7648, 'l2_error_q3': 9.2695, 'r2_score_q1': 0.127, 'r2_score_q2': 0.1801, 'r2_score_q3': 0.2591, 'mape_mean': 58.907, 'mape_std': 18.3549, 'rmse_mean': 2.6218, 'rmse_std': 0.5084}
2025-11-14 21:48:37,819 - INFO - Fold 5 Val Epoch 20/200, Batch 0, Loss: 7.1942, Pearson: 0.4764, Spearman: 0.4818
2025-11-14 21:48:39,506 - INFO - Fold 5 Val Epoch 20/200, Batch 10, Loss: 7.8091, Pearson: 0.6224, Spearman: 0.6152
2025-11-14 21:48:41,045 - INFO - Fold 5 Val Epoch 20/200, Batch 20, Loss: 9.1054, Pearson: 0.5445, Spearman: 0.5550
2025-11-14 21:48:43,389 - INFO - Fold 5 Val Epoch 20/200, Val Loss: 8.3166, Pearson Mean: 0.5607, Spearman Mean: 0.5528
2025-11-14 21:48:43,390 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3057, 'spearman_mean_genewise': 0.2764, 'l1_error_mean': 2.13, 'l2_errors_mean': 8.2958, 'r2_scores_mean': 0.0944, 'pearson_std': 0.1091, 'l2_error_q1': 5.6175, 'l2_error_q2': 8.037, 'l2_error_q3': 10.9249, 'r2_score_q1': 0.0442, 'r2_score_q2': 0.0741, 'r2_score_q3': 0.1164, 'mape_mean': 62.835, 'mape_std': 19.3086, 'rmse_mean': 2.8306, 'rmse_std': 0.5325}
2025-11-14 21:48:43,390 - INFO - Learning rate for epoch 20: 0.0001
2025-11-14 21:48:43,390 - INFO - No improvement in spearman genewise. Patience: 4/30
2025-11-14 21:48:44,281 - INFO - Fold 5 Train Epoch 21/200, Batch 0, Loss: 6.9796, Pearson: 0.6445, Spearman: 0.6069
2025-11-14 21:48:54,142 - INFO - Fold 5 Train Epoch 21/200, Batch 10, Loss: 7.0609, Pearson: 0.6379, Spearman: 0.6053
2025-11-14 21:49:04,232 - INFO - Fold 5 Train Epoch 21/200, Batch 20, Loss: 7.1090, Pearson: 0.6458, Spearman: 0.6039
2025-11-14 21:49:14,346 - INFO - Fold 5 Train Epoch 21/200, Batch 30, Loss: 7.1041, Pearson: 0.6515, Spearman: 0.6164
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6328
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6374
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6483
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6371
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.132420539855957
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 21 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        8.670128  0.        0.        7.5718584 0.
 8.264749  0.        8.264749 ]
Sample y_pred values (first sample, first 10 genes):
[1.0206192  0.84342885 1.8648202  2.128861   2.929864   2.9870641
 0.8144253  2.8289514  3.2085352  6.1804085 ]
y_true  -> mean=1.9936, std=3.4544, min=0.0000, max=13.1224
y_pred  -> mean=2.0454, std=2.2448, min=0.0000, max=13.2335
Batch 0 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6348
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6314
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6379
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6307
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6259
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6349
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6437
2025-11-14 21:49:24,486 - INFO - Fold 5 Train Epoch 21/200, Batch 40, Loss: 6.8583, Pearson: 0.6460, Spearman: 0.6020
2025-11-14 21:49:34,584 - INFO - Fold 5 Train Epoch 21/200, Batch 50, Loss: 7.1872, Pearson: 0.6423, Spearman: 0.6089
2025-11-14 21:49:44,711 - INFO - Fold 5 Train Epoch 21/200, Batch 60, Loss: 6.8454, Pearson: 0.6452, Spearman: 0.6002
2025-11-14 21:49:53,019 - INFO - Fold 5 Train Epoch 21/200, Batch 70, Loss: 7.1657, Pearson: 0.6424, Spearman: 0.6054
2025-11-14 21:50:01,157 - INFO - Fold 5 Train Epoch 21/200, Batch 80, Loss: 6.9374, Pearson: 0.6438, Spearman: 0.6106
2025-11-14 21:50:07,439 - INFO - Fold 5 Train Epoch 21/200, Train Loss: 7.1060, Pearson Mean: 0.6430, Spearman Mean: 0.6070
2025-11-14 21:50:07,439 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4454, 'spearman_mean_genewise': 0.4003, 'l1_error_mean': 1.8507, 'l2_errors_mean': 7.106, 'r2_scores_mean': 0.2118, 'pearson_std': 0.117, 'l2_error_q1': 4.6877, 'l2_error_q2': 6.7134, 'l2_error_q3': 9.242, 'r2_score_q1': 0.131, 'r2_score_q2': 0.1822, 'r2_score_q3': 0.2617, 'mape_mean': 58.761, 'mape_std': 18.3083, 'rmse_mean': 2.617, 'rmse_std': 0.507}
2025-11-14 21:50:07,841 - INFO - Fold 5 Val Epoch 21/200, Batch 0, Loss: 7.0731, Pearson: 0.4848, Spearman: 0.4872
2025-11-14 21:50:09,499 - INFO - Fold 5 Val Epoch 21/200, Batch 10, Loss: 7.8571, Pearson: 0.6195, Spearman: 0.6128
2025-11-14 21:50:11,095 - INFO - Fold 5 Val Epoch 21/200, Batch 20, Loss: 9.0638, Pearson: 0.5479, Spearman: 0.5581
2025-11-14 21:50:13,431 - INFO - Fold 5 Val Epoch 21/200, Val Loss: 8.3199, Pearson Mean: 0.5606, Spearman Mean: 0.5531
2025-11-14 21:50:13,432 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3033, 'spearman_mean_genewise': 0.2737, 'l1_error_mean': 2.1313, 'l2_errors_mean': 8.3003, 'r2_scores_mean': 0.094, 'pearson_std': 0.1091, 'l2_error_q1': 5.6275, 'l2_error_q2': 8.0604, 'l2_error_q3': 10.8758, 'r2_score_q1': 0.0425, 'r2_score_q2': 0.0746, 'r2_score_q3': 0.1188, 'mape_mean': 62.9756, 'mape_std': 19.6732, 'rmse_mean': 2.8314, 'rmse_std': 0.5323}
2025-11-14 21:50:13,432 - INFO - Learning rate for epoch 21: 0.0001
2025-11-14 21:50:13,432 - INFO - No improvement in spearman genewise. Patience: 5/30
2025-11-14 21:50:14,466 - INFO - Fold 5 Train Epoch 22/200, Batch 0, Loss: 7.3935, Pearson: 0.6419, Spearman: 0.6063
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6287
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6244
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6333
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6340
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6265
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6319
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6344
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6434
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6449
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.105972766876221
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 22 =====================
Sample y_true values (first sample, first 10 genes):
[0.        6.947458  0.        6.255271  0.        6.255271  0.
 6.947458  6.947458  7.3526025]
Sample y_pred values (first sample, first 10 genes):
[0.9733474  0.59207964 1.194592   1.0992571  4.035556   2.9906514
 0.8858171  1.8549812  5.193411   7.177471  ]
y_true  -> mean=2.2047, std=3.5397, min=0.0000, max=12.6644
y_pred  -> mean=2.0509, std=2.2217, min=0.0000, max=14.0230
Batch 0 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6471
2025-11-14 21:50:24,585 - INFO - Fold 5 Train Epoch 22/200, Batch 10, Loss: 7.3287, Pearson: 0.6464, Spearman: 0.6116
2025-11-14 21:50:34,699 - INFO - Fold 5 Train Epoch 22/200, Batch 20, Loss: 7.1636, Pearson: 0.6370, Spearman: 0.6068
2025-11-14 21:50:44,809 - INFO - Fold 5 Train Epoch 22/200, Batch 30, Loss: 7.3112, Pearson: 0.6416, Spearman: 0.6090
2025-11-14 21:50:54,922 - INFO - Fold 5 Train Epoch 22/200, Batch 40, Loss: 6.8878, Pearson: 0.6409, Spearman: 0.6038
2025-11-14 21:51:05,030 - INFO - Fold 5 Train Epoch 22/200, Batch 50, Loss: 6.8916, Pearson: 0.6401, Spearman: 0.6059
2025-11-14 21:51:15,156 - INFO - Fold 5 Train Epoch 22/200, Batch 60, Loss: 7.2701, Pearson: 0.6315, Spearman: 0.6048
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6351
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6370
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6389
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6338
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6312
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6408
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6315
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6421
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6442
2025-11-14 21:51:23,345 - INFO - Fold 5 Train Epoch 22/200, Batch 70, Loss: 7.0033, Pearson: 0.6582, Spearman: 0.6180
2025-11-14 21:51:32,063 - INFO - Fold 5 Train Epoch 22/200, Batch 80, Loss: 7.0979, Pearson: 0.6430, Spearman: 0.6080
2025-11-14 21:51:38,324 - INFO - Fold 5 Train Epoch 22/200, Train Loss: 7.0725, Pearson Mean: 0.6451, Spearman Mean: 0.6094
2025-11-14 21:51:38,325 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4495, 'spearman_mean_genewise': 0.4036, 'l1_error_mean': 1.8457, 'l2_errors_mean': 7.0725, 'r2_scores_mean': 0.2152, 'pearson_std': 0.1163, 'l2_error_q1': 4.6476, 'l2_error_q2': 6.6862, 'l2_error_q3': 9.2009, 'r2_score_q1': 0.1347, 'r2_score_q2': 0.1859, 'r2_score_q3': 0.2645, 'mape_mean': 58.6154, 'mape_std': 18.2953, 'rmse_mean': 2.611, 'rmse_std': 0.5052}
2025-11-14 21:51:38,657 - INFO - Fold 5 Val Epoch 22/200, Batch 0, Loss: 7.0920, Pearson: 0.4826, Spearman: 0.4852
2025-11-14 21:51:40,304 - INFO - Fold 5 Val Epoch 22/200, Batch 10, Loss: 7.8571, Pearson: 0.6194, Spearman: 0.6121
2025-11-14 21:51:41,875 - INFO - Fold 5 Val Epoch 22/200, Batch 20, Loss: 9.0851, Pearson: 0.5457, Spearman: 0.5562
2025-11-14 21:51:44,203 - INFO - Fold 5 Val Epoch 22/200, Val Loss: 8.3238, Pearson Mean: 0.5603, Spearman Mean: 0.5522
2025-11-14 21:51:44,203 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3034, 'spearman_mean_genewise': 0.2756, 'l1_error_mean': 2.1429, 'l2_errors_mean': 8.3037, 'r2_scores_mean': 0.0936, 'pearson_std': 0.1091, 'l2_error_q1': 5.6378, 'l2_error_q2': 8.061, 'l2_error_q3': 10.9011, 'r2_score_q1': 0.0423, 'r2_score_q2': 0.0737, 'r2_score_q3': 0.1186, 'mape_mean': 62.6787, 'mape_std': 19.4353, 'rmse_mean': 2.832, 'rmse_std': 0.5323}
2025-11-14 21:51:44,204 - INFO - Learning rate for epoch 22: 1e-05
2025-11-14 21:51:44,204 - INFO - No improvement in spearman genewise. Patience: 6/30
2025-11-14 21:51:45,171 - INFO - Fold 5 Train Epoch 23/200, Batch 0, Loss: 7.2581, Pearson: 0.6415, Spearman: 0.6102
2025-11-14 21:51:55,136 - INFO - Fold 5 Train Epoch 23/200, Batch 10, Loss: 6.9524, Pearson: 0.6434, Spearman: 0.6102
2025-11-14 21:52:05,233 - INFO - Fold 5 Train Epoch 23/200, Batch 20, Loss: 6.9627, Pearson: 0.6565, Spearman: 0.6193
2025-11-14 21:52:15,388 - INFO - Fold 5 Train Epoch 23/200, Batch 30, Loss: 6.7907, Pearson: 0.6411, Spearman: 0.6046
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6369
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6360
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6415
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6454
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.072477340698242
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 23 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 7.473078 0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.135295   0.         1.1116507  0.         3.2170777  1.3220918
 0.56986547 0.7132057  4.563875   7.9123936 ]
y_true  -> mean=2.1393, std=3.5091, min=0.0000, max=12.9766
y_pred  -> mean=2.0442, std=2.1994, min=0.0000, max=13.3637
Batch 0 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6327
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6358
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6440
2025-11-14 21:52:25,516 - INFO - Fold 5 Train Epoch 23/200, Batch 40, Loss: 7.1513, Pearson: 0.6486, Spearman: 0.6158
2025-11-14 21:52:35,622 - INFO - Fold 5 Train Epoch 23/200, Batch 50, Loss: 6.8565, Pearson: 0.6481, Spearman: 0.6111
2025-11-14 21:52:45,741 - INFO - Fold 5 Train Epoch 23/200, Batch 60, Loss: 6.9567, Pearson: 0.6502, Spearman: 0.6090
2025-11-14 21:52:54,005 - INFO - Fold 5 Train Epoch 23/200, Batch 70, Loss: 7.2433, Pearson: 0.6527, Spearman: 0.6156
2025-11-14 21:53:02,857 - INFO - Fold 5 Train Epoch 23/200, Batch 80, Loss: 7.0536, Pearson: 0.6529, Spearman: 0.6190
2025-11-14 21:53:09,085 - INFO - Fold 5 Train Epoch 23/200, Train Loss: 7.0192, Pearson Mean: 0.6486, Spearman Mean: 0.6133
2025-11-14 21:53:09,086 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4561, 'spearman_mean_genewise': 0.4093, 'l1_error_mean': 1.8374, 'l2_errors_mean': 7.0192, 'r2_scores_mean': 0.2207, 'pearson_std': 0.115, 'l2_error_q1': 4.6163, 'l2_error_q2': 6.65, 'l2_error_q3': 9.1266, 'r2_score_q1': 0.1387, 'r2_score_q2': 0.1917, 'r2_score_q3': 0.2706, 'mape_mean': 58.3512, 'mape_std': 18.2461, 'rmse_mean': 2.6012, 'rmse_std': 0.5028}
2025-11-14 21:53:09,417 - INFO - Fold 5 Val Epoch 23/200, Batch 0, Loss: 7.0631, Pearson: 0.4842, Spearman: 0.4867
2025-11-14 21:53:11,009 - INFO - Fold 5 Val Epoch 23/200, Batch 10, Loss: 7.8070, Pearson: 0.6226, Spearman: 0.6139
2025-11-14 21:53:12,506 - INFO - Fold 5 Val Epoch 23/200, Batch 20, Loss: 9.0792, Pearson: 0.5480, Spearman: 0.5572
2025-11-14 21:53:14,819 - INFO - Fold 5 Val Epoch 23/200, Val Loss: 8.3055, Pearson Mean: 0.5613, Spearman Mean: 0.5532
2025-11-14 21:53:14,819 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3055, 'spearman_mean_genewise': 0.277, 'l1_error_mean': 2.122, 'l2_errors_mean': 8.2852, 'r2_scores_mean': 0.0953, 'pearson_std': 0.1099, 'l2_error_q1': 5.6293, 'l2_error_q2': 8.0512, 'l2_error_q3': 10.8934, 'r2_score_q1': 0.0437, 'r2_score_q2': 0.0747, 'r2_score_q3': 0.1202, 'mape_mean': 63.3727, 'mape_std': 19.4435, 'rmse_mean': 2.829, 'rmse_std': 0.5308}
2025-11-14 21:53:14,820 - INFO - Learning rate for epoch 23: 1e-05
2025-11-14 21:53:14,820 - INFO - No improvement in spearman genewise. Patience: 7/30
2025-11-14 21:53:15,854 - INFO - Fold 5 Train Epoch 24/200, Batch 0, Loss: 6.7848, Pearson: 0.6488, Spearman: 0.6128
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6402
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6255
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6341
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6449
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6426
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.019174575805664
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 24 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 0.        7.7498336 9.358927 ]
Sample y_pred values (first sample, first 10 genes):
[0.        0.        1.77255   0.        3.9019132 0.6475299 0.
 1.2990961 3.9662528 8.781183 ]
y_true  -> mean=1.9316, std=3.4195, min=0.0000, max=12.6507
y_pred  -> mean=2.0457, std=2.2251, min=0.0000, max=13.0644
Batch 0 Pearson correlation: 0.6488
2025-11-14 21:53:25,968 - INFO - Fold 5 Train Epoch 24/200, Batch 10, Loss: 6.8704, Pearson: 0.6548, Spearman: 0.6178
2025-11-14 21:53:36,099 - INFO - Fold 5 Train Epoch 24/200, Batch 20, Loss: 7.1393, Pearson: 0.6545, Spearman: 0.6193
2025-11-14 21:53:46,207 - INFO - Fold 5 Train Epoch 24/200, Batch 30, Loss: 7.0072, Pearson: 0.6503, Spearman: 0.6152
2025-11-14 21:53:56,321 - INFO - Fold 5 Train Epoch 24/200, Batch 40, Loss: 7.2017, Pearson: 0.6412, Spearman: 0.6065
2025-11-14 21:54:06,471 - INFO - Fold 5 Train Epoch 24/200, Batch 50, Loss: 6.8386, Pearson: 0.6526, Spearman: 0.6156
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6382
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6334
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6555
2025-11-14 21:54:16,594 - INFO - Fold 5 Train Epoch 24/200, Batch 60, Loss: 7.0538, Pearson: 0.6517, Spearman: 0.6179
2025-11-14 21:54:24,863 - INFO - Fold 5 Train Epoch 24/200, Batch 70, Loss: 6.8510, Pearson: 0.6590, Spearman: 0.6205
2025-11-14 21:54:33,446 - INFO - Fold 5 Train Epoch 24/200, Batch 80, Loss: 6.8981, Pearson: 0.6504, Spearman: 0.6161
2025-11-14 21:54:39,717 - INFO - Fold 5 Train Epoch 24/200, Train Loss: 7.0059, Pearson Mean: 0.6494, Spearman Mean: 0.6140
2025-11-14 21:54:39,717 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4576, 'spearman_mean_genewise': 0.4105, 'l1_error_mean': 1.8362, 'l2_errors_mean': 7.0061, 'r2_scores_mean': 0.222, 'pearson_std': 0.1148, 'l2_error_q1': 4.6184, 'l2_error_q2': 6.6222, 'l2_error_q3': 9.1072, 'r2_score_q1': 0.1403, 'r2_score_q2': 0.1929, 'r2_score_q3': 0.2736, 'mape_mean': 58.3251, 'mape_std': 18.249, 'rmse_mean': 2.5989, 'rmse_std': 0.502}
2025-11-14 21:54:40,090 - INFO - Fold 5 Val Epoch 24/200, Batch 0, Loss: 7.0832, Pearson: 0.4821, Spearman: 0.4853
2025-11-14 21:54:41,796 - INFO - Fold 5 Val Epoch 24/200, Batch 10, Loss: 7.8432, Pearson: 0.6203, Spearman: 0.6119
2025-11-14 21:54:43,397 - INFO - Fold 5 Val Epoch 24/200, Batch 20, Loss: 9.0980, Pearson: 0.5460, Spearman: 0.5553
2025-11-14 21:54:45,740 - INFO - Fold 5 Val Epoch 24/200, Val Loss: 8.3217, Pearson Mean: 0.5600, Spearman Mean: 0.5521
2025-11-14 21:54:45,741 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3042, 'spearman_mean_genewise': 0.275, 'l1_error_mean': 2.1345, 'l2_errors_mean': 8.3013, 'r2_scores_mean': 0.0939, 'pearson_std': 0.1093, 'l2_error_q1': 5.6369, 'l2_error_q2': 8.0519, 'l2_error_q3': 10.9117, 'r2_score_q1': 0.0426, 'r2_score_q2': 0.0742, 'r2_score_q3': 0.1193, 'mape_mean': 62.8582, 'mape_std': 19.3748, 'rmse_mean': 2.8316, 'rmse_std': 0.5323}
2025-11-14 21:54:45,741 - INFO - Learning rate for epoch 24: 1e-05
2025-11-14 21:54:45,741 - INFO - No improvement in spearman genewise. Patience: 8/30
2025-11-14 21:54:46,611 - INFO - Fold 5 Train Epoch 25/200, Batch 0, Loss: 7.1433, Pearson: 0.6504, Spearman: 0.6152
2025-11-14 21:54:56,464 - INFO - Fold 5 Train Epoch 25/200, Batch 10, Loss: 7.0883, Pearson: 0.6510, Spearman: 0.6163
2025-11-14 21:55:06,588 - INFO - Fold 5 Train Epoch 25/200, Batch 20, Loss: 6.8470, Pearson: 0.6651, Spearman: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6479
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6470
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 7.00606632232666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 25 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.13317482 0.         0.15891552 0.
 0.         0.2161704  0.12978214 2.274977  ]
y_true  -> mean=2.1632, std=3.5149, min=0.0000, max=12.5039
y_pred  -> mean=2.0469, std=2.2420, min=0.0000, max=14.0274
Batch 0 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6293
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6354
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
2025-11-14 21:55:16,713 - INFO - Fold 5 Train Epoch 25/200, Batch 30, Loss: 7.1901, Pearson: 0.6437, Spearman: 0.6119
2025-11-14 21:55:26,857 - INFO - Fold 5 Train Epoch 25/200, Batch 40, Loss: 7.0048, Pearson: 0.6428, Spearman: 0.6115
2025-11-14 21:55:36,964 - INFO - Fold 5 Train Epoch 25/200, Batch 50, Loss: 6.7017, Pearson: 0.6631, Spearman: 0.6283
2025-11-14 21:55:47,082 - INFO - Fold 5 Train Epoch 25/200, Batch 60, Loss: 7.1723, Pearson: 0.6458, Spearman: 0.6068
2025-11-14 21:55:55,422 - INFO - Fold 5 Train Epoch 25/200, Batch 70, Loss: 6.7701, Pearson: 0.6595, Spearman: 0.6224
2025-11-14 21:56:04,029 - INFO - Fold 5 Train Epoch 25/200, Batch 80, Loss: 7.1805, Pearson: 0.6404, Spearman: 0.6107
Batch 27 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6378
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6426
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6490
all_y_true: <class 'numpy.ndarray'>2025-11-14 21:56:10,291 - INFO - Fold 5 Train Epoch 25/200, Train Loss: 7.0018, Pearson Mean: 0.6497, Spearman Mean: 0.6143
2025-11-14 21:56:10,291 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4583, 'spearman_mean_genewise': 0.4109, 'l1_error_mean': 1.8358, 'l2_errors_mean': 7.0017, 'r2_scores_mean': 0.2225, 'pearson_std': 0.1146, 'l2_error_q1': 4.6178, 'l2_error_q2': 6.6234, 'l2_error_q3': 9.0959, 'r2_score_q1': 0.1408, 'r2_score_q2': 0.194, 'r2_score_q3': 0.2736, 'mape_mean': 58.328, 'mape_std': 18.2442, 'rmse_mean': 2.598, 'rmse_std': 0.5019}
2025-11-14 21:56:10,649 - INFO - Fold 5 Val Epoch 25/200, Batch 0, Loss: 7.0696, Pearson: 0.4842, Spearman: 0.4864
2025-11-14 21:56:12,280 - INFO - Fold 5 Val Epoch 25/200, Batch 10, Loss: 7.8183, Pearson: 0.6218, Spearman: 0.6131
2025-11-14 21:56:13,806 - INFO - Fold 5 Val Epoch 25/200, Batch 20, Loss: 9.1288, Pearson: 0.5445, Spearman: 0.5535
2025-11-14 21:56:16,139 - INFO - Fold 5 Val Epoch 25/200, Val Loss: 8.3257, Pearson Mean: 0.5599, Spearman Mean: 0.5518
2025-11-14 21:56:16,140 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3047, 'spearman_mean_genewise': 0.2753, 'l1_error_mean': 2.1254, 'l2_errors_mean': 8.3045, 'r2_scores_mean': 0.0937, 'pearson_std': 0.1091, 'l2_error_q1': 5.6254, 'l2_error_q2': 8.0672, 'l2_error_q3': 10.9102, 'r2_score_q1': 0.0432, 'r2_score_q2': 0.0739, 'r2_score_q3': 0.1192, 'mape_mean': 63.1563, 'mape_std': 19.4006, 'rmse_mean': 2.8321, 'rmse_std': 0.5329}
2025-11-14 21:56:16,140 - INFO - Learning rate for epoch 25: 1e-05
2025-11-14 21:56:16,140 - INFO - No improvement in spearman genewise. Patience: 9/30
2025-11-14 21:56:17,188 - INFO - Fold 5 Train Epoch 26/200, Batch 0, Loss: 7.0538, Pearson: 0.6445, Spearman: 0.6143
2025-11-14 21:56:27,264 - INFO - Fold 5 Train Epoch 26/200, Batch 10, Loss: 6.8436, Pearson: 0.6637, Spearman: 0.6178
2025-11-14 21:56:37,367 - INFO - Fold 5 Train Epoch 26/200, Batch 20, Loss: 7.2916, Pearson: 0.6384, Spearman: 0.6039
2025-11-14 21:56:47,485 - INFO - Fold 5 Train Epoch 26/200, Batch 30, Loss: 6.9919, Pearson: 0.6518, Spearman: 0.6144
2025-11-14 21:56:57,608 - INFO - Fold 5 Train Epoch 26/200, Batch 40, Loss: 6.8341, Pearson: 0.6552, Spearman: 0.6163
2025-11-14 21:57:07,721 - INFO - Fold 5 Train Epoch 26/200, Batch 50, Loss: 6.8482, Pearson: 0.6571, Spearman: 0.6210

all_y_pred: <class 'numpy.ndarray'>
========================= 7.001721382141113
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 26 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       8.779795 7.68149  0.       6.988804 0.
 6.988804 8.597504]
Sample y_pred values (first sample, first 10 genes):
[1.7055507 2.7370005 3.288455  2.9941957 6.048622  4.2678323 1.2409006
 3.694576  3.6398554 7.7337565]
y_true  -> mean=2.0096, std=3.4731, min=0.0000, max=12.5794
y_pred  -> mean=2.0401, std=2.2055, min=0.0000, max=13.2338
Batch 0 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6384
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6372
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6502
2025-11-14 21:57:17,853 - INFO - Fold 5 Train Epoch 26/200, Batch 60, Loss: 6.9631, Pearson: 0.6479, Spearman: 0.6095
2025-11-14 21:57:26,060 - INFO - Fold 5 Train Epoch 26/200, Batch 70, Loss: 7.1587, Pearson: 0.6425, Spearman: 0.6112
2025-11-14 21:57:34,728 - INFO - Fold 5 Train Epoch 26/200, Batch 80, Loss: 6.9572, Pearson: 0.6490, Spearman: 0.6167
2025-11-14 21:57:40,964 - INFO - Fold 5 Train Epoch 26/200, Train Loss: 6.9938, Pearson Mean: 0.6502, Spearman Mean: 0.6148
2025-11-14 21:57:40,964 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4591, 'spearman_mean_genewise': 0.4117, 'l1_error_mean': 1.8352, 'l2_errors_mean': 6.9938, 'r2_scores_mean': 0.2233, 'pearson_std': 0.1145, 'l2_error_q1': 4.6046, 'l2_error_q2': 6.6223, 'l2_error_q3': 9.0788, 'r2_score_q1': 0.1418, 'r2_score_q2': 0.1949, 'r2_score_q3': 0.2737, 'mape_mean': 58.2658, 'mape_std': 18.2343, 'rmse_mean': 2.5966, 'rmse_std': 0.5012}
2025-11-14 21:57:41,289 - INFO - Fold 5 Val Epoch 26/200, Batch 0, Loss: 7.0522, Pearson: 0.4856, Spearman: 0.4874
2025-11-14 21:57:42,879 - INFO - Fold 5 Val Epoch 26/200, Batch 10, Loss: 7.8244, Pearson: 0.6214, Spearman: 0.6128
2025-11-14 21:57:44,403 - INFO - Fold 5 Val Epoch 26/200, Batch 20, Loss: 9.1047, Pearson: 0.5459, Spearman: 0.5552
2025-11-14 21:57:46,722 - INFO - Fold 5 Val Epoch 26/200, Val Loss: 8.3167, Pearson Mean: 0.5605, Spearman Mean: 0.5525
2025-11-14 21:57:46,723 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3039, 'spearman_mean_genewise': 0.2751, 'l1_error_mean': 2.1294, 'l2_errors_mean': 8.2959, 'r2_scores_mean': 0.0943, 'pearson_std': 0.1095, 'l2_error_q1': 5.6145, 'l2_error_q2': 8.073, 'l2_error_q3': 10.8908, 'r2_score_q1': 0.0435, 'r2_score_q2': 0.0744, 'r2_score_q3': 0.1193, 'mape_mean': 63.2974, 'mape_std': 19.3632, 'rmse_mean': 2.8308, 'rmse_std': 0.5316}
2025-11-14 21:57:46,723 - INFO - Learning rate for epoch 26: 1e-05
2025-11-14 21:57:46,723 - INFO - No improvement in spearman genewise. Patience: 10/30
2025-11-14 21:57:47,742 - INFO - Fold 5 Train Epoch 27/200, Batch 0, Loss: 6.9904, Pearson: 0.6481, Spearman: 0.6140
2025-11-14 21:57:57,871 - INFO - Fold 5 Train Epoch 27/200, Batch 10, Loss: 7.0088, Pearson: 0.6479, Spearman: 0.6080
2025-11-14 21:58:07,975 - INFO - Fold 5 Train Epoch 27/200, Batch 20, Loss: 6.9103, Pearson: 0.6543, Spearman: 0.6135
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6322
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6532
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6488
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.993757724761963
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 27 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        7.0117974 0.        0.        0.
 0.        7.0117974 8.397416 ]
Sample y_pred values (first sample, first 10 genes):
[0.3341479  0.28911164 0.7468295  0.63269293 3.6863537  1.7355869
 1.0740724  1.1055373  4.5674434  7.849782  ]
y_true  -> mean=2.0511, std=3.4716, min=0.0000, max=12.7055
y_pred  -> mean=2.0431, std=2.2174, min=0.0000, max=12.7269
Batch 0 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6516
2025-11-14 21:58:18,103 - INFO - Fold 5 Train Epoch 27/200, Batch 30, Loss: 6.9368, Pearson: 0.6498, Spearman: 0.6107
2025-11-14 21:58:28,230 - INFO - Fold 5 Train Epoch 27/200, Batch 40, Loss: 6.8939, Pearson: 0.6573, Spearman: 0.6209
2025-11-14 21:58:38,387 - INFO - Fold 5 Train Epoch 27/200, Batch 50, Loss: 6.7542, Pearson: 0.6568, Spearman: 0.6166
2025-11-14 21:58:48,501 - INFO - Fold 5 Train Epoch 27/200, Batch 60, Loss: 6.8206, Pearson: 0.6601, Spearman: 0.6168
2025-11-14 21:58:56,775 - INFO - Fold 5 Train Epoch 27/200, Batch 70, Loss: 7.0613, Pearson: 0.6540, Spearman: 0.6168
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6412
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6656
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6316
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6482
2025-11-14 21:59:05,389 - INFO - Fold 5 Train Epoch 27/200, Batch 80, Loss: 6.8843, Pearson: 0.6336, Spearman: 0.6062
2025-11-14 21:59:11,638 - INFO - Fold 5 Train Epoch 27/200, Train Loss: 6.9869, Pearson Mean: 0.6506, Spearman Mean: 0.6152
2025-11-14 21:59:11,638 - INFO - Training Metrics: {'pearson_mean_genewise': 0.46, 'spearman_mean_genewise': 0.4123, 'l1_error_mean': 1.8337, 'l2_errors_mean': 6.9868, 'r2_scores_mean': 0.224, 'pearson_std': 0.1145, 'l2_error_q1': 4.6034, 'l2_error_q2': 6.615, 'l2_error_q3': 9.0575, 'r2_score_q1': 0.1423, 'r2_score_q2': 0.1954, 'r2_score_q3': 0.2752, 'mape_mean': 58.2343, 'mape_std': 18.2475, 'rmse_mean': 2.5953, 'rmse_std': 0.501}
2025-11-14 21:59:12,014 - INFO - Fold 5 Val Epoch 27/200, Batch 0, Loss: 7.0552, Pearson: 0.4845, Spearman: 0.4868
2025-11-14 21:59:13,703 - INFO - Fold 5 Val Epoch 27/200, Batch 10, Loss: 7.8278, Pearson: 0.6212, Spearman: 0.6128
2025-11-14 21:59:15,240 - INFO - Fold 5 Val Epoch 27/200, Batch 20, Loss: 9.0948, Pearson: 0.5462, Spearman: 0.5553
2025-11-14 21:59:17,588 - INFO - Fold 5 Val Epoch 27/200, Val Loss: 8.3154, Pearson Mean: 0.5604, Spearman Mean: 0.5524
2025-11-14 21:59:17,588 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3041, 'spearman_mean_genewise': 0.2751, 'l1_error_mean': 2.1326, 'l2_errors_mean': 8.2948, 'r2_scores_mean': 0.0944, 'pearson_std': 0.1094, 'l2_error_q1': 5.6217, 'l2_error_q2': 8.0748, 'l2_error_q3': 10.9022, 'r2_score_q1': 0.0429, 'r2_score_q2': 0.0749, 'r2_score_q3': 0.1193, 'mape_mean': 62.9882, 'mape_std': 19.4244, 'rmse_mean': 2.8306, 'rmse_std': 0.5315}
2025-11-14 21:59:17,589 - INFO - Learning rate for epoch 27: 1e-05
2025-11-14 21:59:17,589 - INFO - No improvement in spearman genewise. Patience: 11/30
2025-11-14 21:59:18,501 - INFO - Fold 5 Train Epoch 28/200, Batch 0, Loss: 6.9103, Pearson: 0.6515, Spearman: 0.6181
2025-11-14 21:59:28,463 - INFO - Fold 5 Train Epoch 28/200, Batch 10, Loss: 6.8654, Pearson: 0.6505, Spearman: 0.6246
2025-11-14 21:59:38,585 - INFO - Fold 5 Train Epoch 28/200, Batch 20, Loss: 7.4296, Pearson: 0.6472, Spearman: 0.6143
2025-11-14 21:59:48,693 - INFO - Fold 5 Train Epoch 28/200, Batch 30, Loss: 6.9080, Pearson: 0.6463, Spearman: 0.6089
2025-11-14 21:59:58,818 - INFO - Fold 5 Train Epoch 28/200, Batch 40, Loss: 7.1708, Pearson: 0.6381, Spearman: 0.6026
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6463
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6495
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.986821174621582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 28 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.825225]
Sample y_pred values (first sample, first 10 genes):
[0.09745607 0.         0.4940753  0.06161925 0.581107   0.99987006
 0.3135249  0.0771402  0.8625123  4.9485655 ]
y_true  -> mean=2.0408, std=3.4643, min=0.0000, max=12.5794
y_pred  -> mean=2.0408, std=2.1987, min=0.0000, max=12.5950
Batch 0 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6375
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6616
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6570
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6414
2025-11-14 22:00:08,924 - INFO - Fold 5 Train Epoch 28/200, Batch 50, Loss: 6.8631, Pearson: 0.6585, Spearman: 0.6206
2025-11-14 22:00:19,053 - INFO - Fold 5 Train Epoch 28/200, Batch 60, Loss: 7.0385, Pearson: 0.6585, Spearman: 0.6205
2025-11-14 22:00:27,388 - INFO - Fold 5 Train Epoch 28/200, Batch 70, Loss: 7.1096, Pearson: 0.6472, Spearman: 0.6188
2025-11-14 22:00:36,004 - INFO - Fold 5 Train Epoch 28/200, Batch 80, Loss: 6.8081, Pearson: 0.6494, Spearman: 0.6109
2025-11-14 22:00:42,253 - INFO - Fold 5 Train Epoch 28/200, Train Loss: 6.9790, Pearson Mean: 0.6511, Spearman Mean: 0.6157
2025-11-14 22:00:42,253 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4607, 'spearman_mean_genewise': 0.413, 'l1_error_mean': 1.8326, 'l2_errors_mean': 6.9791, 'r2_scores_mean': 0.2247, 'pearson_std': 0.1145, 'l2_error_q1': 4.5898, 'l2_error_q2': 6.6092, 'l2_error_q3': 9.0692, 'r2_score_q1': 0.1427, 'r2_score_q2': 0.1959, 'r2_score_q3': 0.2749, 'mape_mean': 58.1883, 'mape_std': 18.2402, 'rmse_mean': 2.594, 'rmse_std': 0.5004}
2025-11-14 22:00:42,649 - INFO - Fold 5 Val Epoch 28/200, Batch 0, Loss: 7.0357, Pearson: 0.4859, Spearman: 0.4877
2025-11-14 22:00:44,373 - INFO - Fold 5 Val Epoch 28/200, Batch 10, Loss: 7.8580, Pearson: 0.6193, Spearman: 0.6109
2025-11-14 22:00:45,887 - INFO - Fold 5 Val Epoch 28/200, Batch 20, Loss: 9.0788, Pearson: 0.5470, Spearman: 0.5561
2025-11-14 22:00:48,228 - INFO - Fold 5 Val Epoch 28/200, Val Loss: 8.3249, Pearson Mean: 0.5597, Spearman Mean: 0.5518
2025-11-14 22:00:48,229 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.303, 'spearman_mean_genewise': 0.2743, 'l1_error_mean': 2.1405, 'l2_errors_mean': 8.305, 'r2_scores_mean': 0.0934, 'pearson_std': 0.1091, 'l2_error_q1': 5.6256, 'l2_error_q2': 8.0895, 'l2_error_q3': 10.9067, 'r2_score_q1': 0.0421, 'r2_score_q2': 0.0736, 'r2_score_q3': 0.1185, 'mape_mean': 62.8141, 'mape_std': 19.428, 'rmse_mean': 2.8323, 'rmse_std': 0.5321}
2025-11-14 22:00:48,229 - INFO - Learning rate for epoch 28: 1e-05
2025-11-14 22:00:48,229 - INFO - No improvement in spearman genewise. Patience: 12/30
2025-11-14 22:00:49,150 - INFO - Fold 5 Train Epoch 29/200, Batch 0, Loss: 7.1441, Pearson: 0.6437, Spearman: 0.6101
2025-11-14 22:00:59,074 - INFO - Fold 5 Train Epoch 29/200, Batch 10, Loss: 7.1813, Pearson: 0.6482, Spearman: 0.6206
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6544
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6493
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.979062557220459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 29 =====================
Sample y_true values (first sample, first 10 genes):
[0.       7.356245 0.       0.       8.742061 0.       0.       0.
 0.       8.965172]
Sample y_pred values (first sample, first 10 genes):
[0.5452364  1.6706631  2.590157   0.22609848 5.7078238  2.6106048
 0.26805016 1.9182172  4.434825   9.55569   ]
y_true  -> mean=2.0567, std=3.4925, min=0.0000, max=12.5179
y_pred  -> mean=2.0628, std=2.2232, min=0.0000, max=14.0000
Batch 0 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6486
2025-11-14 22:01:09,210 - INFO - Fold 5 Train Epoch 29/200, Batch 20, Loss: 6.8585, Pearson: 0.6490, Spearman: 0.6164
2025-11-14 22:01:19,353 - INFO - Fold 5 Train Epoch 29/200, Batch 30, Loss: 7.1569, Pearson: 0.6423, Spearman: 0.6133
2025-11-14 22:01:29,475 - INFO - Fold 5 Train Epoch 29/200, Batch 40, Loss: 6.8603, Pearson: 0.6555, Spearman: 0.6161
2025-11-14 22:01:39,601 - INFO - Fold 5 Train Epoch 29/200, Batch 50, Loss: 6.8303, Pearson: 0.6527, Spearman: 0.6154
2025-11-14 22:01:49,717 - INFO - Fold 5 Train Epoch 29/200, Batch 60, Loss: 6.9880, Pearson: 0.6599, Spearman: 0.6216
2025-11-14 22:01:57,995 - INFO - Fold 5 Train Epoch 29/200, Batch 70, Loss: 6.6845, Pearson: 0.6581, Spearman: 0.6147
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6336
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6423
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6659
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6608
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6364
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6594
2025-11-14 22:02:06,556 - INFO - Fold 5 Train Epoch 29/200, Batch 80, Loss: 6.9510, Pearson: 0.6473, Spearman: 0.6178
2025-11-14 22:02:12,865 - INFO - Fold 5 Train Epoch 29/200, Train Loss: 6.9760, Pearson Mean: 0.6514, Spearman Mean: 0.6160
2025-11-14 22:02:12,865 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4612, 'spearman_mean_genewise': 0.4133, 'l1_error_mean': 1.832, 'l2_errors_mean': 6.9759, 'r2_scores_mean': 0.2251, 'pearson_std': 0.1143, 'l2_error_q1': 4.599, 'l2_error_q2': 6.5999, 'l2_error_q3': 9.0422, 'r2_score_q1': 0.1432, 'r2_score_q2': 0.1971, 'r2_score_q3': 0.2754, 'mape_mean': 58.1993, 'mape_std': 18.2338, 'rmse_mean': 2.5934, 'rmse_std': 0.5003}
2025-11-14 22:02:13,241 - INFO - Fold 5 Val Epoch 29/200, Batch 0, Loss: 7.0510, Pearson: 0.4850, Spearman: 0.4870
2025-11-14 22:02:14,963 - INFO - Fold 5 Val Epoch 29/200, Batch 10, Loss: 7.8297, Pearson: 0.6211, Spearman: 0.6124
2025-11-14 22:02:16,571 - INFO - Fold 5 Val Epoch 29/200, Batch 20, Loss: 9.1209, Pearson: 0.5450, Spearman: 0.5540
2025-11-14 22:02:18,911 - INFO - Fold 5 Val Epoch 29/200, Val Loss: 8.3273, Pearson Mean: 0.5597, Spearman Mean: 0.5517
2025-11-14 22:02:18,912 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3029, 'spearman_mean_genewise': 0.2745, 'l1_error_mean': 2.1297, 'l2_errors_mean': 8.3064, 'r2_scores_mean': 0.0934, 'pearson_std': 0.1094, 'l2_error_q1': 5.632, 'l2_error_q2': 8.0685, 'l2_error_q3': 10.9283, 'r2_score_q1': 0.0429, 'r2_score_q2': 0.0735, 'r2_score_q3': 0.1175, 'mape_mean': 63.4824, 'mape_std': 19.4, 'rmse_mean': 2.8325, 'rmse_std': 0.5321}
2025-11-14 22:02:18,912 - INFO - Learning rate for epoch 29: 1.0000000000000002e-06
2025-11-14 22:02:18,912 - INFO - No improvement in spearman genewise. Patience: 13/30
2025-11-14 22:02:19,930 - INFO - Fold 5 Train Epoch 30/200, Batch 0, Loss: 6.7063, Pearson: 0.6444, Spearman: 0.6069
2025-11-14 22:02:30,065 - INFO - Fold 5 Train Epoch 30/200, Batch 10, Loss: 7.0356, Pearson: 0.6558, Spearman: 0.6242
2025-11-14 22:02:40,208 - INFO - Fold 5 Train Epoch 30/200, Batch 20, Loss: 7.0727, Pearson: 0.6552, Spearman: 0.6213
2025-11-14 22:02:50,328 - INFO - Fold 5 Train Epoch 30/200, Batch 30, Loss: 6.9632, Pearson: 0.6575, Spearman: 0.6232
2025-11-14 22:03:00,474 - INFO - Fold 5 Train Epoch 30/200, Batch 40, Loss: 6.9346, Pearson: 0.6517, Spearman: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6506
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6517
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.975900650024414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 30 =====================
Sample y_true values (first sample, first 10 genes):
[0.      0.      0.      0.      0.      0.      0.      0.      0.
 8.95971]
Sample y_pred values (first sample, first 10 genes):
[0.04822759 0.         1.0258327  0.0090133  1.2770207  0.40110213
 0.25288987 0.9712438  2.29974    7.319641  ]
y_true  -> mean=1.8592, std=3.3769, min=0.0000, max=13.1224
y_pred  -> mean=2.0481, std=2.2204, min=0.0000, max=13.1664
Batch 0 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6414
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6410
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6498
2025-11-14 22:03:10,600 - INFO - Fold 5 Train Epoch 30/200, Batch 50, Loss: 6.9656, Pearson: 0.6567, Spearman: 0.6198
2025-11-14 22:03:20,697 - INFO - Fold 5 Train Epoch 30/200, Batch 60, Loss: 6.8480, Pearson: 0.6499, Spearman: 0.6155
2025-11-14 22:03:28,604 - INFO - Fold 5 Train Epoch 30/200, Batch 70, Loss: 7.0620, Pearson: 0.6366, Spearman: 0.6106
2025-11-14 22:03:37,611 - INFO - Fold 5 Train Epoch 30/200, Batch 80, Loss: 6.8881, Pearson: 0.6473, Spearman: 0.6084
2025-11-14 22:03:43,873 - INFO - Fold 5 Train Epoch 30/200, Train Loss: 6.9679, Pearson Mean: 0.6519, Spearman Mean: 0.6166
2025-11-14 22:03:43,873 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4623, 'spearman_mean_genewise': 0.4143, 'l1_error_mean': 1.833, 'l2_errors_mean': 6.9677, 'r2_scores_mean': 0.226, 'pearson_std': 0.1142, 'l2_error_q1': 4.5973, 'l2_error_q2': 6.5981, 'l2_error_q3': 9.0178, 'r2_score_q1': 0.1439, 'r2_score_q2': 0.1976, 'r2_score_q3': 0.2774, 'mape_mean': 58.1744, 'mape_std': 18.2456, 'rmse_mean': 2.5918, 'rmse_std': 0.5001}
2025-11-14 22:03:44,252 - INFO - Fold 5 Val Epoch 30/200, Batch 0, Loss: 7.0591, Pearson: 0.4845, Spearman: 0.4866
2025-11-14 22:03:45,941 - INFO - Fold 5 Val Epoch 30/200, Batch 10, Loss: 7.8290, Pearson: 0.6211, Spearman: 0.6127
2025-11-14 22:03:47,448 - INFO - Fold 5 Val Epoch 30/200, Batch 20, Loss: 9.0959, Pearson: 0.5465, Spearman: 0.5555
2025-11-14 22:03:49,796 - INFO - Fold 5 Val Epoch 30/200, Val Loss: 8.3176, Pearson Mean: 0.5604, Spearman Mean: 0.5523
2025-11-14 22:03:49,796 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3037, 'spearman_mean_genewise': 0.2749, 'l1_error_mean': 2.1284, 'l2_errors_mean': 8.2971, 'r2_scores_mean': 0.0942, 'pearson_std': 0.1096, 'l2_error_q1': 5.6273, 'l2_error_q2': 8.0658, 'l2_error_q3': 10.8914, 'r2_score_q1': 0.0428, 'r2_score_q2': 0.0738, 'r2_score_q3': 0.1191, 'mape_mean': 63.3524, 'mape_std': 19.4182, 'rmse_mean': 2.831, 'rmse_std': 0.5314}
2025-11-14 22:03:49,797 - INFO - Learning rate for epoch 30: 1.0000000000000002e-06
2025-11-14 22:03:49,797 - INFO - No improvement in spearman genewise. Patience: 14/30
2025-11-14 22:03:50,656 - INFO - Fold 5 Train Epoch 31/200, Batch 0, Loss: 6.8410, Pearson: 0.6502, Spearman: 0.6161
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6366
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6639
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6574
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6507
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.967746734619141
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 31 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       8.579257 0.       0.       0.
 0.       9.272309]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.59333706 0.35910052 1.3940067  0.274448
 0.5747117  0.33783227 1.441042   6.746129  ]
y_true  -> mean=1.9755, std=3.4414, min=0.0000, max=12.6904
y_pred  -> mean=2.0424, std=2.2296, min=0.0000, max=13.4794
Batch 0 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6525
2025-11-14 22:04:00,509 - INFO - Fold 5 Train Epoch 31/200, Batch 10, Loss: 6.8799, Pearson: 0.6545, Spearman: 0.6205
2025-11-14 22:04:10,635 - INFO - Fold 5 Train Epoch 31/200, Batch 20, Loss: 6.7051, Pearson: 0.6563, Spearman: 0.6161
2025-11-14 22:04:20,707 - INFO - Fold 5 Train Epoch 31/200, Batch 30, Loss: 6.8056, Pearson: 0.6580, Spearman: 0.6178
2025-11-14 22:04:30,846 - INFO - Fold 5 Train Epoch 31/200, Batch 40, Loss: 6.7650, Pearson: 0.6606, Spearman: 0.6231
2025-11-14 22:04:40,959 - INFO - Fold 5 Train Epoch 31/200, Batch 50, Loss: 7.0754, Pearson: 0.6675, Spearman: 0.6283
2025-11-14 22:04:51,078 - INFO - Fold 5 Train Epoch 31/200, Batch 60, Loss: 7.0399, Pearson: 0.6491, Spearman: 0.6210
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6368
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6385
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6363
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6566
2025-11-14 22:04:59,024 - INFO - Fold 5 Train Epoch 31/200, Batch 70, Loss: 7.0035, Pearson: 0.6512, Spearman: 0.6160
2025-11-14 22:05:07,567 - INFO - Fold 5 Train Epoch 31/200, Batch 80, Loss: 6.8080, Pearson: 0.6420, Spearman: 0.6092
2025-11-14 22:05:13,777 - INFO - Fold 5 Train Epoch 31/200, Train Loss: 6.9673, Pearson Mean: 0.6519, Spearman Mean: 0.6166
2025-11-14 22:05:13,777 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4624, 'spearman_mean_genewise': 0.4143, 'l1_error_mean': 1.8313, 'l2_errors_mean': 6.967, 'r2_scores_mean': 0.2261, 'pearson_std': 0.114, 'l2_error_q1': 4.5905, 'l2_error_q2': 6.6012, 'l2_error_q3': 9.0297, 'r2_score_q1': 0.1441, 'r2_score_q2': 0.1978, 'r2_score_q3': 0.2765, 'mape_mean': 58.1053, 'mape_std': 18.2164, 'rmse_mean': 2.5917, 'rmse_std': 0.5002}
2025-11-14 22:05:14,136 - INFO - Fold 5 Val Epoch 31/200, Batch 0, Loss: 7.0558, Pearson: 0.4849, Spearman: 0.4869
2025-11-14 22:05:15,722 - INFO - Fold 5 Val Epoch 31/200, Batch 10, Loss: 7.8329, Pearson: 0.6209, Spearman: 0.6121
2025-11-14 22:05:17,227 - INFO - Fold 5 Val Epoch 31/200, Batch 20, Loss: 9.1065, Pearson: 0.5460, Spearman: 0.5550
2025-11-14 22:05:19,562 - INFO - Fold 5 Val Epoch 31/200, Val Loss: 8.3223, Pearson Mean: 0.5601, Spearman Mean: 0.5520
2025-11-14 22:05:19,563 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3039, 'spearman_mean_genewise': 0.2748, 'l1_error_mean': 2.1303, 'l2_errors_mean': 8.3017, 'r2_scores_mean': 0.0938, 'pearson_std': 0.1094, 'l2_error_q1': 5.6257, 'l2_error_q2': 8.0759, 'l2_error_q3': 10.9076, 'r2_score_q1': 0.043, 'r2_score_q2': 0.0741, 'r2_score_q3': 0.1186, 'mape_mean': 63.1921, 'mape_std': 19.3244, 'rmse_mean': 2.8317, 'rmse_std': 0.5322}
2025-11-14 22:05:19,563 - INFO - Learning rate for epoch 31: 1.0000000000000002e-06
2025-11-14 22:05:19,563 - INFO - No improvement in spearman genewise. Patience: 15/30
2025-11-14 22:05:20,537 - INFO - Fold 5 Train Epoch 32/200, Batch 0, Loss: 7.1305, Pearson: 0.6530, Spearman: 0.6180
2025-11-14 22:05:30,490 - INFO - Fold 5 Train Epoch 32/200, Batch 10, Loss: 6.9620, Pearson: 0.6537, Spearman: 0.6196
2025-11-14 22:05:40,622 - INFO - Fold 5 Train Epoch 32/200, Batch 20, Loss: 7.1095, Pearson: 0.6541, Spearman: 0.6183
2025-11-14 22:05:50,753 - INFO - Fold 5 Train Epoch 32/200, Batch 30, Loss: 7.2078, Pearson: 0.6467, Spearman: 0.6139
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6483
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6492
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.967039585113525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 32 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.945545]
Sample y_pred values (first sample, first 10 genes):
[0.17499396 2.8558362  2.1063697  0.31008673 2.449172   1.308708
 0.19934523 0.9242755  2.3264527  8.638725  ]
y_true  -> mean=2.1631, std=3.5215, min=0.0000, max=12.6488
y_pred  -> mean=2.0502, std=2.2296, min=0.0000, max=13.4744
Batch 0 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6395
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6530
2025-11-14 22:06:00,855 - INFO - Fold 5 Train Epoch 32/200, Batch 40, Loss: 7.0804, Pearson: 0.6392, Spearman: 0.6038
2025-11-14 22:06:11,000 - INFO - Fold 5 Train Epoch 32/200, Batch 50, Loss: 7.0970, Pearson: 0.6566, Spearman: 0.6215
2025-11-14 22:06:21,085 - INFO - Fold 5 Train Epoch 32/200, Batch 60, Loss: 7.0173, Pearson: 0.6496, Spearman: 0.6097
2025-11-14 22:06:29,393 - INFO - Fold 5 Train Epoch 32/200, Batch 70, Loss: 7.2037, Pearson: 0.6427, Spearman: 0.6166
2025-11-14 22:06:37,928 - INFO - Fold 5 Train Epoch 32/200, Batch 80, Loss: 7.1223, Pearson: 0.6517, Spearman: 0.6238
2025-11-14 22:06:44,146 - INFO - Fold 5 Train Epoch 32/200, Train Loss: 6.9666, Pearson Mean: 0.6520, Spearman Mean: 0.6167
2025-11-14 22:06:44,146 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4623, 'spearman_mean_genewise': 0.4144, 'l1_error_mean': 1.8313, 'l2_errors_mean': 6.9665, 'r2_scores_mean': 0.2261, 'pearson_std': 0.1141, 'l2_error_q1': 4.586, 'l2_error_q2': 6.6008, 'l2_error_q3': 9.053, 'r2_score_q1': 0.1444, 'r2_score_q2': 0.1976, 'r2_score_q3': 0.2766, 'mape_mean': 58.1062, 'mape_std': 18.231, 'rmse_mean': 2.5916, 'rmse_std': 0.5}
2025-11-14 22:06:44,440 - INFO - Fold 5 Val Epoch 32/200, Batch 0, Loss: 7.0432, Pearson: 0.4856, Spearman: 0.4874
2025-11-14 22:06:46,139 - INFO - Fold 5 Val Epoch 32/200, Batch 10, Loss: 7.8487, Pearson: 0.6200, Spearman: 0.6116
2025-11-14 22:06:47,765 - INFO - Fold 5 Val Epoch 32/200, Batch 20, Loss: 9.0876, Pearson: 0.5463, Spearman: 0.5556
2025-11-14 22:06:50,095 - INFO - Fold 5 Val Epoch 32/200, Val Loss: 8.3207, Pearson Mean: 0.5601, Spearman Mean: 0.5522
2025-11-14 22:06:50,096 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3032, 'spearman_mean_genewise': 0.2741, 'l1_error_mean': 2.1416, 'l2_errors_mean': 8.3005, 'r2_scores_mean': 0.0939, 'pearson_std': 0.1091, 'l2_error_q1': 5.6287, 'l2_error_q2': 8.0675, 'l2_error_q3': 10.9155, 'r2_score_q1': 0.0423, 'r2_score_q2': 0.0743, 'r2_score_q3': 0.1195, 'mape_mean': 62.6767, 'mape_std': 19.3299, 'rmse_mean': 2.8315, 'rmse_std': 0.5319}
2025-11-14 22:06:50,096 - INFO - Learning rate for epoch 32: 1.0000000000000002e-06
2025-11-14 22:06:50,096 - INFO - No improvement in spearman genewise. Patience: 16/30
2025-11-14 22:06:51,064 - INFO - Fold 5 Train Epoch 33/200, Batch 0, Loss: 6.8775, Pearson: 0.6543, Spearman: 0.6124
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6662
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6394
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6400
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6621
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6447
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6584
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.96653938293457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 33 =====================
Sample y_true values (first sample, first 10 genes):
[6.24495   8.728076  7.3422675 6.24495   7.3422675 6.24495   6.24495
 0.        0.        9.501179 ]
Sample y_pred values (first sample, first 10 genes):
[2.4917092  5.274308   3.8355873  3.1773667  7.126527   5.7160473
 0.99705696 4.3934402  4.948742   9.022816  ]
y_true  -> mean=2.0636, std=3.4674, min=0.0000, max=12.7929
y_pred  -> mean=2.0512, std=2.2426, min=0.0000, max=13.0573
Batch 0 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6497
2025-11-14 22:07:01,020 - INFO - Fold 5 Train Epoch 33/200, Batch 10, Loss: 6.9977, Pearson: 0.6456, Spearman: 0.6094
2025-11-14 22:07:11,160 - INFO - Fold 5 Train Epoch 33/200, Batch 20, Loss: 7.1039, Pearson: 0.6528, Spearman: 0.6151
2025-11-14 22:07:21,292 - INFO - Fold 5 Train Epoch 33/200, Batch 30, Loss: 6.9834, Pearson: 0.6520, Spearman: 0.6192
2025-11-14 22:07:31,383 - INFO - Fold 5 Train Epoch 33/200, Batch 40, Loss: 6.9746, Pearson: 0.6524, Spearman: 0.6249
2025-11-14 22:07:41,498 - INFO - Fold 5 Train Epoch 33/200, Batch 50, Loss: 6.9712, Pearson: 0.6523, Spearman: 0.6199
2025-11-14 22:07:51,629 - INFO - Fold 5 Train Epoch 33/200, Batch 60, Loss: 6.8855, Pearson: 0.6591, Spearman: 0.6205
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6437
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6401
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6563
2025-11-14 22:07:59,857 - INFO - Fold 5 Train Epoch 33/200, Batch 70, Loss: 6.9733, Pearson: 0.6551, Spearman: 0.6202
2025-11-14 22:08:08,745 - INFO - Fold 5 Train Epoch 33/200, Batch 80, Loss: 7.0999, Pearson: 0.6464, Spearman: 0.6104
2025-11-14 22:08:14,990 - INFO - Fold 5 Train Epoch 33/200, Train Loss: 6.9668, Pearson Mean: 0.6519, Spearman Mean: 0.6165
2025-11-14 22:08:14,990 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4624, 'spearman_mean_genewise': 0.4142, 'l1_error_mean': 1.8315, 'l2_errors_mean': 6.9667, 'r2_scores_mean': 0.2261, 'pearson_std': 0.114, 'l2_error_q1': 4.5954, 'l2_error_q2': 6.6027, 'l2_error_q3': 9.0385, 'r2_score_q1': 0.1437, 'r2_score_q2': 0.1976, 'r2_score_q3': 0.2762, 'mape_mean': 58.1215, 'mape_std': 18.2146, 'rmse_mean': 2.5917, 'rmse_std': 0.4999}
2025-11-14 22:08:15,339 - INFO - Fold 5 Val Epoch 33/200, Batch 0, Loss: 7.0666, Pearson: 0.4845, Spearman: 0.4860
2025-11-14 22:08:16,940 - INFO - Fold 5 Val Epoch 33/200, Batch 10, Loss: 7.8342, Pearson: 0.6208, Spearman: 0.6126
2025-11-14 22:08:18,422 - INFO - Fold 5 Val Epoch 33/200, Batch 20, Loss: 9.1431, Pearson: 0.5434, Spearman: 0.5530
2025-11-14 22:08:20,755 - INFO - Fold 5 Val Epoch 33/200, Val Loss: 8.3328, Pearson Mean: 0.5595, Spearman Mean: 0.5516
2025-11-14 22:08:20,755 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3021, 'spearman_mean_genewise': 0.2726, 'l1_error_mean': 2.1296, 'l2_errors_mean': 8.3115, 'r2_scores_mean': 0.0929, 'pearson_std': 0.109, 'l2_error_q1': 5.6367, 'l2_error_q2': 8.0825, 'l2_error_q3': 10.9267, 'r2_score_q1': 0.0425, 'r2_score_q2': 0.0727, 'r2_score_q3': 0.1171, 'mape_mean': 63.4881, 'mape_std': 19.4311, 'rmse_mean': 2.8333, 'rmse_std': 0.5327}
2025-11-14 22:08:20,756 - INFO - Learning rate for epoch 33: 1.0000000000000002e-06
2025-11-14 22:08:20,756 - INFO - No improvement in spearman genewise. Patience: 17/30
2025-11-14 22:08:21,803 - INFO - Fold 5 Train Epoch 34/200, Batch 0, Loss: 7.1364, Pearson: 0.6520, Spearman: 0.6204
2025-11-14 22:08:31,907 - INFO - Fold 5 Train Epoch 34/200, Batch 10, Loss: 6.7940, Pearson: 0.6641, Spearman: 0.6227
2025-11-14 22:08:42,030 - INFO - Fold 5 Train Epoch 34/200, Batch 20, Loss: 6.8371, Pearson: 0.6475, Spearman: 0.6123
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6451
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6581
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.966724872589111
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 34 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        7.507051  6.814453  0.
 0.        0.        9.2113495]
Sample y_pred values (first sample, first 10 genes):
[0.57403696 1.5595686  2.7758927  0.7774668  4.9954796  2.4585268
 0.6417548  2.131339   4.0125737  8.956484  ]
y_true  -> mean=2.1732, std=3.5178, min=0.0000, max=12.7365
y_pred  -> mean=2.0471, std=2.2168, min=0.0000, max=13.7209
Batch 0 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6644
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6592
2025-11-14 22:08:52,126 - INFO - Fold 5 Train Epoch 34/200, Batch 30, Loss: 7.3378, Pearson: 0.6504, Spearman: 0.6140
2025-11-14 22:09:02,223 - INFO - Fold 5 Train Epoch 34/200, Batch 40, Loss: 7.1731, Pearson: 0.6429, Spearman: 0.6122
2025-11-14 22:09:12,337 - INFO - Fold 5 Train Epoch 34/200, Batch 50, Loss: 6.7998, Pearson: 0.6488, Spearman: 0.6120
2025-11-14 22:09:22,437 - INFO - Fold 5 Train Epoch 34/200, Batch 60, Loss: 7.0993, Pearson: 0.6499, Spearman: 0.6146
2025-11-14 22:09:30,717 - INFO - Fold 5 Train Epoch 34/200, Batch 70, Loss: 6.8841, Pearson: 0.6499, Spearman: 0.6120
2025-11-14 22:09:39,177 - INFO - Fold 5 Train Epoch 34/200, Batch 80, Loss: 7.0398, Pearson: 0.6582, Spearman: 0.6223
2025-11-14 22:09:45,439 - INFO - Fold 5 Train Epoch 34/200, Train Loss: 6.9664, Pearson Mean: 0.6520, Spearman Mean: 0.6167
2025-11-14 22:09:45,439 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4624, 'spearman_mean_genewise': 0.4144, 'l1_error_mean': 1.8315, 'l2_errors_mean': 6.9664, 'r2_scores_mean': 0.2262, 'pearson_std': 0.114, 'l2_error_q1': 4.5919, 'l2_error_q2': 6.602, 'l2_error_q3': 9.0432, 'r2_score_q1': 0.1438, 'r2_score_q2': 0.1977, 'r2_score_q3': 0.2767, 'mape_mean': 58.1189, 'mape_std': 18.2236, 'rmse_mean': 2.5916, 'rmse_std': 0.5}
2025-11-14 22:09:45,784 - INFO - Fold 5 Val Epoch 34/200, Batch 0, Loss: 7.0500, Pearson: 0.4847, Spearman: 0.4867
2025-11-14 22:09:47,383 - INFO - Fold 5 Val Epoch 34/200, Batch 10, Loss: 7.8427, Pearson: 0.6203, Spearman: 0.6117
2025-11-14 22:09:48,866 - INFO - Fold 5 Val Epoch 34/200, Batch 20, Loss: 9.0985, Pearson: 0.5457, Spearman: 0.5547
2025-11-14 22:09:51,213 - INFO - Fold 5 Val Epoch 34/200, Val Loss: 8.3277, Pearson Mean: 0.5596, Spearman Mean: 0.5516
2025-11-14 22:09:51,214 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.303, 'spearman_mean_genewise': 0.274, 'l1_error_mean': 2.138, 'l2_errors_mean': 8.3074, 'r2_scores_mean': 0.0933, 'pearson_std': 0.109, 'l2_error_q1': 5.6303, 'l2_error_q2': 8.0864, 'l2_error_q3': 10.9295, 'r2_score_q1': 0.0429, 'r2_score_q2': 0.0739, 'r2_score_q3': 0.1183, 'mape_mean': 62.9563, 'mape_std': 19.4544, 'rmse_mean': 2.8327, 'rmse_std': 0.5324}
2025-11-14 22:09:51,214 - INFO - Learning rate for epoch 34: 1.0000000000000002e-06
2025-11-14 22:09:51,214 - INFO - No improvement in spearman genewise. Patience: 18/30
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6611
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6427
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6436
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6511
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6507
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.966432571411133
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 35 =====================
Sample y_true values (first sample, first 10 genes):
2025-11-14 22:09:52,180 - INFO - Fold 5 Train Epoch 35/200, Batch 0, Loss: 6.8053, Pearson: 0.6486, Spearman: 0.6116
2025-11-14 22:10:02,138 - INFO - Fold 5 Train Epoch 35/200, Batch 10, Loss: 7.1766, Pearson: 0.6512, Spearman: 0.6173
2025-11-14 22:10:12,273 - INFO - Fold 5 Train Epoch 35/200, Batch 20, Loss: 6.9320, Pearson: 0.6600, Spearman: 0.6246
2025-11-14 22:10:22,411 - INFO - Fold 5 Train Epoch 35/200, Batch 30, Loss: 7.2276, Pearson: 0.6493, Spearman: 0.6123
2025-11-14 22:10:32,502 - INFO - Fold 5 Train Epoch 35/200, Batch 40, Loss: 7.1631, Pearson: 0.6430, Spearman: 0.6092
2025-11-14 22:10:42,638 - INFO - Fold 5 Train Epoch 35/200, Batch 50, Loss: 7.1902, Pearson: 0.6420, Spearman: 0.6137
[0.       0.       0.       0.       0.       0.       0.       0.
 0.       8.568676]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         1.3725438  0.27421376 1.7083209  1.1926872
 1.4004338  0.9603077  1.5237226  7.9565134 ]
y_true  -> mean=1.9217, std=3.4235, min=0.0000, max=12.8299
y_pred  -> mean=2.0477, std=2.2257, min=0.0000, max=13.0545
Batch 0 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6628
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6700
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6419
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6404
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6618
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6420
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6640
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6576
2025-11-14 22:10:52,751 - INFO - Fold 5 Train Epoch 35/200, Batch 60, Loss: 7.1390, Pearson: 0.6582, Spearman: 0.6217
2025-11-14 22:11:00,997 - INFO - Fold 5 Train Epoch 35/200, Batch 70, Loss: 7.0296, Pearson: 0.6504, Spearman: 0.6122
2025-11-14 22:11:09,845 - INFO - Fold 5 Train Epoch 35/200, Batch 80, Loss: 6.9028, Pearson: 0.6523, Spearman: 0.6190
2025-11-14 22:11:16,041 - INFO - Fold 5 Train Epoch 35/200, Train Loss: 6.9714, Pearson Mean: 0.6517, Spearman Mean: 0.6165
2025-11-14 22:11:16,041 - INFO - Training Metrics: {'pearson_mean_genewise': 0.462, 'spearman_mean_genewise': 0.4139, 'l1_error_mean': 1.8329, 'l2_errors_mean': 6.9713, 'r2_scores_mean': 0.2257, 'pearson_std': 0.1139, 'l2_error_q1': 4.584, 'l2_error_q2': 6.6034, 'l2_error_q3': 9.0491, 'r2_score_q1': 0.1433, 'r2_score_q2': 0.1972, 'r2_score_q3': 0.276, 'mape_mean': 58.1716, 'mape_std': 18.2183, 'rmse_mean': 2.5925, 'rmse_std': 0.5001}
2025-11-14 22:11:16,414 - INFO - Fold 5 Val Epoch 35/200, Batch 0, Loss: 7.0635, Pearson: 0.4849, Spearman: 0.4865
2025-11-14 22:11:18,063 - INFO - Fold 5 Val Epoch 35/200, Batch 10, Loss: 7.8292, Pearson: 0.6212, Spearman: 0.6124
2025-11-14 22:11:19,643 - INFO - Fold 5 Val Epoch 35/200, Batch 20, Loss: 9.1418, Pearson: 0.5449, Spearman: 0.5538
2025-11-14 22:11:21,971 - INFO - Fold 5 Val Epoch 35/200, Val Loss: 8.3318, Pearson Mean: 0.5598, Spearman Mean: 0.5517
2025-11-14 22:11:21,972 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3038, 'spearman_mean_genewise': 0.2745, 'l1_error_mean': 2.1177, 'l2_errors_mean': 8.3105, 'r2_scores_mean': 0.093, 'pearson_std': 0.1096, 'l2_error_q1': 5.6354, 'l2_error_q2': 8.0679, 'l2_error_q3': 10.9233, 'r2_score_q1': 0.0426, 'r2_score_q2': 0.0727, 'r2_score_q3': 0.1174, 'mape_mean': 63.81, 'mape_std': 19.3769, 'rmse_mean': 2.8331, 'rmse_std': 0.5328}
2025-11-14 22:11:21,972 - INFO - Learning rate for epoch 35: 1.0000000000000002e-07
2025-11-14 22:11:21,972 - INFO - No improvement in spearman genewise. Patience: 19/30
2025-11-14 22:11:22,974 - INFO - Fold 5 Train Epoch 36/200, Batch 0, Loss: 7.0642, Pearson: 0.6576, Spearman: 0.6211
2025-11-14 22:11:33,008 - INFO - Fold 5 Train Epoch 36/200, Batch 10, Loss: 7.2193, Pearson: 0.6479, Spearman: 0.6129
2025-11-14 22:11:43,127 - INFO - Fold 5 Train Epoch 36/200, Batch 20, Loss: 7.2055, Pearson: 0.6523, Spearman: 0.6182
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6617
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6606
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6571
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.971312999725342
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 36 =====================
Sample y_true values (first sample, first 10 genes):
[7.5207863 7.5207863 0.        0.        7.5207863 7.5207863 0.
 0.        0.        0.       ]
Sample y_pred values (first sample, first 10 genes):
[2.0032852  2.3098106  3.5318532  0.45845258 4.84348    4.523427
 0.4729887  1.942075   4.6860065  5.342982  ]
y_true  -> mean=2.1719, std=3.5224, min=0.0000, max=12.4440
y_pred  -> mean=2.0491, std=2.2319, min=0.0000, max=13.5192
Batch 0 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6613
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6650
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6593
2025-11-14 22:11:53,235 - INFO - Fold 5 Train Epoch 36/200, Batch 30, Loss: 7.2715, Pearson: 0.6457, Spearman: 0.6127
2025-11-14 22:12:03,383 - INFO - Fold 5 Train Epoch 36/200, Batch 40, Loss: 6.8320, Pearson: 0.6573, Spearman: 0.6150
2025-11-14 22:12:13,491 - INFO - Fold 5 Train Epoch 36/200, Batch 50, Loss: 6.8103, Pearson: 0.6492, Spearman: 0.6150
2025-11-14 22:12:23,607 - INFO - Fold 5 Train Epoch 36/200, Batch 60, Loss: 6.9261, Pearson: 0.6429, Spearman: 0.6153
2025-11-14 22:12:31,808 - INFO - Fold 5 Train Epoch 36/200, Batch 70, Loss: 7.1311, Pearson: 0.6560, Spearman: 0.6217
2025-11-14 22:12:40,643 - INFO - Fold 5 Train Epoch 36/200, Batch 80, Loss: 6.9658, Pearson: 0.6544, Spearman: 0.6211
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6343
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6362
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6386
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6631
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6429
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6707
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6417
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6438
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6607
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6531
2025-11-14 22:12:46,940 - INFO - Fold 5 Train Epoch 36/200, Train Loss: 6.9655, Pearson Mean: 0.6520, Spearman Mean: 0.6167
2025-11-14 22:12:46,940 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4625, 'spearman_mean_genewise': 0.4144, 'l1_error_mean': 1.8307, 'l2_errors_mean': 6.9654, 'r2_scores_mean': 0.2262, 'pearson_std': 0.114, 'l2_error_q1': 4.5934, 'l2_error_q2': 6.5995, 'l2_error_q3': 9.0363, 'r2_score_q1': 0.1444, 'r2_score_q2': 0.1973, 'r2_score_q3': 0.277, 'mape_mean': 58.1141, 'mape_std': 18.2285, 'rmse_mean': 2.5915, 'rmse_std': 0.4998}
2025-11-14 22:12:47,336 - INFO - Fold 5 Val Epoch 36/200, Batch 0, Loss: 7.0720, Pearson: 0.4840, Spearman: 0.4859
2025-11-14 22:12:48,993 - INFO - Fold 5 Val Epoch 36/200, Batch 10, Loss: 7.8356, Pearson: 0.6207, Spearman: 0.6118
2025-11-14 22:12:50,586 - INFO - Fold 5 Val Epoch 36/200, Batch 20, Loss: 9.1027, Pearson: 0.5468, Spearman: 0.5560
2025-11-14 22:12:52,940 - INFO - Fold 5 Val Epoch 36/200, Val Loss: 8.3252, Pearson Mean: 0.5600, Spearman Mean: 0.5519
2025-11-14 22:12:52,940 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3037, 'spearman_mean_genewise': 0.2745, 'l1_error_mean': 2.1245, 'l2_errors_mean': 8.3047, 'r2_scores_mean': 0.0935, 'pearson_std': 0.1096, 'l2_error_q1': 5.6333, 'l2_error_q2': 8.0724, 'l2_error_q3': 10.9057, 'r2_score_q1': 0.0426, 'r2_score_q2': 0.073, 'r2_score_q3': 0.1179, 'mape_mean': 63.5848, 'mape_std': 19.3282, 'rmse_mean': 2.8323, 'rmse_std': 0.532}
2025-11-14 22:12:52,941 - INFO - Learning rate for epoch 36: 1.0000000000000002e-07
2025-11-14 22:12:52,941 - INFO - No improvement in spearman genewise. Patience: 20/30
2025-11-14 22:12:53,990 - INFO - Fold 5 Train Epoch 37/200, Batch 0, Loss: 7.1583, Pearson: 0.6529, Spearman: 0.6192
2025-11-14 22:13:04,133 - INFO - Fold 5 Train Epoch 37/200, Batch 10, Loss: 6.8001, Pearson: 0.6541, Spearman: 0.6193
2025-11-14 22:13:14,251 - INFO - Fold 5 Train Epoch 37/200, Batch 20, Loss: 6.9314, Pearson: 0.6454, Spearman: 0.6125
2025-11-14 22:13:24,364 - INFO - Fold 5 Train Epoch 37/200, Batch 30, Loss: 6.7791, Pearson: 0.6537, Spearman: 0.6183
2025-11-14 22:13:34,485 - INFO - Fold 5 Train Epoch 37/200, Batch 40, Loss: 7.0049, Pearson: 0.6637, Spearman: 0.6230
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6567
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6509
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.96542501449585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 37 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.6782465 7.6782465 7.6782465 7.6782465 0.
 0.        7.6782465 0.       ]
Sample y_pred values (first sample, first 10 genes):
[0.8504616 1.1430371 2.6074743 1.3255769 3.893022  4.3076167 1.2702378
 1.3030586 1.8850074 4.3339624]
y_true  -> mean=2.1641, std=3.5279, min=0.0000, max=13.0244
y_pred  -> mean=2.0506, std=2.2287, min=0.0000, max=13.9127
Batch 0 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6442
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6495
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6425
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6377
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6507
2025-11-14 22:13:44,625 - INFO - Fold 5 Train Epoch 37/200, Batch 50, Loss: 6.8551, Pearson: 0.6510, Spearman: 0.6144
2025-11-14 22:13:54,762 - INFO - Fold 5 Train Epoch 37/200, Batch 60, Loss: 6.8947, Pearson: 0.6532, Spearman: 0.6147
2025-11-14 22:14:02,581 - INFO - Fold 5 Train Epoch 37/200, Batch 70, Loss: 6.8429, Pearson: 0.6517, Spearman: 0.6157
2025-11-14 22:14:11,829 - INFO - Fold 5 Train Epoch 37/200, Batch 80, Loss: 7.1007, Pearson: 0.6555, Spearman: 0.6189
2025-11-14 22:14:18,125 - INFO - Fold 5 Train Epoch 37/200, Train Loss: 6.9636, Pearson Mean: 0.6521, Spearman Mean: 0.6168
2025-11-14 22:14:18,125 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4627, 'spearman_mean_genewise': 0.4145, 'l1_error_mean': 1.8303, 'l2_errors_mean': 6.9635, 'r2_scores_mean': 0.2264, 'pearson_std': 0.114, 'l2_error_q1': 4.5936, 'l2_error_q2': 6.5894, 'l2_error_q3': 9.0316, 'r2_score_q1': 0.1443, 'r2_score_q2': 0.1984, 'r2_score_q3': 0.2765, 'mape_mean': 58.1007, 'mape_std': 18.2206, 'rmse_mean': 2.5911, 'rmse_std': 0.4998}
2025-11-14 22:14:18,515 - INFO - Fold 5 Val Epoch 37/200, Batch 0, Loss: 7.0587, Pearson: 0.4851, Spearman: 0.4868
2025-11-14 22:14:20,136 - INFO - Fold 5 Val Epoch 37/200, Batch 10, Loss: 7.8251, Pearson: 0.6214, Spearman: 0.6126
2025-11-14 22:14:21,647 - INFO - Fold 5 Val Epoch 37/200, Batch 20, Loss: 9.1302, Pearson: 0.5449, Spearman: 0.5538
2025-11-14 22:14:24,036 - INFO - Fold 5 Val Epoch 37/200, Val Loss: 8.3249, Pearson Mean: 0.5601, Spearman Mean: 0.5519
2025-11-14 22:14:24,037 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3041, 'spearman_mean_genewise': 0.275, 'l1_error_mean': 2.1217, 'l2_errors_mean': 8.3036, 'r2_scores_mean': 0.0937, 'pearson_std': 0.1096, 'l2_error_q1': 5.6286, 'l2_error_q2': 8.0645, 'l2_error_q3': 10.9085, 'r2_score_q1': 0.0433, 'r2_score_q2': 0.0739, 'r2_score_q3': 0.1183, 'mape_mean': 63.5433, 'mape_std': 19.3895, 'rmse_mean': 2.832, 'rmse_std': 0.5323}
2025-11-14 22:14:24,037 - INFO - Learning rate for epoch 37: 1.0000000000000002e-07
2025-11-14 22:14:24,037 - INFO - No improvement in spearman genewise. Patience: 21/30
2025-11-14 22:14:24,872 - INFO - Fold 5 Train Epoch 38/200, Batch 0, Loss: 6.9569, Pearson: 0.6597, Spearman: 0.6252
2025-11-14 22:14:34,637 - INFO - Fold 5 Train Epoch 38/200, Batch 10, Loss: 7.0505, Pearson: 0.6522, Spearman: 0.6150
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6637
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6642
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6625
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6500
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.963499546051025
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 38 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       0.       0.       0.       0.
 7.493502 7.493502]
Sample y_pred values (first sample, first 10 genes):
[0.12017659 0.13180003 0.4861261  0.518589   2.029365   1.4128226
 0.32836524 1.1002688  2.8633595  7.0065317 ]
y_true  -> mean=2.1521, std=3.5041, min=0.0000, max=12.6288
y_pred  -> mean=2.0453, std=2.2070, min=0.0000, max=14.3995
Batch 0 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6591
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6553
2025-11-14 22:14:44,771 - INFO - Fold 5 Train Epoch 38/200, Batch 20, Loss: 6.9518, Pearson: 0.6556, Spearman: 0.6237
2025-11-14 22:14:54,886 - INFO - Fold 5 Train Epoch 38/200, Batch 30, Loss: 6.8757, Pearson: 0.6478, Spearman: 0.6178
2025-11-14 22:15:05,012 - INFO - Fold 5 Train Epoch 38/200, Batch 40, Loss: 6.9903, Pearson: 0.6698, Spearman: 0.6270
2025-11-14 22:15:15,142 - INFO - Fold 5 Train Epoch 38/200, Batch 50, Loss: 6.8197, Pearson: 0.6563, Spearman: 0.6185
2025-11-14 22:15:25,252 - INFO - Fold 5 Train Epoch 38/200, Batch 60, Loss: 6.9990, Pearson: 0.6506, Spearman: 0.6169
2025-11-14 22:15:33,130 - INFO - Fold 5 Train Epoch 38/200, Batch 70, Loss: 7.0634, Pearson: 0.6499, Spearman: 0.6094
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6669
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6698
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6622
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6426
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6541
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6398
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6600
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6536
2025-11-14 22:15:42,323 - INFO - Fold 5 Train Epoch 38/200, Batch 80, Loss: 6.7386, Pearson: 0.6555, Spearman: 0.6166
2025-11-14 22:15:48,567 - INFO - Fold 5 Train Epoch 38/200, Train Loss: 6.9642, Pearson Mean: 0.6521, Spearman Mean: 0.6168
2025-11-14 22:15:48,567 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4626, 'spearman_mean_genewise': 0.4145, 'l1_error_mean': 1.8308, 'l2_errors_mean': 6.9641, 'r2_scores_mean': 0.2263, 'pearson_std': 0.1141, 'l2_error_q1': 4.5925, 'l2_error_q2': 6.5996, 'l2_error_q3': 9.0314, 'r2_score_q1': 0.1442, 'r2_score_q2': 0.1978, 'r2_score_q3': 0.2772, 'mape_mean': 58.1159, 'mape_std': 18.2285, 'rmse_mean': 2.5912, 'rmse_std': 0.4998}
2025-11-14 22:15:48,947 - INFO - Fold 5 Val Epoch 38/200, Batch 0, Loss: 7.0443, Pearson: 0.4845, Spearman: 0.4865
2025-11-14 22:15:50,682 - INFO - Fold 5 Val Epoch 38/200, Batch 10, Loss: 7.8219, Pearson: 0.6217, Spearman: 0.6131
2025-11-14 22:15:52,179 - INFO - Fold 5 Val Epoch 38/200, Batch 20, Loss: 9.0702, Pearson: 0.5475, Spearman: 0.5562
2025-11-14 22:15:54,549 - INFO - Fold 5 Val Epoch 38/200, Val Loss: 8.3118, Pearson Mean: 0.5607, Spearman Mean: 0.5526
2025-11-14 22:15:54,549 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3042, 'spearman_mean_genewise': 0.2754, 'l1_error_mean': 2.1362, 'l2_errors_mean': 8.2918, 'r2_scores_mean': 0.0946, 'pearson_std': 0.1096, 'l2_error_q1': 5.6188, 'l2_error_q2': 8.0653, 'l2_error_q3': 10.877, 'r2_score_q1': 0.0429, 'r2_score_q2': 0.0746, 'r2_score_q3': 0.1199, 'mape_mean': 62.724, 'mape_std': 19.5391, 'rmse_mean': 2.8301, 'rmse_std': 0.5313}
2025-11-14 22:15:54,550 - INFO - Learning rate for epoch 38: 1.0000000000000002e-07
2025-11-14 22:15:54,550 - INFO - No improvement in spearman genewise. Patience: 22/30
2025-11-14 22:15:55,405 - INFO - Fold 5 Train Epoch 39/200, Batch 0, Loss: 7.2013, Pearson: 0.6477, Spearman: 0.6169
2025-11-14 22:16:05,239 - INFO - Fold 5 Train Epoch 39/200, Batch 10, Loss: 7.0452, Pearson: 0.6511, Spearman: 0.6120
2025-11-14 22:16:15,379 - INFO - Fold 5 Train Epoch 39/200, Batch 20, Loss: 6.6991, Pearson: 0.6574, Spearman: 0.6161
2025-11-14 22:16:25,500 - INFO - Fold 5 Train Epoch 39/200, Batch 30, Loss: 7.0375, Pearson: 0.6553, Spearman: 0.6203
2025-11-14 22:16:35,659 - INFO - Fold 5 Train Epoch 39/200, Batch 40, Loss: 6.8397, Pearson: 0.6478, Spearman: 0.6107
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6555
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6392
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6483
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6530
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.964147090911865
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 39 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       8.182788 0.       0.       0.       7.48992  0.
 8.182788 7.48992 ]
Sample y_pred values (first sample, first 10 genes):
[0.9787033  1.1027274  4.5218873  0.43426502 5.3392987  3.3935852
 0.93550813 1.6818578  5.017104   7.233175  ]
y_true  -> mean=2.1538, std=3.5181, min=0.0000, max=12.8087
y_pred  -> mean=2.0481, std=2.2042, min=0.0000, max=13.5384
Batch 0 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6511
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6399
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6665
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6403
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6431
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6595
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6627
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6560
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6355
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6493
2025-11-14 22:16:45,771 - INFO - Fold 5 Train Epoch 39/200, Batch 50, Loss: 6.7427, Pearson: 0.6586, Spearman: 0.6125
2025-11-14 22:16:55,927 - INFO - Fold 5 Train Epoch 39/200, Batch 60, Loss: 6.9609, Pearson: 0.6464, Spearman: 0.6210
2025-11-14 22:17:03,821 - INFO - Fold 5 Train Epoch 39/200, Batch 70, Loss: 6.7605, Pearson: 0.6674, Spearman: 0.6250
2025-11-14 22:17:12,722 - INFO - Fold 5 Train Epoch 39/200, Batch 80, Loss: 6.9681, Pearson: 0.6536, Spearman: 0.6161
2025-11-14 22:17:18,996 - INFO - Fold 5 Train Epoch 39/200, Train Loss: 6.9675, Pearson Mean: 0.6519, Spearman Mean: 0.6167
2025-11-14 22:17:18,996 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4623, 'spearman_mean_genewise': 0.4143, 'l1_error_mean': 1.8317, 'l2_errors_mean': 6.9675, 'r2_scores_mean': 0.226, 'pearson_std': 0.114, 'l2_error_q1': 4.5913, 'l2_error_q2': 6.6063, 'l2_error_q3': 9.0408, 'r2_score_q1': 0.1441, 'r2_score_q2': 0.1973, 'r2_score_q3': 0.277, 'mape_mean': 58.1407, 'mape_std': 18.2284, 'rmse_mean': 2.5918, 'rmse_std': 0.5001}
2025-11-14 22:17:19,373 - INFO - Fold 5 Val Epoch 39/200, Batch 0, Loss: 7.0370, Pearson: 0.4855, Spearman: 0.4874
2025-11-14 22:17:21,019 - INFO - Fold 5 Val Epoch 39/200, Batch 10, Loss: 7.8346, Pearson: 0.6208, Spearman: 0.6121
2025-11-14 22:17:22,598 - INFO - Fold 5 Val Epoch 39/200, Batch 20, Loss: 9.0692, Pearson: 0.5479, Spearman: 0.5569
2025-11-14 22:17:24,955 - INFO - Fold 5 Val Epoch 39/200, Val Loss: 8.3139, Pearson Mean: 0.5606, Spearman Mean: 0.5525
2025-11-14 22:17:24,955 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3039, 'spearman_mean_genewise': 0.2751, 'l1_error_mean': 2.1364, 'l2_errors_mean': 8.294, 'r2_scores_mean': 0.0944, 'pearson_std': 0.1096, 'l2_error_q1': 5.6239, 'l2_error_q2': 8.0789, 'l2_error_q3': 10.888, 'r2_score_q1': 0.0429, 'r2_score_q2': 0.0748, 'r2_score_q3': 0.1194, 'mape_mean': 62.8949, 'mape_std': 19.4907, 'rmse_mean': 2.8305, 'rmse_std': 0.5313}
2025-11-14 22:17:24,956 - INFO - Learning rate for epoch 39: 1.0000000000000002e-07
2025-11-14 22:17:24,956 - INFO - No improvement in spearman genewise. Patience: 23/30
2025-11-14 22:17:25,843 - INFO - Fold 5 Train Epoch 40/200, Batch 0, Loss: 6.8826, Pearson: 0.6535, Spearman: 0.6208
2025-11-14 22:17:35,734 - INFO - Fold 5 Train Epoch 40/200, Batch 10, Loss: 7.1034, Pearson: 0.6418, Spearman: 0.6115
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6666
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6380
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6371
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6359
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6674
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6626
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6672
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6565
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6498
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.967471599578857
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 40 =====================
Sample y_true values (first sample, first 10 genes):
[0.       7.507961 0.       0.       7.507961 7.507961 0.       0.
 0.       8.606208]
Sample y_pred values (first sample, first 10 genes):
[0.24924403 1.2283509  2.1236217  0.89112735 5.160367   2.4612434
 0.79876107 1.3957944  3.6828036  8.849741  ]
y_true  -> mean=2.0150, std=3.4654, min=0.0000, max=12.5442
y_pred  -> mean=2.0440, std=2.2256, min=0.0000, max=13.4996
Batch 0 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6413
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6418
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6428
2025-11-14 22:17:45,860 - INFO - Fold 5 Train Epoch 40/200, Batch 20, Loss: 7.0312, Pearson: 0.6575, Spearman: 0.6221
2025-11-14 22:17:55,966 - INFO - Fold 5 Train Epoch 40/200, Batch 30, Loss: 6.8614, Pearson: 0.6529, Spearman: 0.6207
2025-11-14 22:18:06,082 - INFO - Fold 5 Train Epoch 40/200, Batch 40, Loss: 6.9167, Pearson: 0.6543, Spearman: 0.6222
2025-11-14 22:18:16,152 - INFO - Fold 5 Train Epoch 40/200, Batch 50, Loss: 7.1545, Pearson: 0.6547, Spearman: 0.6216
2025-11-14 22:18:26,291 - INFO - Fold 5 Train Epoch 40/200, Batch 60, Loss: 7.0968, Pearson: 0.6493, Spearman: 0.6167
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6393
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6654
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6376
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6446
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6632
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6489
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6560
2025-11-14 22:18:34,105 - INFO - Fold 5 Train Epoch 40/200, Batch 70, Loss: 6.8652, Pearson: 0.6367, Spearman: 0.6061
2025-11-14 22:18:43,234 - INFO - Fold 5 Train Epoch 40/200, Batch 80, Loss: 7.3582, Pearson: 0.6463, Spearman: 0.6154
2025-11-14 22:18:49,503 - INFO - Fold 5 Train Epoch 40/200, Train Loss: 6.9635, Pearson Mean: 0.6522, Spearman Mean: 0.6169
2025-11-14 22:18:49,503 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4627, 'spearman_mean_genewise': 0.4146, 'l1_error_mean': 1.8308, 'l2_errors_mean': 6.9635, 'r2_scores_mean': 0.2264, 'pearson_std': 0.114, 'l2_error_q1': 4.5901, 'l2_error_q2': 6.601, 'l2_error_q3': 9.0367, 'r2_score_q1': 0.1447, 'r2_score_q2': 0.1983, 'r2_score_q3': 0.2765, 'mape_mean': 58.1164, 'mape_std': 18.2269, 'rmse_mean': 2.5911, 'rmse_std': 0.4997}
2025-11-14 22:18:49,841 - INFO - Fold 5 Val Epoch 40/200, Batch 0, Loss: 7.0511, Pearson: 0.4850, Spearman: 0.4868
2025-11-14 22:18:51,435 - INFO - Fold 5 Val Epoch 40/200, Batch 10, Loss: 7.8187, Pearson: 0.6218, Spearman: 0.6131
2025-11-14 22:18:52,912 - INFO - Fold 5 Val Epoch 40/200, Batch 20, Loss: 9.0831, Pearson: 0.5479, Spearman: 0.5568
2025-11-14 22:18:55,249 - INFO - Fold 5 Val Epoch 40/200, Val Loss: 8.3106, Pearson Mean: 0.5610, Spearman Mean: 0.5528
2025-11-14 22:18:55,249 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3048, 'spearman_mean_genewise': 0.2758, 'l1_error_mean': 2.1226, 'l2_errors_mean': 8.2902, 'r2_scores_mean': 0.0948, 'pearson_std': 0.1099, 'l2_error_q1': 5.6192, 'l2_error_q2': 8.0606, 'l2_error_q3': 10.8868, 'r2_score_q1': 0.0433, 'r2_score_q2': 0.0745, 'r2_score_q3': 0.119, 'mape_mean': 63.3921, 'mape_std': 19.4653, 'rmse_mean': 2.8299, 'rmse_std': 0.531}
2025-11-14 22:18:55,249 - INFO - Learning rate for epoch 40: 1.0000000000000002e-07
2025-11-14 22:18:55,249 - INFO - No improvement in spearman genewise. Patience: 24/30
2025-11-14 22:18:56,196 - INFO - Fold 5 Train Epoch 41/200, Batch 0, Loss: 7.2302, Pearson: 0.6589, Spearman: 0.6209
2025-11-14 22:19:06,339 - INFO - Fold 5 Train Epoch 41/200, Batch 10, Loss: 7.1461, Pearson: 0.6506, Spearman: 0.6156
2025-11-14 22:19:16,437 - INFO - Fold 5 Train Epoch 41/200, Batch 20, Loss: 7.0904, Pearson: 0.6462, Spearman: 0.6199
2025-11-14 22:19:26,564 - INFO - Fold 5 Train Epoch 41/200, Batch 30, Loss: 7.1739, Pearson: 0.6501, Spearman: 0.6177
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6367
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6585
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6652
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6649
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6454
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6405
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6580
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6489
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.9634785652160645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 41 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        0.        0.        0.
 0.        7.7732987 0.       ]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.77953744 0.         1.3659645  0.19881022
 0.3968647  0.35515845 3.0694911  6.159463  ]
y_true  -> mean=2.2919, std=3.5578, min=0.0000, max=12.5441
y_pred  -> mean=2.0545, std=2.2367, min=0.0000, max=13.6826
Batch 0 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6527
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6635
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6445
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6506
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6453
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6469
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6572
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6383
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6463
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6638
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6542
2025-11-14 22:19:36,698 - INFO - Fold 5 Train Epoch 41/200, Batch 40, Loss: 7.0168, Pearson: 0.6447, Spearman: 0.6161
2025-11-14 22:19:46,807 - INFO - Fold 5 Train Epoch 41/200, Batch 50, Loss: 6.9510, Pearson: 0.6422, Spearman: 0.6078
2025-11-14 22:19:56,933 - INFO - Fold 5 Train Epoch 41/200, Batch 60, Loss: 7.0333, Pearson: 0.6381, Spearman: 0.6057
2025-11-14 22:20:04,774 - INFO - Fold 5 Train Epoch 41/200, Batch 70, Loss: 6.8339, Pearson: 0.6603, Spearman: 0.6225
2025-11-14 22:20:14,036 - INFO - Fold 5 Train Epoch 41/200, Batch 80, Loss: 6.9921, Pearson: 0.6534, Spearman: 0.6160
2025-11-14 22:20:20,330 - INFO - Fold 5 Train Epoch 41/200, Train Loss: 6.9632, Pearson Mean: 0.6522, Spearman Mean: 0.6169
2025-11-14 22:20:20,330 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4628, 'spearman_mean_genewise': 0.4146, 'l1_error_mean': 1.8306, 'l2_errors_mean': 6.9633, 'r2_scores_mean': 0.2264, 'pearson_std': 0.114, 'l2_error_q1': 4.5957, 'l2_error_q2': 6.6049, 'l2_error_q3': 9.0063, 'r2_score_q1': 0.1445, 'r2_score_q2': 0.1975, 'r2_score_q3': 0.2772, 'mape_mean': 58.1109, 'mape_std': 18.2225, 'rmse_mean': 2.591, 'rmse_std': 0.4999}
2025-11-14 22:20:20,736 - INFO - Fold 5 Val Epoch 41/200, Batch 0, Loss: 7.0574, Pearson: 0.4843, Spearman: 0.4864
2025-11-14 22:20:22,377 - INFO - Fold 5 Val Epoch 41/200, Batch 10, Loss: 7.8300, Pearson: 0.6211, Spearman: 0.6125
2025-11-14 22:20:23,929 - INFO - Fold 5 Val Epoch 41/200, Batch 20, Loss: 9.1093, Pearson: 0.5456, Spearman: 0.5546
2025-11-14 22:20:26,290 - INFO - Fold 5 Val Epoch 41/200, Val Loss: 8.3232, Pearson Mean: 0.5600, Spearman Mean: 0.5520
2025-11-14 22:20:26,290 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3036, 'spearman_mean_genewise': 0.2745, 'l1_error_mean': 2.131, 'l2_errors_mean': 8.3025, 'r2_scores_mean': 0.0937, 'pearson_std': 0.1094, 'l2_error_q1': 5.6296, 'l2_error_q2': 8.0772, 'l2_error_q3': 10.9, 'r2_score_q1': 0.0428, 'r2_score_q2': 0.0743, 'r2_score_q3': 0.118, 'mape_mean': 63.1348, 'mape_std': 19.3956, 'rmse_mean': 2.8318, 'rmse_std': 0.5322}
2025-11-14 22:20:26,291 - INFO - Learning rate for epoch 41: 1.0000000000000004e-08
2025-11-14 22:20:26,291 - INFO - No improvement in spearman genewise. Patience: 25/30
2025-11-14 22:20:27,244 - INFO - Fold 5 Train Epoch 42/200, Batch 0, Loss: 6.8888, Pearson: 0.6460, Spearman: 0.6163
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6447
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6444
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6390
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6614
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6537
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6422
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6584
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6457
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6407
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6606
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6615
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6494
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6500
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6617
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.963261604309082
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 42 =====================
Sample y_true values (first sample, first 10 genes):
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.85386425 0.2678982  2.3170485  0.6226474
 0.         0.6259475  1.6833851  5.7815113 ]
y_true  -> mean=1.9503, std=3.4362, min=0.0000, max=12.6035
y_pred  -> mean=2.0364, std=2.1836, min=0.0000, max=12.6686
Batch 0 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6534
2025-11-14 22:20:37,318 - INFO - Fold 5 Train Epoch 42/200, Batch 10, Loss: 6.6877, Pearson: 0.6496, Spearman: 0.6064
2025-11-14 22:20:47,450 - INFO - Fold 5 Train Epoch 42/200, Batch 20, Loss: 7.1353, Pearson: 0.6577, Spearman: 0.6221
2025-11-14 22:20:57,578 - INFO - Fold 5 Train Epoch 42/200, Batch 30, Loss: 6.9265, Pearson: 0.6551, Spearman: 0.6203
2025-11-14 22:21:07,700 - INFO - Fold 5 Train Epoch 42/200, Batch 40, Loss: 6.7465, Pearson: 0.6641, Spearman: 0.6207
2025-11-14 22:21:17,855 - INFO - Fold 5 Train Epoch 42/200, Batch 50, Loss: 6.7688, Pearson: 0.6501, Spearman: 0.6172
2025-11-14 22:21:27,991 - INFO - Fold 5 Train Epoch 42/200, Batch 60, Loss: 6.8526, Pearson: 0.6543, Spearman: 0.6224
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6568
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6593
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6466
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6577
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6387
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6452
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6477
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6641
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6492
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6645
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6464
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6609
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6560
2025-11-14 22:21:35,480 - INFO - Fold 5 Train Epoch 42/200, Batch 70, Loss: 7.0040, Pearson: 0.6575, Spearman: 0.6167
2025-11-14 22:21:44,964 - INFO - Fold 5 Train Epoch 42/200, Batch 80, Loss: 6.7931, Pearson: 0.6602, Spearman: 0.6216
2025-11-14 22:21:51,174 - INFO - Fold 5 Train Epoch 42/200, Train Loss: 6.9653, Pearson Mean: 0.6520, Spearman Mean: 0.6167
2025-11-14 22:21:51,174 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4626, 'spearman_mean_genewise': 0.4144, 'l1_error_mean': 1.8311, 'l2_errors_mean': 6.9654, 'r2_scores_mean': 0.2262, 'pearson_std': 0.1139, 'l2_error_q1': 4.5879, 'l2_error_q2': 6.6036, 'l2_error_q3': 9.0281, 'r2_score_q1': 0.144, 'r2_score_q2': 0.1978, 'r2_score_q3': 0.2766, 'mape_mean': 58.1147, 'mape_std': 18.2212, 'rmse_mean': 2.5914, 'rmse_std': 0.4998}
2025-11-14 22:21:51,493 - INFO - Fold 5 Val Epoch 42/200, Batch 0, Loss: 7.0379, Pearson: 0.4851, Spearman: 0.4872
2025-11-14 22:21:53,120 - INFO - Fold 5 Val Epoch 42/200, Batch 10, Loss: 7.8495, Pearson: 0.6200, Spearman: 0.6117
2025-11-14 22:21:54,606 - INFO - Fold 5 Val Epoch 42/200, Batch 20, Loss: 9.0503, Pearson: 0.5482, Spearman: 0.5573
2025-11-14 22:21:56,938 - INFO - Fold 5 Val Epoch 42/200, Val Loss: 8.3128, Pearson Mean: 0.5606, Spearman Mean: 0.5526
2025-11-14 22:21:56,938 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3034, 'spearman_mean_genewise': 0.2745, 'l1_error_mean': 2.1467, 'l2_errors_mean': 8.2934, 'r2_scores_mean': 0.0944, 'pearson_std': 0.1094, 'l2_error_q1': 5.6225, 'l2_error_q2': 8.089, 'l2_error_q3': 10.8815, 'r2_score_q1': 0.0424, 'r2_score_q2': 0.0743, 'r2_score_q3': 0.121, 'mape_mean': 62.4872, 'mape_std': 19.5061, 'rmse_mean': 2.8304, 'rmse_std': 0.5312}
2025-11-14 22:21:56,939 - INFO - Learning rate for epoch 42: 1.0000000000000004e-08
2025-11-14 22:21:56,939 - INFO - No improvement in spearman genewise. Patience: 26/30
2025-11-14 22:21:57,983 - INFO - Fold 5 Train Epoch 43/200, Batch 0, Loss: 6.9053, Pearson: 0.6566, Spearman: 0.6197
2025-11-14 22:22:08,103 - INFO - Fold 5 Train Epoch 43/200, Batch 10, Loss: 6.8315, Pearson: 0.6361, Spearman: 0.6036
2025-11-14 22:22:18,203 - INFO - Fold 5 Train Epoch 43/200, Batch 20, Loss: 7.0385, Pearson: 0.6450, Spearman: 0.6137
2025-11-14 22:22:28,337 - INFO - Fold 5 Train Epoch 43/200, Batch 30, Loss: 6.8703, Pearson: 0.6612, Spearman: 0.6272
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6518
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6646
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6575
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6499
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6433
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6563
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6526
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6443
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6519
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.965396404266357
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 43 =====================
Sample y_true values (first sample, first 10 genes):
[0.       0.       0.       0.       7.585537 0.       0.       7.585537
 7.585537 0.      ]
Sample y_pred values (first sample, first 10 genes):
[0.7896384  1.6737704  4.182784   0.37178326 4.8670616  4.22118
 0.79638    2.135108   4.0661397  4.74411   ]
y_true  -> mean=2.0750, std=3.4830, min=0.0000, max=12.5413
y_pred  -> mean=2.0530, std=2.2258, min=0.0000, max=13.3680
Batch 0 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6647
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6361
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6461
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6540
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6501
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6612
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6424
2025-11-14 22:22:38,454 - INFO - Fold 5 Train Epoch 43/200, Batch 40, Loss: 6.9440, Pearson: 0.6440, Spearman: 0.6096
2025-11-14 22:22:48,611 - INFO - Fold 5 Train Epoch 43/200, Batch 50, Loss: 7.0960, Pearson: 0.6409, Spearman: 0.6106
2025-11-14 22:22:58,723 - INFO - Fold 5 Train Epoch 43/200, Batch 60, Loss: 7.2301, Pearson: 0.6520, Spearman: 0.6152
2025-11-14 22:23:06,177 - INFO - Fold 5 Train Epoch 43/200, Batch 70, Loss: 6.7970, Pearson: 0.6515, Spearman: 0.6119
2025-11-14 22:23:15,713 - INFO - Fold 5 Train Epoch 43/200, Batch 80, Loss: 7.1170, Pearson: 0.6491, Spearman: 0.6121
2025-11-14 22:23:21,948 - INFO - Fold 5 Train Epoch 43/200, Train Loss: 6.9635, Pearson Mean: 0.6521, Spearman Mean: 0.6168
2025-11-14 22:23:21,949 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4628, 'spearman_mean_genewise': 0.4146, 'l1_error_mean': 1.8305, 'l2_errors_mean': 6.9635, 'r2_scores_mean': 0.2265, 'pearson_std': 0.1139, 'l2_error_q1': 4.5862, 'l2_error_q2': 6.588, 'l2_error_q3': 9.0461, 'r2_score_q1': 0.1446, 'r2_score_q2': 0.1981, 'r2_score_q3': 0.2762, 'mape_mean': 58.1016, 'mape_std': 18.2181, 'rmse_mean': 2.591, 'rmse_std': 0.5}
2025-11-14 22:23:22,280 - INFO - Fold 5 Val Epoch 43/200, Batch 0, Loss: 7.0467, Pearson: 0.4853, Spearman: 0.4869
2025-11-14 22:23:23,899 - INFO - Fold 5 Val Epoch 43/200, Batch 10, Loss: 7.8370, Pearson: 0.6207, Spearman: 0.6119
2025-11-14 22:23:25,368 - INFO - Fold 5 Val Epoch 43/200, Batch 20, Loss: 9.1151, Pearson: 0.5449, Spearman: 0.5543
2025-11-14 22:23:27,683 - INFO - Fold 5 Val Epoch 43/200, Val Loss: 8.3254, Pearson Mean: 0.5598, Spearman Mean: 0.5518
2025-11-14 22:23:27,684 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3034, 'spearman_mean_genewise': 0.2745, 'l1_error_mean': 2.1341, 'l2_errors_mean': 8.3046, 'r2_scores_mean': 0.0936, 'pearson_std': 0.1094, 'l2_error_q1': 5.6287, 'l2_error_q2': 8.073, 'l2_error_q3': 10.9113, 'r2_score_q1': 0.0429, 'r2_score_q2': 0.074, 'r2_score_q3': 0.1182, 'mape_mean': 63.0439, 'mape_std': 19.4163, 'rmse_mean': 2.8322, 'rmse_std': 0.5324}
2025-11-14 22:23:27,684 - INFO - Learning rate for epoch 43: 1.0000000000000004e-08
2025-11-14 22:23:27,684 - INFO - No improvement in spearman genewise. Patience: 27/30
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6598
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6580
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6532
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6409
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6675
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6552
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6594
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6502
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6516
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6589
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6529
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6510
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6507
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6565
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.963517189025879
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 44 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        7.3014727 0.        0.        0.        0.
 0.        0.        8.399635 ]
Sample y_pred values (first sample, first 10 genes):
[0.27165654 0.24570408 1.4381043  0.54751265 2.4096756  1.2288114
 0.7447562  1.1390678  3.3218703  8.730814  ]
2025-11-14 22:23:28,686 - INFO - Fold 5 Train Epoch 44/200, Batch 0, Loss: 7.0835, Pearson: 0.6551, Spearman: 0.6209
2025-11-14 22:23:38,805 - INFO - Fold 5 Train Epoch 44/200, Batch 10, Loss: 6.8170, Pearson: 0.6391, Spearman: 0.6055
2025-11-14 22:23:48,930 - INFO - Fold 5 Train Epoch 44/200, Batch 20, Loss: 7.1251, Pearson: 0.6498, Spearman: 0.6153
2025-11-14 22:23:59,052 - INFO - Fold 5 Train Epoch 44/200, Batch 30, Loss: 7.0581, Pearson: 0.6432, Spearman: 0.6124
2025-11-14 22:24:09,169 - INFO - Fold 5 Train Epoch 44/200, Batch 40, Loss: 7.0124, Pearson: 0.6558, Spearman: 0.6182
2025-11-14 22:24:19,336 - INFO - Fold 5 Train Epoch 44/200, Batch 50, Loss: 6.9745, Pearson: 0.6528, Spearman: 0.6193
y_true  -> mean=2.1663, std=3.5178, min=0.0000, max=12.8201
y_pred  -> mean=2.0530, std=2.2230, min=0.0000, max=13.6354
Batch 0 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6480
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6582
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6416
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6597
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6391
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6396
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6660
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6481
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6432
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6522
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6554
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6592
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6602
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6565
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6528
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6590
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6455
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6491
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6563
2025-11-14 22:24:29,464 - INFO - Fold 5 Train Epoch 44/200, Batch 60, Loss: 6.8657, Pearson: 0.6471, Spearman: 0.6150
2025-11-14 22:24:36,904 - INFO - Fold 5 Train Epoch 44/200, Batch 70, Loss: 6.8855, Pearson: 0.6544, Spearman: 0.6111
2025-11-14 22:24:46,453 - INFO - Fold 5 Train Epoch 44/200, Batch 80, Loss: 6.9217, Pearson: 0.6533, Spearman: 0.6208
2025-11-14 22:24:52,714 - INFO - Fold 5 Train Epoch 44/200, Train Loss: 6.9678, Pearson Mean: 0.6519, Spearman Mean: 0.6166
2025-11-14 22:24:52,714 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4623, 'spearman_mean_genewise': 0.4142, 'l1_error_mean': 1.8317, 'l2_errors_mean': 6.9677, 'r2_scores_mean': 0.226, 'pearson_std': 0.1139, 'l2_error_q1': 4.5931, 'l2_error_q2': 6.5976, 'l2_error_q3': 9.0338, 'r2_score_q1': 0.1437, 'r2_score_q2': 0.1979, 'r2_score_q3': 0.276, 'mape_mean': 58.1404, 'mape_std': 18.2236, 'rmse_mean': 2.5919, 'rmse_std': 0.4999}
2025-11-14 22:24:53,066 - INFO - Fold 5 Val Epoch 44/200, Batch 0, Loss: 7.0628, Pearson: 0.4848, Spearman: 0.4863
2025-11-14 22:24:54,625 - INFO - Fold 5 Val Epoch 44/200, Batch 10, Loss: 7.8454, Pearson: 0.6201, Spearman: 0.6115
2025-11-14 22:24:56,149 - INFO - Fold 5 Val Epoch 44/200, Batch 20, Loss: 9.1163, Pearson: 0.5456, Spearman: 0.5550
2025-11-14 22:24:58,505 - INFO - Fold 5 Val Epoch 44/200, Val Loss: 8.3283, Pearson Mean: 0.5599, Spearman Mean: 0.5519
2025-11-14 22:24:58,506 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3026, 'spearman_mean_genewise': 0.2736, 'l1_error_mean': 2.1261, 'l2_errors_mean': 8.3075, 'r2_scores_mean': 0.0932, 'pearson_std': 0.1094, 'l2_error_q1': 5.6352, 'l2_error_q2': 8.0805, 'l2_error_q3': 10.9147, 'r2_score_q1': 0.0427, 'r2_score_q2': 0.0731, 'r2_score_q3': 0.1174, 'mape_mean': 63.6677, 'mape_std': 19.4276, 'rmse_mean': 2.8328, 'rmse_std': 0.532}
2025-11-14 22:24:58,506 - INFO - Learning rate for epoch 44: 1.0000000000000004e-08
2025-11-14 22:24:58,506 - INFO - No improvement in spearman genewise. Patience: 28/30
2025-11-14 22:24:59,466 - INFO - Fold 5 Train Epoch 45/200, Batch 0, Loss: 6.9325, Pearson: 0.6587, Spearman: 0.6233
2025-11-14 22:25:09,630 - INFO - Fold 5 Train Epoch 45/200, Batch 10, Loss: 6.9627, Pearson: 0.6456, Spearman: 0.6109
2025-11-14 22:25:19,747 - INFO - Fold 5 Train Epoch 45/200, Batch 20, Loss: 6.8977, Pearson: 0.6521, Spearman: 0.6166
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6381
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6558
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6488
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6544
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6566
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6512
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6474
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6545
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6604
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6702
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6509
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.9677348136901855
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 45 =====================
Sample y_true values (first sample, first 10 genes):
[0.        0.        0.        0.        7.1777062 7.870472  0.
 0.        7.1777062 7.870472 ]
Sample y_pred values (first sample, first 10 genes):
[0.6741548  2.2913356  3.9062858  1.6112828  4.9136457  4.8166113
 0.46689695 2.0651896  3.7116113  7.110921  ]
y_true  -> mean=2.1326, std=3.4965, min=0.0000, max=12.4751
y_pred  -> mean=2.0451, std=2.2369, min=0.0000, max=13.0738
Batch 0 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6564
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6411
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6472
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6465
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6475
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6651
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6520
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6320
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6576
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6428
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6462
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6500
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6505
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6535
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6513
2025-11-14 22:25:29,863 - INFO - Fold 5 Train Epoch 45/200, Batch 30, Loss: 6.8701, Pearson: 0.6476, Spearman: 0.6154
2025-11-14 22:25:39,987 - INFO - Fold 5 Train Epoch 45/200, Batch 40, Loss: 7.0576, Pearson: 0.6658, Spearman: 0.6259
2025-11-14 22:25:50,123 - INFO - Fold 5 Train Epoch 45/200, Batch 50, Loss: 6.9104, Pearson: 0.6459, Spearman: 0.6081
2025-11-14 22:26:00,248 - INFO - Fold 5 Train Epoch 45/200, Batch 60, Loss: 6.9306, Pearson: 0.6504, Spearman: 0.6177
2025-11-14 22:26:07,729 - INFO - Fold 5 Train Epoch 45/200, Batch 70, Loss: 7.0122, Pearson: 0.6430, Spearman: 0.6122
2025-11-14 22:26:17,100 - INFO - Fold 5 Train Epoch 45/200, Batch 80, Loss: 6.9355, Pearson: 0.6561, Spearman: 0.6200
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6547
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6448
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6476
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6493
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6556
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6548
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6658
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6625
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6538
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6571
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6603
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6388
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6470
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6479
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6581
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6434
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6507
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6648
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6440
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6497
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6449
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6573
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6559
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6630
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6471
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6406
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6634
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6509
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6415
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6456
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6551
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6424
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6517
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6430
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6561
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6636
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6599
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6584
2025-11-14 22:26:23,335 - INFO - Fold 5 Train Epoch 45/200, Train Loss: 6.9668, Pearson Mean: 0.6520, Spearman Mean: 0.6167
2025-11-14 22:26:23,336 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4624, 'spearman_mean_genewise': 0.4143, 'l1_error_mean': 1.8317, 'l2_errors_mean': 6.9667, 'r2_scores_mean': 0.2261, 'pearson_std': 0.114, 'l2_error_q1': 4.5913, 'l2_error_q2': 6.6016, 'l2_error_q3': 9.0393, 'r2_score_q1': 0.1444, 'r2_score_q2': 0.1976, 'r2_score_q3': 0.2771, 'mape_mean': 58.1351, 'mape_std': 18.2235, 'rmse_mean': 2.5916, 'rmse_std': 0.5001}
2025-11-14 22:26:23,700 - INFO - Fold 5 Val Epoch 45/200, Batch 0, Loss: 7.0584, Pearson: 0.4858, Spearman: 0.4872
2025-11-14 22:26:25,334 - INFO - Fold 5 Val Epoch 45/200, Batch 10, Loss: 7.8181, Pearson: 0.6219, Spearman: 0.6129
2025-11-14 22:26:26,908 - INFO - Fold 5 Val Epoch 45/200, Batch 20, Loss: 9.1476, Pearson: 0.5443, Spearman: 0.5535
2025-11-14 22:26:29,249 - INFO - Fold 5 Val Epoch 45/200, Val Loss: 8.3291, Pearson Mean: 0.5601, Spearman Mean: 0.5520
2025-11-14 22:26:29,250 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3043, 'spearman_mean_genewise': 0.2752, 'l1_error_mean': 2.115, 'l2_errors_mean': 8.3075, 'r2_scores_mean': 0.0933, 'pearson_std': 0.1098, 'l2_error_q1': 5.6306, 'l2_error_q2': 8.0678, 'l2_error_q3': 10.914, 'r2_score_q1': 0.0427, 'r2_score_q2': 0.073, 'r2_score_q3': 0.1177, 'mape_mean': 63.7942, 'mape_std': 19.4303, 'rmse_mean': 2.8327, 'rmse_std': 0.5325}
2025-11-14 22:26:29,250 - INFO - Learning rate for epoch 45: 1.0000000000000004e-08
2025-11-14 22:26:29,250 - INFO - No improvement in spearman genewise. Patience: 29/30
2025-11-14 22:26:30,169 - INFO - Fold 5 Train Epoch 46/200, Batch 0, Loss: 6.8694, Pearson: 0.6536, Spearman: 0.6195
2025-11-14 22:26:40,094 - INFO - Fold 5 Train Epoch 46/200, Batch 10, Loss: 7.0219, Pearson: 0.6515, Spearman: 0.6191
2025-11-14 22:26:50,229 - INFO - Fold 5 Train Epoch 46/200, Batch 20, Loss: 6.9420, Pearson: 0.6482, Spearman: 0.6068
2025-11-14 22:27:00,367 - INFO - Fold 5 Train Epoch 46/200, Batch 30, Loss: 6.9082, Pearson: 0.6467, Spearman: 0.6154
2025-11-14 22:27:10,492 - INFO - Fold 5 Train Epoch 46/200, Batch 40, Loss: 7.2016, Pearson: 0.6468, Spearman: 0.6189
2025-11-14 22:27:20,618 - INFO - Fold 5 Train Epoch 46/200, Batch 50, Loss: 6.8826, Pearson: 0.6596, Spearman: 0.6207
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6546
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6521
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.966696262359619
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])

===================== DEBUG: Epoch 46 =====================
Sample y_true values (first sample, first 10 genes):
[ 0.        0.        0.        0.        0.        0.        0.
  0.        0.       10.071948]
Sample y_pred values (first sample, first 10 genes):
[0.         0.         0.5850878  0.         2.2667906  0.03951591
 0.21298316 0.8421871  2.1592646  7.4288235 ]
y_true  -> mean=2.0473, std=3.4622, min=0.0000, max=13.8155
y_pred  -> mean=2.0476, std=2.2052, min=0.0000, max=12.4446
Batch 0 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 1 Pearson correlation: 0.6542
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 2 Pearson correlation: 0.6557
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 3 Pearson correlation: 0.6397
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 4 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 5 Pearson correlation: 0.6525
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 6 Pearson correlation: 0.6655
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 7 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 8 Pearson correlation: 0.6496
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 9 Pearson correlation: 0.6534
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 10 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 11 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 12 Pearson correlation: 0.6667
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 13 Pearson correlation: 0.6610
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 14 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 15 Pearson correlation: 0.6513
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 16 Pearson correlation: 0.6483
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 17 Pearson correlation: 0.6574
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 18 Pearson correlation: 0.6503
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 19 Pearson correlation: 0.6531
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 20 Pearson correlation: 0.6482
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 21 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 22 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 23 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 24 Pearson correlation: 0.6524
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 25 Pearson correlation: 0.6549
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 26 Pearson correlation: 0.6536
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 27 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 28 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 29 Pearson correlation: 0.6519
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 30 Pearson correlation: 0.6467
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 31 Pearson correlation: 0.6450
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 32 Pearson correlation: 0.6633
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 33 Pearson correlation: 0.6486
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 34 Pearson correlation: 0.6504
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 35 Pearson correlation: 0.6460
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 36 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 37 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 38 Pearson correlation: 0.6508
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 39 Pearson correlation: 0.6439
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 40 Pearson correlation: 0.6468
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 41 Pearson correlation: 0.6533
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 42 Pearson correlation: 0.6578
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 43 Pearson correlation: 0.6443
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 44 Pearson correlation: 0.6441
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 45 Pearson correlation: 0.6356
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 46 Pearson correlation: 0.6562
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 47 Pearson correlation: 0.6485
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 48 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 49 Pearson correlation: 0.6539
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 50 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 51 Pearson correlation: 0.6523
2025-11-14 22:27:30,733 - INFO - Fold 5 Train Epoch 46/200, Batch 60, Loss: 7.0004, Pearson: 0.6459, Spearman: 0.6182
2025-11-14 22:27:38,207 - INFO - Fold 5 Train Epoch 46/200, Batch 70, Loss: 6.9575, Pearson: 0.6498, Spearman: 0.6108
2025-11-14 22:27:47,636 - INFO - Fold 5 Train Epoch 46/200, Batch 80, Loss: 6.9087, Pearson: 0.6530, Spearman: 0.6192
2025-11-14 22:27:53,873 - INFO - Fold 5 Train Epoch 46/200, Train Loss: 6.9614, Pearson Mean: 0.6523, Spearman Mean: 0.6169
2025-11-14 22:27:53,873 - INFO - Training Metrics: {'pearson_mean_genewise': 0.4629, 'spearman_mean_genewise': 0.4147, 'l1_error_mean': 1.8302, 'l2_errors_mean': 6.9613, 'r2_scores_mean': 0.2266, 'pearson_std': 0.114, 'l2_error_q1': 4.5808, 'l2_error_q2': 6.5981, 'l2_error_q3': 9.0125, 'r2_score_q1': 0.1449, 'r2_score_q2': 0.1982, 'r2_score_q3': 0.2784, 'mape_mean': 58.0957, 'mape_std': 18.2269, 'rmse_mean': 2.5907, 'rmse_std': 0.4996}
2025-11-14 22:27:54,254 - INFO - Fold 5 Val Epoch 46/200, Batch 0, Loss: 7.0677, Pearson: 0.4846, Spearman: 0.4860
2025-11-14 22:27:55,963 - INFO - Fold 5 Val Epoch 46/200, Batch 10, Loss: 7.8280, Pearson: 0.6213, Spearman: 0.6124
2025-11-14 22:27:57,474 - INFO - Fold 5 Val Epoch 46/200, Batch 20, Loss: 9.1304, Pearson: 0.5449, Spearman: 0.5541
2025-11-14 22:27:59,833 - INFO - Fold 5 Val Epoch 46/200, Val Loss: 8.3331, Pearson Mean: 0.5597, Spearman Mean: 0.5514
2025-11-14 22:27:59,834 - INFO - Validation Metrics: {'pearson_mean_genewise': 0.3033, 'spearman_mean_genewise': 0.2741, 'l1_error_mean': 2.1218, 'l2_errors_mean': 8.3121, 'r2_scores_mean': 0.0928, 'pearson_std': 0.1094, 'l2_error_q1': 5.6337, 'l2_error_q2': 8.0707, 'l2_error_q3': 10.9332, 'r2_score_q1': 0.0422, 'r2_score_q2': 0.073, 'r2_score_q3': 0.1179, 'mape_mean': 63.6819, 'mape_std': 19.463, 'rmse_mean': 2.8334, 'rmse_std': 0.5327}
2025-11-14 22:27:59,834 - INFO - Learning rate for epoch 46: 1.0000000000000004e-08
2025-11-14 22:27:59,834 - INFO - No improvement in spearman genewise. Patience: 30/30
2025-11-14 22:27:59,834 - INFO - Early stopping triggered. Breaking training loop.
2025-11-14 22:27:59,835 - INFO - ===== Completed Fold 5/5 =====
2025-11-14 22:27:59,835 - INFO - All folds completed successfully.
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 52 Pearson correlation: 0.6567
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 53 Pearson correlation: 0.6546
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 54 Pearson correlation: 0.6569
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 55 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 56 Pearson correlation: 0.6624
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 57 Pearson correlation: 0.6601
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 58 Pearson correlation: 0.6586
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 59 Pearson correlation: 0.6473
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 60 Pearson correlation: 0.6459
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 61 Pearson correlation: 0.6583
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 62 Pearson correlation: 0.6521
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 63 Pearson correlation: 0.6553
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 64 Pearson correlation: 0.6487
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 65 Pearson correlation: 0.6484
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 66 Pearson correlation: 0.6514
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 67 Pearson correlation: 0.6490
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 68 Pearson correlation: 0.6543
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 69 Pearson correlation: 0.6605
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 70 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 71 Pearson correlation: 0.6478
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 72 Pearson correlation: 0.6550
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 73 Pearson correlation: 0.6596
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 74 Pearson correlation: 0.6435
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 75 Pearson correlation: 0.6588
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 76 Pearson correlation: 0.6451
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 77 Pearson correlation: 0.6523
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 78 Pearson correlation: 0.6498
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 79 Pearson correlation: 0.6579
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 80 Pearson correlation: 0.6530
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 81 Pearson correlation: 0.6515
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 82 Pearson correlation: 0.6458
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 83 Pearson correlation: 0.6587
image : torch.Size([128, 3, 224, 224]), y_true: torch.Size([128, 785]), y_pred: torch.Size([128, 785])
Batch 84 Pearson correlation: 0.6551
image : torch.Size([121, 3, 224, 224]), y_true: torch.Size([121, 785]), y_pred: torch.Size([121, 785])
Batch 85 Pearson correlation: 0.6571
all_y_true: <class 'numpy.ndarray'>
all_y_pred: <class 'numpy.ndarray'>
========================= 6.961309432983398
